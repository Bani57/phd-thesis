@inproceedings{liao_efficient_2019,
	title = {Efficient {Graph} {Generation} with {Graph} {Recurrent} {Attention} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/d0921d442ee91b896ad95059d13df618-Abstract.html},
	abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs).
Our model generates graphs one block of nodes and associated edges at a time.
The block size and sampling stride allow us to trade off sample quality for efficiency.
Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention.
This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs.
Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. 
Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings.
On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models.
Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality.
Our code is released at: {\textbackslash}url\{https://github.com/lrjconan/GRAN\}.},
	urldate = {2025-01-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Hamilton, Will and Duvenaud, David K and Urtasun, Raquel and Zemel, Richard},
	year = {2019},
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).  We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic.  The MMD can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {25},
	urldate = {2025-01-30},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773},
}

@article{ramakrishnan_quantum_2014,
	title = {Quantum chemistry structures and properties of 134 kilo molecules},
	volume = {1},
	copyright = {2014 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201422},
	doi = {10.1038/sdata.2014.22},
	abstract = {Computational de novo design of new drugs and materials requires rigorous and unbiased exploration of chemical compound space. However, large uncharted territories persist due to its size scaling combinatorially with molecular size. We report computed geometric, energetic, electronic, and thermodynamic properties for 134k stable small organic molecules made up of CHONF. These molecules correspond to the subset of all 133,885 species with up to nine heavy atoms (CONF) out of the GDB-17 chemical universe of 166 billion organic molecules. We report geometries minimal in energy, corresponding harmonic frequencies, dipole moments, polarizabilities, along with energies, enthalpies, and free energies of atomization. All properties were calculated at the B3LYP/6-31G(2df,p) level of quantum chemistry. Furthermore, for the predominant stoichiometry, C7H10O2, there are 6,095 constitutional isomers among the 134k molecules. We report energies, enthalpies, and free energies of atomization at the more accurate G4MP2 level of theory for all of them. As such, this data set provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.},
	language = {en},
	number = {1},
	urldate = {2025-01-30},
	journal = {Scientific Data},
	author = {Ramakrishnan, Raghunathan and Dral, Pavlo O. and Rupp, Matthias and von Lilienfeld, O. Anatole},
	month = aug,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational chemistry, Density functional theory, Quantum chemistry},
	pages = {140022},
}

@article{rosenthal_minorization_1995,
	title = {Minorization conditions and convergence rates for {Markov} chain {Monte} {Carlo}},
	volume = {90},
	number = {430},
	journal = {Journal of the American Statistical Association},
	author = {Rosenthal, Jeffrey S},
	year = {1995},
	note = {Publisher: Taylor \& Francis},
	pages = {558--566},
}

@book{koller_probabilistic_2009,
	title = {Probabilistic graphical models: principles and techniques},
	publisher = {MIT press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
}

@misc{dieleman_diffusion_2024,
	title = {Diffusion is spectral autoregression},
	url = {https://sander.ai/2024/09/02/spectral-autoregression.html},
	author = {Dieleman, Sander},
	year = {2024},
}

@article{ho_cascaded_2022,
	title = {Cascaded diffusion models for high fidelity image generation},
	volume = {23},
	number = {47},
	journal = {Journal of Machine Learning Research},
	author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
	year = {2022},
	pages = {1--33},
}

@inproceedings{dadi_improving_2025,
	title = {Improving {Single} {Noise} {Level} {Denoising} {Samplers} with {Restricted} {Gaussian} {Oracles}},
	url = {https://openreview.net/forum?id=xkiI5tou6J},
	booktitle = {{ICLR} 2025 {Workshop} on {Deep} {Generative} {Model} in {Machine} {Learning}: {Theory}, {Principle} and {Efficacy}},
	author = {Dadi, Leello Tadesse and Janchevski, Andrej and Cevher, Volkan},
	year = {2025},
}

@inproceedings{choi_stargan_2020,
	title = {{StarGAN} v2: {Diverse} {Image} {Synthesis} for {Multiple} {Domains}},
	doi = {10.1109/CVPR42600.2020.00821},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
	year = {2020},
	keywords = {Generators, Training, Animals, Image generation, Image reconstruction, Transforms, Visualization},
	pages = {8185--8194},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey and {others}},
	year = {2009},
	note = {Publisher: Toronto, ON, Canada},
}

@inproceedings{sahoo_diffusion_2025,
	title = {The {Diffusion} {Duality}},
	url = {https://openreview.net/forum?id=CB0Ub2yXjC},
	booktitle = {{ICLR} 2025 {Workshop} on {Deep} {Generative} {Model} in {Machine} {Learning}: {Theory}, {Principle} and {Efficacy}},
	author = {Sahoo, Subham Sekhar and Deschenaux, Justin and Gokaslan, Aaron and Wang, Guanghan and Chiu, Justin T. and Kuleshov, Volodymyr},
	year = {2025},
}

@inproceedings{akhound-sadegh_iterated_2024,
	title = {Iterated {Denoising} {Energy} {Matching} for {Sampling} from {Boltzmann} {Densities}},
	url = {https://proceedings.mlr.press/v235/akhound-sadegh24a.html},
	abstract = {Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient—and no data samples—to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging from standard synthetic energy functions to invariant nnn-body particle systems. We show that the proposed approach achieves state-of-the-art performance on all metrics and trains 2−5×2−5×2-5{\textbackslash}times faster, which allows it to be the first method to train using energy on the challenging 555555-particle Lennard-Jones system.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Akhound-Sadegh, Tara and Rector-Brooks, Jarrid and Bose, Joey and Mittal, Sarthak and Lemos, Pablo and Liu, Cheng-Hao and Sendera, Marcin and Ravanbakhsh, Siamak and Gidel, Gauthier and Bengio, Yoshua and Malkin, Nikolay and Tong, Alexander},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {760--786},
}

@inproceedings{chehab_practical_2024,
	title = {A {Practical} {Diffusion} {Path} for {Sampling}},
	url = {https://openreview.net/forum?id=1pjDEW2AsZ},
	booktitle = {{ICML} 2024 {Workshop} on {Structured} {Probabilistic} {Inference} \& {Generative} {Modeling}},
	author = {Chehab, Omar and Korba, Anna},
	year = {2024},
}

@inproceedings{sabour_align_2024,
	address = {Vienna, Austria},
	series = {{ICML}'24},
	title = {Align your steps: optimizing sampling schedules in diffusion models},
	volume = {235},
	shorttitle = {Align your steps},
	abstract = {Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called Align Your Steps. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime.},
	urldate = {2025-09-19},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Sabour, Amirmojtaba and Fidler, Sanja and Kreis, Karsten},
	month = jul,
	year = {2024},
	pages = {42947--42975},
}

@inproceedings{karras_analyzing_2024,
	title = {Analyzing and {Improving} the {Training} {Dynamics} of {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2024/html/Karras_Analyzing_and_Improving_the_Training_Dynamics_of_Diffusion_Models_CVPR_2024_paper.html},
	language = {en},
	urldate = {2025-09-20},
	author = {Karras, Tero and Aittala, Miika and Lehtinen, Jaakko and Hellsten, Janne and Aila, Timo and Laine, Samuli},
	year = {2024},
	pages = {24174--24184},
}

@inproceedings{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html},
	urldate = {2025-09-20},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	year = {2019},
	pages = {4401--4410},
}

@inproceedings{saremi_chain_2024,
	title = {Chain of {Log}-{Concave} {Markov} {Chains}},
	url = {https://openreview.net/forum?id=yiMB2DOjsR},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Saremi, Saeed and Park, Ji Won and Bach, Francis},
	year = {2024},
}

@inproceedings{lee_structured_2021,
	title = {Structured {Logconcave} {Sampling} with a {Restricted} {Gaussian} {Oracle}},
	url = {https://proceedings.mlr.press/v134/lee21a.html},
	abstract = {We give algorithms for sampling several structured logconcave families to high accuracy. We further develop a reduction framework, inspired by proximal point methods in convex optimization, which bootstraps samplers for regularized densities to generically improve dependences on problem conditioning κκ{\textbackslash}kappa from polynomial to linear. A key ingredient in our framework is the notion of a "restricted Gaussian oracle" (RGO) for g:Rd→Rg:Rd→Rg: {\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}rightarrow {\textbackslash}mathbb\{R\}, which is a sampler for distributions whose negative log-likelihood sums a quadratic (in a multiple of the identity) and ggg. By combining our reduction framework with our new samplers, we obtain the following bounds for sampling structured distributions to total variation distance {\textbackslash}eps{\textbackslash}eps{\textbackslash}eps.
For composite densities exp(−f(x)−g(x))exp⁡(−f(x)−g(x)){\textbackslash}exp(-f(x) - g(x)), where fff has condition number κκ{\textbackslash}kappa and convex (but possibly non-smooth) ggg admits an RGO, we obtain a mixing time of O(κdlog3κdϵ)O(κdlog3⁡κdϵ)O({\textbackslash}kappa d {\textbackslash}log{\textasciicircum}3{\textbackslash}tfrac\{{\textbackslash}kappa d\}\{{\textbackslash}epsilon\}), matching the state-of-the-art non-composite bound Lee et.{\textbackslash} al.{\textbackslash}. No composite samplers with better mixing than general-purpose logconcave samplers were previously known.
For logconcave finite sums exp(−F(x))exp⁡(−F(x)){\textbackslash}exp(-F(x)), where F(x)=1n∑i∈[n]fi(x)F(x)=1n∑i∈[n]fi(x)F(x) = {\textbackslash}tfrac\{1\}\{n\}{\textbackslash}sum\_\{i {\textbackslash}in [n]\} f\_i(x) has condition number κκ{\textbackslash}kappa, we give a sampler querying O˜(n+κmax(d,nd−−√))O{\textasciitilde}(n+κmax(d,nd)){\textbackslash}widetilde\{O\}(n + {\textbackslash}kappa{\textbackslash}max(d, {\textbackslash}sqrt\{nd\})) gradient oracles{\textbackslash}footnote\{For convenience of exposition, the O˜O{\textasciitilde}{\textbackslash}widetilde\{O\} notation hides logarithmic factors in the dimension ddd, problem conditioning κκ{\textbackslash}kappa, desired accuracy ϵϵ{\textbackslash}epsilon, and summand count nnn (when applicable). A first-order (gradient) oracle for f:Rd→Rf:Rd→Rf:{\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}rightarrow {\textbackslash}mathbb\{R\} returns (f(x),∇f(x))(f(x),∇f(x))(f(x), {\textbackslash}nabla f(x)) on input xxx, and a zeroth-order (value) oracle only returns f(x)f(x)f(x).\} to \{fi\}i∈[n]\{fi\}i∈[n]{\textbackslash}\{f\_i{\textbackslash}\}\_\{i {\textbackslash}in [n]\}. No high-accuracy samplers with nontrivial gradient query complexity were previously known.
For densities with condition number κκ{\textbackslash}kappa, we give an algorithm obtaining mixing time O(κdlog2κdϵ)O(κdlog2⁡κdϵ)O({\textbackslash}kappa d {\textbackslash}log{\textasciicircum}2{\textbackslash}tfrac\{{\textbackslash}kappa d\}\{{\textbackslash}epsilon\}), improving Lee et.{\textbackslash} al.{\textbackslash} by a logarithmic factor with a significantly simpler analysis. We also show a zeroth-order algorithm attains the same query complexity.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Lee, Yin Tat and Shen, Ruoqi and Tian, Kevin},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2993--3050},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\BV2ZVDTL\\Lee et al. - 2021 - Structured Logconcave Sampling with a Restricted Gaussian Oracle.pdf:application/pdf},
}

@misc{shen_composite_2020,
	title = {Composite {Logconcave} {Sampling} with a {Restricted} {Gaussian} {Oracle}},
	url = {https://ui.adsabs.harvard.edu/abs/2020arXiv200605976S},
	doi = {10.48550/arXiv.2006.05976},
	abstract = {We consider sampling from composite densities on \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ of the form \$d{\textbackslash}pi(x) {\textbackslash}propto {\textbackslash}exp(-f(x) - g(x))dx\$ for well-conditioned \$f\$ and convex (but possibly non-smooth) \$g\$, a family generalizing restrictions to a convex set, through the abstraction of a restricted Gaussian oracle. For \$f\$ with condition number \${\textbackslash}kappa\$, our algorithm runs in \$O {\textbackslash}left({\textbackslash}kappa{\textasciicircum}2 d {\textbackslash}log{\textasciicircum}2{\textbackslash}tfrac\{{\textbackslash}kappa d\}\{{\textbackslash}epsilon\}{\textbackslash}right)\$ iterations, each querying a gradient of \$f\$ and a restricted Gaussian oracle, to achieve total variation distance \${\textbackslash}epsilon\$. The restricted Gaussian oracle, which draws samples from a distribution whose negative log-likelihood sums a quadratic and \$g\$, has been previously studied and is a natural extension of the proximal oracle used in composite optimization. Our algorithm is conceptually simple and obtains stronger provable guarantees and greater generality than existing methods for composite sampling. We conduct experiments showing our algorithm vastly improves upon the hit-and-run algorithm for sampling the restriction of a (non-diagonal) Gaussian to the positive orthant.},
	urldate = {2025-09-20},
	publisher = {arXiv},
	author = {Shen, Ruoqi and Tian, Kevin and Tat Lee, Yin},
	month = jun,
	year = {2020},
	note = {ADS Bibcode: 2020arXiv200605976S},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\DCB6L54G\\Shen et al. - 2020 - Composite Logconcave Sampling with a Restricted Gaussian Oracle.pdf:application/pdf},
}

@inproceedings{kim_consistency_2024,
	title = {Consistency {Trajectory} {Models}: {Learning} {Probability} {Flow} {ODE} {Trajectory} of {Diffusion}},
	url = {https://openreview.net/forum?id=ymjI8feDTD},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Kim, Dongjun and Lai, Chieh-Hsin and Liao, Wei-Hsiang and Murata, Naoki and Takida, Yuhta and Uesaka, Toshimitsu and He, Yutong and Mitsufuji, Yuki and Ermon, Stefano},
	year = {2024},
}

@inproceedings{song_denoising_2021,
	title = {Denoising {Diffusion} {Implicit} {Models}},
	url = {https://openreview.net/forum?id=St1giarCHLP},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	year = {2021},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2025-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\ZTSKU88T\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{vargas_denoising_2023,
	title = {Denoising {Diffusion} {Samplers}},
	url = {https://openreview.net/forum?id=8pvnfTAbu1f},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Vargas, Francisco and Grathwohl, Will Sussman and Doucet, Arnaud},
	year = {2023},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://proceedings.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2025-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	pages = {8780--8794},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\C2JMY5T4\\Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf},
}

@inproceedings{yang_diffusion_2023,
	title = {Diffusion {Probabilistic} {Model} {Made} {Slim}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.html},
	language = {en},
	urldate = {2025-09-20},
	author = {Yang, Xingyi and Zhou, Daquan and Feng, Jiashi and Wang, Xinchao},
	year = {2023},
	pages = {22552--22562},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\JFP4FC8N\\Yang et al. - 2023 - Diffusion Probabilistic Model Made Slim.pdf:application/pdf},
}

@inproceedings{chen_diffusive_2024,
	title = {Diffusive {Gibbs} {Sampling}},
	url = {https://proceedings.mlr.press/v235/chen24be.html},
	abstract = {The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Wenlin and Zhang, Mingtian and Paige, Brooks and Hernández-Lobato, José Miguel and Barber, David},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {7731--7747},
}

@article{lu_dpm-solver_2022,
	title = {{DPM}-{Solver}: {A} {Fast} {ODE} {Solver} for {Diffusion} {Probabilistic} {Model} {Sampling} in {Around} 10 {Steps}},
	volume = {35},
	shorttitle = {{DPM}-{Solver}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/260a14acce2a89dad36adc8eefe7c59e-Abstract-Conference.html},
	language = {en},
	urldate = {2025-09-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
	month = dec,
	year = {2022},
	pages = {5775--5787},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\A42UJEK4\\Lu et al. - 2022 - DPM-Solver A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps.pdf:application/pdf},
}

@misc{lu_dpm-solver_2023,
	title = {{DPM}-{Solver}++: {Fast} {Solver} for {Guided} {Sampling} of {Diffusion} {Probabilistic} {Models}},
	url = {https://openreview.net/forum?id=4vGwQqviud5},
	author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
	year = {2023},
}

@misc{balaji_ediff-i_2022,
	title = {{eDiff}-{I}: {Text}-to-{Image} {Diffusion} {Models} with an {Ensemble} of {Expert} {Denoisers}},
	shorttitle = {{eDiff}-{I}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv221101324B},
	doi = {10.48550/arXiv.2211.01324},
	abstract = {Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's "paint-with-words" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/},
	urldate = {2025-09-20},
	publisher = {arXiv},
	author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
	month = nov,
	year = {2022},
	note = {ADS Bibcode: 2022arXiv221101324B},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\3XIVSMVW\\Balaji et al. - 2022 - eDiff-I Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers.pdf:application/pdf},
}

@article{karras_elucidating_2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html},
	language = {en},
	urldate = {2025-09-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = dec,
	year = {2022},
	pages = {26565--26577},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\L7F7QF5T\\Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Generative Models.pdf:application/pdf},
}

@article{casella_explaining_1992,
	title = {Explaining the {Gibbs} {Sampler}},
	volume = {46},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1992.10475878},
	doi = {10.1080/00031305.1992.10475878},
	number = {3},
	journal = {The American Statistician},
	author = {Casella, George and George, Edward I.},
	year = {1992},
	pages = {167--174},
}

@inproceedings{rissanen_generative_2023,
	title = {Generative {Modelling} with {Inverse} {Heat} {Dissipation}},
	url = {https://openreview.net/forum?id=4PJUBT9f2Ol},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Rissanen, Severi and Heinonen, Markus and Solin, Arno},
	year = {2023},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2025-09-20},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
}

@inproceedings{chen_improved_2023,
	title = {Improved {Analysis} of {Score}-based {Generative} {Modeling}: {User}-{Friendly} {Bounds} under {Minimal} {Smoothness} {Assumptions}},
	shorttitle = {Improved {Analysis} of {Score}-based {Generative} {Modeling}},
	url = {https://proceedings.mlr.press/v202/chen23q.html},
	abstract = {We give an improved theoretical analysis of score-based generative modeling. Under a score estimate with small L2L2L{\textasciicircum}2 error (averaged across timesteps), we provide efficient convergence guarantees for any data distribution with second-order moment, by either employing early stopping or assuming smoothness condition on the score function of the data distribution. Our result does not rely on any log-concavity or functional inequality assumption and has a logarithmic dependence on the smoothness. In particular, we show that under only a finite second moment condition, approximating the following in reverse KL divergence in ϵϵ{\textbackslash}epsilon-accuracy can be done in O{\textasciitilde}(dlog(1/δ)ϵ)O{\textasciitilde}(dlog⁡(1/δ)ϵ){\textbackslash}tilde O{\textbackslash}left({\textbackslash}frac\{d {\textbackslash}log (1/{\textbackslash}delta)\}\{{\textbackslash}epsilon\}{\textbackslash}right) steps: 1) the variance-δδ{\textbackslash}delta Gaussian perturbation of any data distribution; 2) data distributions with 1/δ1/δ1/{\textbackslash}delta-smooth score functions. Our analysis also provides a quantitative comparison between different discrete approximations and may guide the choice of discretization points in practice.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Hongrui and Lee, Holden and Lu, Jianfeng},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {4735--4763},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\UMX7ZIX8\\Chen et al. - 2023 - Improved Analysis of Score-based Generative Modeling User-Friendly Bounds under Minimal Smoothness.pdf:application/pdf},
}

@inproceedings{jain_journey_2022,
	title = {Journey to the {BAOAB}-limit: finding effective {MCMC} samplers for score-based models},
	url = {https://openreview.net/forum?id=j4-a3SNyaY},
	booktitle = {{NeurIPS} 2022 {Workshop} on {Score}-{Based} {Methods}},
	author = {Jain, Ajay and Poole, Ben},
	year = {2022},
}

@inproceedings{gao_learning_2021,
	title = {Learning {Energy}-{Based} {Models} by {Diffusion} {Recovery} {Likelihood}},
	url = {https://openreview.net/forum?id=v_1Soh8QUNc},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gao, Ruiqi and Song, Yang and Poole, Ben and Wu, Ying Nian and Kingma, Diederik P.},
	year = {2021},
}

@inproceedings{benton_nearly_2024,
	title = {Nearly {\textbackslash}d{\textbackslash}-{Linear} {Convergence} {Bounds} for {Diffusion} {Models} via {Stochastic} {Localization}},
	url = {https://openreview.net/forum?id=r5njV3BsuD},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Benton, Joe and Bortoli, Valentin De and Doucet, Arnaud and Deligiannidis, George},
	year = {2024},
}

@inproceedings{saremi_multimeasurement_2022,
	title = {Multimeasurement {Generative} {Models}},
	url = {https://openreview.net/forum?id=QRX0nCX_gk},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Saremi, Saeed and Srivastava, Rupesh Kumar},
	year = {2022},
}

@article{saremi_neural_2019,
	title = {Neural {Empirical} {Bayes}},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/19-216.html},
	abstract = {We unify kernel density estimation and empirical Bayes and address a set of problems in unsupervised machine learning with a geometric interpretation of those methods, rooted in the concentration of measure phenomenon. Kernel density is viewed symbolically as X⇀YX⇀YX{\textbackslash}rightharpoonup Y where the random variable XXX is smoothed to Y=X+N(0,σ2Id)Y=X+N(0,σ2Id)Y= X+N(0,{\textbackslash}sigma{\textasciicircum}2 I\_d), and empirical Bayes is the machinery to denoise in a least-squares sense, which we express as X↽YX↽YX {\textbackslash}leftharpoondown Y. A learning objective is derived by combining these two, symbolically captured by X⇌YX⇌YX {\textbackslash}rightleftharpoons Y. Crucially, instead of using the original nonparametric estimators, we parametrize the energy function with a neural network denoted by ϕϕ{\textbackslash}phi; at optimality, ∇ϕ≈−∇logf∇ϕ≈−∇log⁡f{\textbackslash}nabla {\textbackslash}phi {\textbackslash}approx -{\textbackslash}nabla {\textbackslash}log f where fff is the density of YYY. The optimization problem is abstracted as interactions of high-dimensional spheres which emerge due to the concentration of isotropic Gaussians. We introduce two algorithmic frameworks based on this machinery: (i) a “walk-jump” sampling scheme that combines Langevin MCMC (walks) and empirical Bayes (jumps), and (ii) a probabilistic framework for associative memory, called NEBULA, defined a la Hopfield by the gradient flow of the learned energy to a set of attractors. We finish the paper by reporting the emergence of very rich “creative memories” as attractors of NEBULA for highly-overlapping spheres.},
	number = {181},
	urldate = {2025-09-20},
	journal = {Journal of Machine Learning Research},
	author = {Saremi, Saeed and Hyvärinen, Aapo},
	year = {2019},
	pages = {1--23},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\LJMNEJT2\\Saremi and Hyvärinen - 2019 - Neural Empirical Bayes.pdf:application/pdf},
}

@article{chen_importance_2023,
	title = {On the {Importance} of {Noise} {Scheduling} for {Diffusion} {Models}},
	url = {https://arxiv.org/abs/2301.10972},
	journal = {arXiv preprint arXiv:2301.10972},
	author = {Chen, Ting},
	year = {2023},
}

@misc{syed_optimised_2024,
	title = {Optimised {Annealed} {Sequential} {Monte} {Carlo} {Samplers}},
	url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240812057S},
	doi = {10.48550/arXiv.2408.12057},
	abstract = {Annealed Sequential Monte Carlo (SMC) samplers are special cases of SMC samplers where the sequence of distributions can be embedded in a smooth path of distributions. Using this underlying path of distributions and a performance model based on the variance of the normalisation constant estimator, we systematically study dense schedule and large particle limits. From our theory and adaptive methods emerges a notion of global barrier capturing the inherent complexity of normalisation constant approximation under our performance model. We then turn the resulting approximations into surrogate objective functions of algorithm performance, and use them for methodology development. We obtain novel adaptive methodologies, Sequential SMC (SSMC) and Sequential AIS (SAIS) samplers, which address practical difficulties inherent in previous adaptive SMC methods. First, our SSMC algorithms are predictable: they produce a sequence of increasingly precise estimates at deterministic and known times. Second, SAIS, a special case of SSMC, enables schedule adaptation at a memory cost constant in the number of particles and require much less communication. Finally, these characteristics make SAIS highly efficient on GPUs. We develop an open-source, high-performance GPU implementation based on our methodology and demonstrate up to a hundred-fold speed improvement compared to state-of-the-art adaptive AIS methods.},
	urldate = {2025-09-20},
	publisher = {arXiv},
	author = {Syed, Saifuddin and Bouchard-Côté, Alexandre and Chern, Kevin and Doucet, Arnaud},
	month = aug,
	year = {2024},
	note = {ADS Bibcode: 2024arXiv240812057S},
	keywords = {Statistics - Computation},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\X2VXXQJQ\\Syed et al. - 2024 - Optimised Annealed Sequential Monte Carlo Samplers.pdf:application/pdf},
}

@inproceedings{gonzalez_parallel_2011,
	title = {Parallel {Gibbs} {Sampling}: {From} {Colored} {Fields} to {Thin} {Junction} {Trees}},
	shorttitle = {Parallel {Gibbs} {Sampling}},
	url = {https://proceedings.mlr.press/v15/gonzalez11a.html},
	abstract = {We explore the task of constructing a parallel Gibbs sampler, to both improve mixing and the exploration of high likelihood states. Recent work in parallel Gibbs sampling has focused on update schedules which do not guarantee convergence to the intended stationary distribution.  In this work, we propose two methods to construct parallel Gibbs samplers guaranteed to draw from the targeted distribution. The first method, called the Chromatic sampler, uses graph coloring to construct a direct parallelization of the classic sequential scan Gibbs sampler.  In the case of 2-colorable models we relate the Chromatic sampler to the Synchronous Gibbs sampler (which draws all variables simultaneously in parallel), and reveal new ergodic properties of Synchronous Gibbs chains.  Our second method, the Splash sampler, is a complementary strategy which can be used when the variables are tightly coupled. This constructs and samples multiple blocks in parallel, using a novel locking protocol and an iterative junction tree generation algorithm.  We further improve the Splash sampler through adaptive tree construction.  We demonstrate the benefits of our two sampling algorithms on large synthetic and real-world models using a 32 processor multi-core system.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the {Fourteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gonzalez, Joseph and Low, Yucheng and Gretton, Arthur and Guestrin, Carlos},
	month = jun,
	year = {2011},
	note = {ISSN: 1938-7228},
	pages = {324--332},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\Q65I2WGI\\Gonzalez et al. - 2011 - Parallel Gibbs Sampling From Colored Fields to Thin Junction Trees.pdf:application/pdf},
}

@article{shih_parallel_2023,
	title = {Parallel {Sampling} of {Diffusion} {Models}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0d1986a61e30e5fa408c81216a616e20-Abstract-Conference.html},
	language = {en},
	urldate = {2025-09-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Shih, Andy and Belkhale, Suneel and Ermon, Stefano and Sadigh, Dorsa and Anari, Nima},
	month = dec,
	year = {2023},
	pages = {4263--4276},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\Y4G558X7\\Shih et al. - 2023 - Parallel Sampling of Diffusion Models.pdf:application/pdf},
}

@inproceedings{phillips_particle_2024,
	title = {Particle {Denoising} {Diffusion} {Sampler}},
	url = {https://proceedings.mlr.press/v235/phillips24a.html},
	abstract = {Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Phillips, Angus and Dau, Hai-Dang and Hutchinson, Michael John and Bortoli, Valentin De and Deligiannidis, George and Doucet, Arnaud},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {40688--40724},
}

@misc{montanari_posterior_2024,
	title = {Posterior {Sampling} in {High} {Dimension} via {Diffusion} {Processes}},
	url = {http://arxiv.org/abs/2304.11449},
	doi = {10.48550/arXiv.2304.11449},
	abstract = {Sampling from the posterior is a key technical problem in Bayesian statistics. Rigorous guarantees are difficult to obtain for Markov Chain Monte Carlo algorithms of common use. In this paper, we study an alternative class of algorithms based on diffusion processes and variational methods. The diffusion is constructed in such a way that, at its final time, it approximates the target posterior distribution. The drift of this diffusion is given by the posterior expectation of the unknown parameter vector \$\{{\textbackslash}boldsymbol {\textbackslash}theta\}\$ given the data and the additional noisy observations. In order to construct an efficient sampling algorithm, we use a simple Euler discretization of the diffusion process, and leverage message passing algorithms and variational inference techniques to approximate the posterior expectation oracle. We apply this method to posterior sampling in two canonical problems in high-dimensional statistics: sparse regression and low-rank matrix estimation within the spiked model. In both cases we develop the first algorithms with accuracy guarantees in the regime of constant signal-to-noise ratios.},
	urldate = {2025-09-20},
	publisher = {arXiv},
	author = {Montanari, Andrea and Wu, Yuchen},
	month = aug,
	year = {2024},
	note = {arXiv:2304.11449 [math]},
	keywords = {Mathematics - Statistics Theory, Statistics - Statistics Theory},
	annote = {Comment: 70 pages; 3 pdf figures},
	file = {Preprint PDF:C\:\\Users\\bani5\\Zotero\\storage\\77GA8B8S\\Montanari and Wu - 2024 - Posterior Sampling in High Dimension via Diffusion Processes.pdf:application/pdf;Snapshot:C\:\\Users\\bani5\\Zotero\\storage\\K67BEDSF\\2304.html:text/html},
}

@inproceedings{mcdonald_proposal_2022,
	title = {Proposal of a {Score} {Based} {Approach} to {Sampling} {Using} {Monte} {Carlo} {Estimation} of {Score} and {Oracle} {Access} to {Target} {Density}},
	url = {https://openreview.net/forum?id=ez2cB__DaTV},
	booktitle = {{NeurIPS} 2022 {Workshop} on {Score}-{Based} {Methods}},
	author = {McDonald, Curtis James and Barron, Andrew R.},
	year = {2022},
}

@inproceedings{frey_protein_2024,
	title = {Protein {Discovery} with {Discrete} {Walk}-{Jump} {Sampling}},
	url = {https://openreview.net/forum?id=zMPHKOmQNb},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Frey, Nathan C. and Berenberg, Dan and Zadorozhny, Karina and Kleinhenz, Joseph and Lafrance-Vanasse, Julien and Hotzel, Isidro and Wu, Yan and Ra, Stephen and Bonneau, Richard and Cho, Kyunghyun and Loukas, Andreas and Gligorijevic, Vladimir and Saremi, Saeed},
	year = {2024},
}

@inproceedings{huang_reverse_2024,
	title = {Reverse {Diffusion} {Monte} {Carlo}},
	url = {https://openreview.net/forum?id=kIPEyMSdFV},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Huang, Xunpeng and Dong, Hanze and HAO, Yifan and Ma, Yian and Zhang, Tong},
	year = {2024},
}

@inproceedings{chen_sampling_2023,
	title = {Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions},
	url = {https://openreview.net/forum?id=zyLVMgsZ0U_},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Chen, Sitan and Chewi, Sinho and Li, Jerry and Li, Yuanzhi and Salim, Adil and Zhang, Anru},
	year = {2023},
}

@misc{montanari_sampling_2023,
	title = {Sampling, {Diffusions}, and {Stochastic} {Localization}},
	url = {https://ui.adsabs.harvard.edu/abs/2023arXiv230510690M},
	doi = {10.48550/arXiv.2305.10690},
	abstract = {Diffusions are a successful technique to sample from high-dimensional distributions. The target distribution can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution. The drift of the diffusion process is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was recently proposed in order to sample from certain statistical mechanics models. This expository article has three objectives: \$(i)\${\textasciitilde}Generalize the algorithmic construction to other stochastic localization processes. This construction is both simple and broadly applicable; \$(ii)\${\textasciitilde}Clarify the connection between diffusions and stochastic localization. This allows to derive several known sampling schemes in a unified fashion; \$(iii)\${\textasciitilde}Describe the insights that follow from this unified viewpoint.},
	urldate = {2025-09-20},
	publisher = {arXiv},
	author = {Montanari, Andrea},
	month = may,
	year = {2023},
	note = {ADS Bibcode: 2023arXiv230510690M},
	keywords = {Machine Learning},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\J4W4D27H\\Montanari - 2023 - Sampling, Diffusions, and Stochastic Localization.pdf:application/pdf},
}

@inproceedings{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {https://openreview.net/forum?id=PxTIG12RRHS},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	year = {2021},
}

@article{pethick_stable_2023,
	title = {Stable {Nonconvex}-{Nonconcave} {Training} via {Linear} {Interpolation}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9c256fa1965318b7fcb9ed104c265540-Abstract-Conference.html},
	language = {en},
	urldate = {2025-09-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pethick, Thomas and Xie, Wanyun and Cevher, Volkan},
	month = dec,
	year = {2023},
	pages = {49830--49841},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\PGRW9YSP\\Pethick et al. - 2023 - Stable Nonconvex-Nonconcave Training via Linear Interpolation.pdf:application/pdf},
}

@inproceedings{grenioux_stochastic_2024,
	title = {Stochastic {Localization} via {Iterative} {Posterior} {Sampling}},
	url = {https://proceedings.mlr.press/v235/grenioux24a.html},
	abstract = {Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, Stochastic Localization via Iterative Posterior Sampling (SLIPS), to obtain approximate samples of these dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Grenioux, Louis and Noble, Maxence and Gabrié, Marylou and Durmus, Alain Oliviero},
	month = jul,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {16337--16376},
}

@inproceedings{xiao_tackling_2022,
	title = {Tackling the {Generative} {Learning} {Trilemma} with {Denoising} {Diffusion} {GANs}},
	url = {https://openreview.net/forum?id=JprM0p-q0Co},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Xiao, Zhisheng and Kreis, Karsten and Vahdat, Arash},
	year = {2022},
}

@inproceedings{hsieh_limits_2021,
	title = {The {Limits} of {Min}-{Max} {Optimization} {Algorithms}: {Convergence} to {Spurious} {Non}-{Critical} {Sets}},
	shorttitle = {The {Limits} of {Min}-{Max} {Optimization} {Algorithms}},
	url = {https://proceedings.mlr.press/v139/hsieh21a.html},
	abstract = {Compared to minimization, the min-max optimization in machine learning applications is considerably more convoluted because of the existence of cycles and similar phenomena. Such oscillatory behaviors are well-understood in the convex-concave regime, and many algorithms are known to overcome them. In this paper, we go beyond this basic setting and characterize the convergence properties of many popular methods in solving non-convex/non-concave problems. In particular, we show that a wide class of state-of-the-art schemes and heuristics may converge with arbitrarily high probability to attractors that are in no way min-max optimal or even stationary. Our work thus points out a potential pitfall among many existing theoretical frameworks, and we corroborate our theoretical claims by explicitly showcasing spurious attractors in simple two-dimensional problems.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hsieh, Ya-Ping and Mertikopoulos, Panayotis and Cevher, Volkan},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4337--4348},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\FNI559QG\\Hsieh et al. - 2021 - The Limits of Min-Max Optimization Algorithms Convergence to Spurious Non-Critical Sets.pdf:application/pdf;Supplementary PDF:C\:\\Users\\bani5\\Zotero\\storage\\IA2R2TL3\\Hsieh et al. - 2021 - The Limits of Min-Max Optimization Algorithms Convergence to Spurious Non-Critical Sets.pdf:application/pdf},
}

@article{roberts_updating_1997,
	title = {Updating {Schemes}, {Correlation} {Structure}, {Blocking} and {Parameterization} for the {Gibbs} {Sampler}},
	volume = {59},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346048},
	abstract = {In this paper many convergence issues concerning the implementation of the Gibbs sampler are investigated. Exact computable rates of convergence for Gaussian target distributions are obtained. Different random and non-random updating strategies and blocking combinations are compared using the rates. The effect of dimensionality and correlation structure on the convergence rates are studied. Some examples are considered to demonstrate the results. For a Gaussian image analysis problem several updating strategies are described and compared. For problems in Bayesian linear models several possible parameterizations are analysed in terms of their convergence rates characterizing the optimal choice.},
	number = {2},
	urldate = {2025-09-20},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Roberts, G. O. and Sahu, S. K.},
	year = {1997},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	pages = {291--317},
}

@inproceedings{he_zeroth-order_2024,
	title = {Zeroth-{Order} {Sampling} {Methods} for {Non}-{Log}-{Concave} {Distributions}: {Alleviating} {Metastability} by {Denoising} {Diffusion}},
	url = {https://openreview.net/forum?id=X3Aljulsw5},
	booktitle = {The {Thirty}-eighth {Annual} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {He, Ye and Rojas, Kevin and Tao, Molei},
	year = {2024},
}


@article{perez_film_2018,
	title = {{FiLM}: {Visual} {Reasoning} with a {General} {Conditioning} {Layer}},
	volume = {32},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11671},
	doi = {10.1609/aaai.v32i1.11671},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Perez, Ethan and Strub, Florian and de Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
	month = apr,
	year = {2018},
}

@inproceedings{dong_refining_2025,
	address = {Abu Dhabi, UAE},
	title = {Refining {Noisy} {Knowledge} {Graph} with {Large} {Language} {Models}},
	url = {https://aclanthology.org/2025.genaik-1.9/},
	abstract = {Knowledge graphs (KGs) represent structured real-world information composed by triplets of head entity, relation, and tail entity. These graphs can be constructed automatically from text or manually curated. However, regardless of the construction method, KGs often suffer from misinformation, incompleteness, and noise, which hinder their reliability and utility. This study addresses the challenge of noisy KGs, where incorrect or misaligned entities and relations degrade graph quality. Leveraging recent advancements in large language models (LLMs) with strong capabilities across diverse tasks, we explore their potential to detect and refine noise in KGs. Specifically, we propose a novel method, LLM\_sim, to enhance the detection and refinement of noisy triples. Our results confirm the effectiveness of this approach in elevating KG quality in noisy environments. Additionally, we apply our proposed method to Knowledge Graph Completion (KGC), a downstream KG task that aims to predict missing links and improve graph completeness. Traditional KGC methods assume that KGs are noise-free, which is unrealistic in practical scenarios. Our experiments analyze the impact of varying noise levels on KGC performance, revealing that LLMs can mitigate noise by identifying and refining incorrect entries, thus enhancing KG quality.},
	booktitle = {Proceedings of the {Workshop} on {Generative} {AI} and {Knowledge} {Graphs} ({GenAIK})},
	publisher = {International Committee on Computational Linguistics},
	author = {Dong, Na and Kertkeidkachorn, Natthawut and Liu, Xin and Shirai, Kiyoaki},
	editor = {Gesese, Genet Asefa and Sack, Harald and Paulheim, Heiko and Merono-Penuela, Albert and Chen, Lihu},
	month = jan,
	year = {2025},
	pages = {78--86},
}

@phdthesis{dadi_noisy_2025,
	address = {Lausanne},
	type = {{PhD} {Thesis}},
	title = {Noisy Gradient Descent in machine learning: Generalization, games and sampling},
	url = {},
	language = {en},
	school = {EPFL},
	author = {Dadi, Leello Tadesse},
	year = {2025},
	doi = {},
	keywords = {},
}

@phdthesis{krawczuk_graph_2024,
	address = {Lausanne},
	type = {{PhD} {Thesis}},
	title = {Graph generative deep learning models with an application to circuit topologies},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/208954},
	language = {en},
	school = {EPFL},
	author = {Krawczuk, Igor},
	year = {2024},
	doi = {10.5075/epfl-thesis-9020},
	keywords = {Permutation Equivariance. Modularity. Graph Generation {\textbar} Denoising Diffusion Models {\textbar} Generative Adversarial Networks {\textbar} EDA {\textbar} Scalability {\textbar} Graph Neural Networks {\textbar} Topology Generation {\textbar} Deep Learning {\textbar} Circuit Generation},
}

@inproceedings{martinkus_spectre_2022,
	address = {San Diego},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{SPECTRE}: {Spectral} {Conditioning} {Helps} to {Overcome} the {Expressivity} {Limits} of {One}-shot {Graph} {Generators}},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/197427},
	booktitle = {International {Conference} {On} {Machine} {Learning}, {Vol} 162},
	publisher = {JMLR-JOURNAL MACHINE LEARNING RESEARCH},
	author = {Martinkus, Karolis and Loukas, Andreas and Perraudin, Nathanael and Wattenhofer, Roger},
	month = jan,
	year = {2022},
	note = {ISSN: 2640-3498},
	keywords = {Artificial Intelligence {\textbar} Computer Science, Computer Science},
}

@article{schomburg_brenda_2004,
	title = {{BRENDA}, the enzyme database: updates and major new developments},
	volume = {32},
	issn = {1362-4962},
	shorttitle = {{BRENDA}, the enzyme database},
	doi = {10.1093/nar/gkh081},
	abstract = {BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83,000 different enzymes from 9800 different organisms, classified in approximately 4200 EC numbers. BRENDA includes biochemical and molecular information on classification and nomenclature, reaction and specificity, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from approximately 46,000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with {\textgreater}50\% of all data updated in 2002 or in the first half of 2003, the development of a new EC-tree browser, a taxonomy-tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information fields and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni-koeln.de.},
	language = {eng},
	number = {Database issue},
	journal = {Nucleic Acids Research},
	author = {Schomburg, Ida and Chang, Antje and Ebeling, Christian and Gremse, Marion and Heldt, Christian and Huhn, Gregor and Schomburg, Dietmar},
	month = jan,
	year = {2004},
	pmid = {14681450},
	pmcid = {PMC308815},
	keywords = {Humans, Animals, Databases, Protein, Enzymes, Information Storage and Retrieval, Internet, Ligands, Organ Specificity, Protein Conformation, Protein Transport, Substrate Specificity, Terminology as Topic},
	pages = {D431--433},
}

@article{sen_collective_2008,
	title = {Collective {Classification} in {Network} {Data}},
	volume = {29},
	copyright = {Copyright (c) 0},
	issn = {2371-9621},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2157},
	doi = {10.1609/aimag.v29i3.2157},
	abstract = {Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.},
	language = {en},
	number = {3},
	urldate = {2025-08-28},
	journal = {AI Magazine},
	author = {Sen, Prithviraj and Namata, Galileo and Bilgic, Mustafa and Getoor, Lise and Galligher, Brian and Eliassi-Rad, Tina},
	month = sep,
	year = {2008},
	pages = {93--93},
}

@article{dobson_distinguishing_2003,
	title = {Distinguishing {Enzyme} {Structures} from {Non}-enzymes {Without} {Alignments}},
	volume = {330},
	issn = {0022-2836},
	url = {https://www.sciencedirect.com/science/article/pii/S0022283603006284},
	doi = {10.1016/S0022-2836(03)00628-4},
	abstract = {The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of known function. For proteins that are highly dissimilar or are only similar to proteins also lacking functional annotations, these methods fail. Here, we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a structurally non-redundant subset of the Protein Data Bank using simple features such as secondary-structure content, amino acid propensities, surface properties and ligands. The subset is split into two functional groupings, enzymes and non-enzymes. We use the support vector machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can be predicted to an accuracy of 77\% using 52 features to describe each protein. An adaptive search of possible subsets of features produces a simplified model based on 36 features that predicts at an accuracy of 80\%. We compare the method to sequence-based methods that also avoid calculating alignments and predict a recently released set of unrelated proteins. The most useful features for distinguishing enzymes from non-enzymes are secondary-structure content, amino acid frequencies, number of disulphide bonds and size of the largest cleft. This method is applicable to any structure as it does not require the identification of sequence or structural similarity to a protein of known function.},
	number = {4},
	urldate = {2025-08-28},
	journal = {Journal of Molecular Biology},
	author = {Dobson, Paul D. and Doig, Andrew J.},
	month = jul,
	year = {2003},
	keywords = {machine learning, enzyme, protein function prediction, structural genomics, structure},
	pages = {771--783},
}

@inproceedings{rampasek_recipe_2022,
	title = {Recipe for a {General}, {Powerful}, {Scalable} {Graph} {Transformer}},
	url = {https://openreview.net/forum?id=lMMaNf6oxKM},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rampasek, Ladislav and Galkin, Mikhail and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
	editor = {Oh, Alice H. and Agarwal, Alekh and Belgrave, Danielle and Cho, Kyunghyun},
	year = {2022},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2025-08-28},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2256--2265},
	file = {Full Text PDF:C\:\\Users\\bani5\\Zotero\\storage\\W5RZ4ZLG\\Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@misc{dwivedi_generalization_2021,
	title = {A {Generalization} of {Transformer} {Networks} to {Graphs}},
	url = {http://arxiv.org/abs/2012.09699},
	doi = {10.48550/arXiv.2012.09699},
	abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
	urldate = {2025-08-28},
	publisher = {arXiv},
	author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
	month = jan,
	year = {2021},
	note = {arXiv:2012.09699 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: AAAI 2021 Workshop on Deep Learning on Graphs: Methods and Applications (DLG-AAAI 2021); Code at https://github.com/graphdeeplearning/graphtransformer},
}

@inproceedings{karami_higen_2024,
	title = {{HiGen}: {Hierarchical} {Graph} {Generative} {Networks}},
	url = {https://openreview.net/forum?id=KNvubydSB5},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Karami, Mahdi},
	year = {2024},
}

@inproceedings{karami_multi-resolution_2024,
	title = {Multi-{Resolution} {Graph} {Diffusion}},
	url = {https://openreview.net/forum?id=0rZiMEOzh0},
	booktitle = {{ICLR} 2024 {Workshop} on {Machine} {Learning} for {Genomics} {Explorations}},
	author = {Karami, Mahdi and Krawczuk, Igor and Cevher, Volkan},
	year = {2024},
}

@inproceedings{janchevski_coins_2025,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {{COINs}: {Knowledge} {Graph} {Inference} with {Model}-based {Acceleration}},
	copyright = {All rights reserved},
	abstract = {We present a new method, called COINs for COmmunity INformed graph embeddings, to enhance the efficiency of knowledge graph models for link prediction and conjunctive query answering. COINs uses a community-detection-based graph data augmentation and a two-step prediction pipeline: we first achieve node localization through community prediction, and subsequently, we further localize within the predicted community. We establish theoretical criteria to evaluate our method in our specific context and establish a direct expression of the reduction in time complexity. We empirically demonstrate an important scalability-performance trade-off where for the median evaluation sample we preserve 97.18\% of the baseline accuracy in single-hop query answering, for only 7.52\% of the original computational cost on a single-CPU-GPU machine.},
	language = {en},
	booktitle = {{ICT} {Innovations} 2025. {AI} and the {Digital} {Frontier}: {Reshaping} {Modern} {Society} {Through} {Technology} and {Computer} {Science}},
	publisher = {Springer International Publishing},
	author = {Janchevski, Andrej and Coriou, Vincent and Cevher, Volkan},
	editor = {Chorbev, Ivan and Spasov, Dejan and R. Stojkovska, Biljana},
	year = {2025},
	keywords = {Knowledge Graph Inference, Scalability, Graph Embeddings, Community Structure},
}


@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	url = {https://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	language = {en},
	urldate = {2025-08-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {214--223},
}

@inproceedings{gulrajani_improved_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Improved training of wasserstein {GANs}},
	isbn = {978-1-5108-6096-4},
	abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
	urldate = {2025-08-27},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	month = dec,
	year = {2017},
	pages = {5769--5779},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2021-10-08},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {\_eprint: 1806.01261},
}

@article{kleitman_algorithms_1973,
	title = {Algorithms for constructing graphs and digraphs with given valences and factors},
	volume = {6},
	issn = {0012-365X},
	url = {https://doi.org/10.1016/0012-365X(73)90037-X},
	doi = {10.1016/0012-365X(73)90037-X},
	abstract = {Given a set of valences \{v"i\} such that \{v"i\} and \{v"i-k\} are both realizable as valences of graphs without loops or multiple edges, an explicit construction method is described for obtaining a graph with valences \{v"i\} having a k-factor. A number of extensions of the result are obtained. Similar results are obtained for directed graphs.},
	number = {1},
	urldate = {2022-01-10},
	journal = {Discrete Mathematics},
	author = {Kleitman, D. J. and Wang, D. L.},
	month = sep,
	year = {1973},
	pages = {79--88},
}

@article{newman_structure_2003,
	title = {The {Structure} and {Function} of {Complex} {Networks}},
	volume = {45},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/s003614450342480},
	doi = {10.1137/S003614450342480},
	abstract = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
	number = {2},
	urldate = {2022-01-10},
	journal = {SIAM Review},
	author = {Newman, M. E. J.},
	month = jan,
	year = {2003},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {05C75, 05C90, 94C15, complex systems, computer networks, graph theory, networks, percolation theory, random graphs, social networks},
	pages = {167--256},
	file = {Full Text PDF:/Users/admin/Zotero/storage/8CKF388W/Newman - 2003 - The Structure and Function of Complex Networks.pdf:application/pdf},
}

@article{arratia_probabilistic_2016,
	title = {Probabilistic {Divide}-and-{Conquer}: {A} {New} {Exact} {Simulation} {Method}, {With} {Integer} {Partitions} as an {Example}},
	volume = {25},
	issn = {0963-5483, 1469-2163},
	shorttitle = {Probabilistic {Divide}-and-{Conquer}},
	url = {https://www.cambridge.org/core/journals/combinatorics-probability-and-computing/article/probabilistic-divideandconquer-a-new-exact-simulation-method-with-integer-partitions-as-an-example/85C6175903F96D32609D1BF6820A4664},
	doi = {10.1017/S0963548315000358},
	abstract = {We propose a new method, probabilistic divide-and-conquer, for improving the success probability in rejection sampling. For the example of integer partitions, there is an ideal recursive scheme which improves the rejection cost from asymptotically order n
               3/4 to a constant. We show other examples for which a non-recursive, one-time application of probabilistic divide-and-conquer removes a substantial fraction of the rejection sampling cost.
            We also present a variation of probabilistic divide-and-conquer for generating i.i.d. samples that exploits features of the coupon collector's problem, in order to obtain a cost that is sublinear in the number of samples.},
	language = {en},
	number = {3},
	urldate = {2021-12-14},
	journal = {Combinatorics, Probability and Computing},
	author = {Arratia, Richard and DeSALVO, Stephen},
	month = may,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	keywords = {Primary 60C05, Secondary 05A17},
	pages = {324--351},
	file = {Full Text PDF:/Users/admin/Zotero/storage/EMP98ARY/Arratia and DeSALVO - 2016 - Probabilistic Divide-and-Conquer A New Exact Simu.pdf:application/pdf},
}

@article{fristedt_structure_1993,
	title = {The structure of random partitions of large integers},
	volume = {337},
	issn = {0002-9947, 1088-6850},
	url = {https://www.ams.org/tran/1993-337-02/S0002-9947-1993-1094553-1/},
	doi = {10.1090/S0002-9947-1993-1094553-1},
	abstract = {Random partitions of integers are treated in the case where all partitions of an integer are assumed to have the same probability. The focus is on limit theorems as the number being partitioned approaches ∞. The limiting probability distribution of the appropriately normalized number of parts of some small size is exponential. The large parts are described by a particular Markov chain. A central limit theorem and a law of large numbers holds for the numbers of intermediate parts of certain sizes. The major tool is a simple construction of random partitions that treats the number being partitioned as a random variable. The same technique is useful when some restriction is placed on partitions, such as the requirement that all parts must be distinct.},
	language = {en},
	number = {2},
	urldate = {2021-12-08},
	journal = {Transactions of the American Mathematical Society},
	author = {Fristedt, Bert},
	year = {1993},
	keywords = {integer partitions, probabilistic limit theorems, Random partitions},
	pages = {703--735},
	file = {Full Text PDF:/Users/admin/Zotero/storage/X9PDXISX/Fristedt - 1993 - The structure of random partitions of large intege.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/GTIZSCXQ/S0002-9947-1993-1094553-1.html:text/html},
}

@article{bevilacqua_equivariant_2021,
	title = {Equivariant {Subgraph} {Aggregation} {Networks}},
	url = {http://arxiv.org/abs/2110.02910},
	abstract = {Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures.},
	urldate = {2021-11-22},
	journal = {arXiv:2110.02910 [cs, stat]},
	author = {Bevilacqua, Beatrice and Frasca, Fabrizio and Lim, Derek and Srinivasan, Balasubramaniam and Cai, Chen and Balamurugan, Gopinath and Bronstein, Michael M. and Maron, Haggai},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02910},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 42 pages},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/DM2BMRT9/Bevilacqua et al. - 2021 - Equivariant Subgraph Aggregation Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/M9IUBZL6/2110.html:text/html},
}

@article{wang_graphgan_2017,
	title = {{GraphGAN}: {Graph} {Representation} {Learning} with {Generative} {Adversarial} {Nets}},
	shorttitle = {{GraphGAN}},
	url = {http://arxiv.org/abs/1711.08267},
	abstract = {The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces "fake" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.},
	urldate = {2021-11-22},
	journal = {arXiv:1711.08267 [cs, stat]},
	author = {Wang, Hongwei and Wang, Jia and Wang, Jialin and Zhao, Miao and Zhang, Weinan and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.08267},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The 32nd AAAI Conference on Artificial Intelligence (AAAI 2018), 8 pages},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/KS9EGYWW/Wang et al. - 2017 - GraphGAN Graph Representation Learning with Gener.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/HPPQH2N5/1711.html:text/html;Full Text:/Users/admin/Zotero/storage/738RGVNC/Wang et al. - 2018 - GraphGAN Graph Representation Learning With Gener.pdf:application/pdf},
}

@article{rozemberczki_karate_2020,
	title = {Karate {Club}: {An} {API} {Oriented} {Open}-source {Python} {Framework} for {Unsupervised} {Learning} on {Graphs}},
	shorttitle = {Karate {Club}},
	url = {http://arxiv.org/abs/2003.04819},
	abstract = {We present Karate Club a Python framework combining more than 30 state-of-the-art graph mining algorithms which can solve unsupervised machine learning tasks. The primary goal of the package is to make community detection, node and whole graph embedding available to a wide audience of machine learning researchers and practitioners. We designed Karate Club with an emphasis on a consistent application interface, scalability, ease of use, sensible out of the box model behaviour, standardized dataset ingestion, and output generation. This paper discusses the design principles behind this framework with practical examples. We show Karate Club's efficiency with respect to learning performance on a wide range of real world clustering problems, classification tasks and support evidence with regards to its competitive speed.},
	urldate = {2021-11-10},
	journal = {arXiv:2003.04819 [cs, stat]},
	author = {Rozemberczki, Benedek and Kiss, Oliver and Sarkar, Rik},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.04819},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: The frameworks is available at: https://github.com/benedekrozemberczki/karateclub},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/J8TMMCIB/Rozemberczki et al. - 2020 - Karate Club An API Oriented Open-source Python Fr.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/2XUI5ZTS/2003.html:text/html},
}

@inproceedings{ye_deep_2018,
	address = {New York, NY, USA},
	series = {{CIKM} '18},
	title = {Deep {Autoencoder}-like {Nonnegative} {Matrix} {Factorization} for {Community} {Detection}},
	isbn = {978-1-4503-6014-2},
	url = {https://doi.org/10.1145/3269206.3271697},
	doi = {10.1145/3269206.3271697},
	abstract = {Community structure is ubiquitous in real-world complex networks. The task of community detection over these networks is of paramount importance in a variety of applications. Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection due to its great interpretability and its natural fitness for capturing the community membership of nodes. However, the existing NMF-based community detection approaches are shallow methods. They learn the community assignment by mapping the original network to the community membership space directly. Considering the complicated and diversified topology structures of real-world networks, it is highly possible that the mapping between the original network and the community membership space contains rather complex hierarchical information, which cannot be interpreted by classic shallow NMF-based approaches. Inspired by the unique feature representation learning capability of deep autoencoder, we propose a novel model, named Deep Autoencoder-like NMF (DANMF), for community detection. Similar to deep autoencoder, DANMF consists of an encoder component and a decoder component. This architecture empowers DANMF to learn the hierarchical mappings between the original network and the final community assignment with implicit low-to-high level hidden attributes of the original network learnt in the intermediate layers. Thus, DANMF should be better suited to the community detection task. Extensive experiments on benchmark datasets demonstrate that DANMF can achieve better performance than the state-of-the-art NMF-based community detection approaches.},
	urldate = {2021-11-10},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ye, Fanghua and Chen, Chuan and Zheng, Zibin},
	month = oct,
	year = {2018},
	keywords = {graph clustering, community detection, deep learning, deep nonnegative matrix factorization, network analytics},
	pages = {1393--1402},
	file = {Full Text PDF:/Users/admin/Zotero/storage/T2QN5UAY/Ye et al. - 2018 - Deep Autoencoder-like Nonnegative Matrix Factoriza.pdf:application/pdf},
}

@article{gulcehre_hyperbolic_2018,
	title = {Hyperbolic {Attention} {Networks}},
	url = {http://arxiv.org/abs/1805.09786},
	abstract = {We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.},
	urldate = {2021-11-05},
	journal = {arXiv:1805.09786 [cs]},
	author = {Gulcehre, Caglar and Denil, Misha and Malinowski, Mateusz and Razavi, Ali and Pascanu, Razvan and Hermann, Karl Moritz and Battaglia, Peter and Bapst, Victor and Raposo, David and Santoro, Adam and de Freitas, Nando},
	month = may,
	year = {2018},
	note = {arXiv: 1805.09786},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/NDI96NB2/Gulcehre et al. - 2018 - Hyperbolic Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/ELJ5TVUM/1805.html:text/html},
}

@article{lancichinetti_benchmark_2008,
	title = {Benchmark graphs for testing community detection algorithms},
	volume = {78},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.78.046110},
	doi = {10.1103/PhysRevE.78.046110},
	abstract = {Community structure is one of the most important features of real networks and reveals the internal organization of the nodes. Many algorithms have been proposed but the crucial issue of testing, i.e., the question of how good an algorithm is, with respect to others, is still open. Standard tests include the analysis of simple artificial graphs with a built-in community structure, that the algorithm has to recover. However, the special graphs adopted in actual tests have a structure that does not reflect the real properties of nodes and communities found in real networks. Here we introduce a class of benchmark graphs, that account for the heterogeneity in the distributions of node degrees and of community sizes. We use this benchmark to test two popular methods of community detection, modularity optimization, and Potts model clustering. The results show that the benchmark poses a much more severe test to algorithms than standard benchmarks, revealing limits that may not be apparent at a first analysis.},
	number = {4},
	urldate = {2021-11-04},
	journal = {Physical Review E},
	author = {Lancichinetti, Andrea and Fortunato, Santo and Radicchi, Filippo},
	month = oct,
	year = {2008},
	note = {Publisher: American Physical Society},
	pages = {046110},
	file = {APS Snapshot:/Users/admin/Zotero/storage/IF732S3V/PhysRevE.78.html:text/html;Full Text PDF:/Users/admin/Zotero/storage/E8CEZ9PQ/Lancichinetti et al. - 2008 - Benchmark graphs for testing community detection a.pdf:application/pdf},
}

@article{lancichinetti_benchmarks_2009,
	title = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
	volume = {80},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.80.016118},
	doi = {10.1103/PhysRevE.80.016118},
	abstract = {Many complex networks display a mesoscopic structure with groups of nodes sharing many links with the other nodes in their group and comparatively few with nodes of different groups. This feature is known as community structure and encodes precious information about the organization and the function of the nodes. Many algorithms have been proposed but it is not yet clear how they should be tested. Recently we have proposed a general class of undirected and unweighted benchmark graphs, with heterogeneous distributions of node degree and community size. An increasing attention has been recently devoted to develop algorithms able to consider the direction and the weight of the links, which require suitable benchmark graphs for testing. In this paper we extend the basic ideas behind our previous benchmark to generate directed and weighted networks with built-in community structure. We also consider the possibility that nodes belong to more communities, a feature occurring in real systems, such as social networks. As a practical application, we show how modularity optimization performs on our benchmark.},
	number = {1},
	urldate = {2021-11-04},
	journal = {Physical Review E},
	author = {Lancichinetti, Andrea and Fortunato, Santo},
	month = jul,
	year = {2009},
	note = {Publisher: American Physical Society},
	pages = {016118},
	file = {APS Snapshot:/Users/admin/Zotero/storage/4BH6WFLR/PhysRevE.80.html:text/html;Full Text PDF:/Users/admin/Zotero/storage/Z4RXEKI5/Lancichinetti and Fortunato - 2009 - Benchmarks for testing community detection algorit.pdf:application/pdf},
}

@inproceedings{jia_communitygan_2019,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {{CommunityGAN}: {Community} {Detection} with {Generative} {Adversarial} {Nets}},
	isbn = {978-1-4503-6674-8},
	shorttitle = {{CommunityGAN}},
	url = {https://doi.org/10.1145/3308558.3313564},
	doi = {10.1145/3308558.3313564},
	abstract = {Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods.},
	urldate = {2021-11-04},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Jia, Yuting and Zhang, Qinqin and Zhang, Weinan and Wang, Xinbing},
	month = may,
	year = {2019},
	keywords = {Community Detection, Generative Adversarial Nets, Graph Representation Learning},
	pages = {784--794},
	file = {Full Text PDF:/Users/admin/Zotero/storage/9HKRPXDE/Jia et al. - 2019 - CommunityGAN Community Detection with Generative .pdf:application/pdf},
}

@article{lancichinetti_finding_2011,
	title = {Finding {Statistically} {Significant} {Communities} in {Networks}},
	volume = {6},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018961},
	doi = {10.1371/journal.pone.0018961},
	abstract = {Community structure is one of the main structural features of networks, revealing both their internal organization and the similarity of their elementary units. Despite the large variety of methods proposed to detect communities in graphs, there is a big need for multi-purpose techniques, able to handle different types of datasets and the subtleties of community structure. In this paper we present OSLOM (Order Statistics Local Optimization Method), the first method capable to detect clusters in networks accounting for edge directions, edge weights, overlapping communities, hierarchies and community dynamics. It is based on the local optimization of a fitness function expressing the statistical significance of clusters with respect to random fluctuations, which is estimated with tools of Extreme and Order Statistics. OSLOM can be used alone or as a refinement procedure of partitions/covers delivered by other techniques. We have also implemented sequential algorithms combining OSLOM with other fast techniques, so that the community structure of very large networks can be uncovered. Our method has a comparable performance as the best existing algorithms on artificial benchmark graphs. Several applications on real networks are shown as well. OSLOM is implemented in a freely available software (http://www.oslom.org), and we believe it will be a valuable tool in the analysis of networks.},
	language = {en},
	number = {4},
	urldate = {2021-11-04},
	journal = {PLOS ONE},
	author = {Lancichinetti, Andrea and Radicchi, Filippo and Ramasco, José J. and Fortunato, Santo},
	month = apr,
	year = {2011},
	note = {Publisher: Public Library of Science},
	keywords = {Clustering algorithms, Airports, Algorithms, Community structure, Graphs, Mathematical models, Network analysis, Random graphs},
	pages = {e18961},
	file = {Full Text PDF:/Users/admin/Zotero/storage/DJ5V22WZ/Lancichinetti et al. - 2011 - Finding Statistically Significant Communities in N.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/AZ2ZD3S2/article.html:text/html},
}

@inproceedings{li_deeper_2018,
	title = {Deeper {Insights} {Into} {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16098},
	abstract = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection.  In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Qimai and Han, Zhichao and Wu, Xiao-ming},
	month = apr,
	year = {2018},
	file = {Full Text PDF:/Users/admin/Zotero/storage/ZPI4ZQEJ/Li et al. - 2018 - Deeper Insights Into Graph Convolutional Networks .pdf:application/pdf},
}

@article{huang_combining_2020,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.13993},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-11-03},
	journal = {arXiv:2010.13993 [cs]},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	note = {arXiv: 2010.13993},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/2VEE8U5F/Huang et al. - 2020 - Combining Label Propagation and Simple Models Out-.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/JWHYZBPV/2010.html:text/html},
}

@article{wang_unifying_2020,
	title = {Unifying {Graph} {Convolutional} {Neural} {Networks} and {Label} {Propagation}},
	url = {http://arxiv.org/abs/2002.06755},
	abstract = {Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.},
	urldate = {2021-11-03},
	journal = {arXiv:2002.06755 [cs, stat]},
	author = {Wang, Hongwei and Leskovec, Jure},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06755},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/PPUWFNHF/Wang and Leskovec - 2020 - Unifying Graph Convolutional Neural Networks and L.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/SW46X3BB/2002.html:text/html},
}

@inproceedings{iscen_label_2019,
	title = {Label {Propagation} for {Deep} {Semi}-{Supervised} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8954421},
	doi = {10.1109/CVPR.2019.00521},
	abstract = {Semi-supervised learning is becoming increasingly important because it can combine data carefully labeled by humans with abundant unlabeled data to train deep neural networks. Classic methods on semi-supervised learning that have focused on transductive learning have not been fully exploited in the inductive framework followed by modern deep learning. The same holds for the manifold assumption-that similar examples should get the same prediction. In this work, we employ a transductive label propagation method that is based on the manifold assumption to make predictions on the entire dataset and use these predictions to generate pseudo-labels for the unlabeled data and train a deep neural network. At the core of the transductive method lies a nearest neighbor graph of the dataset that we create based on the embeddings of the same network. Therefore our learning process iterates between these two steps. We improve performance on several datasets especially in the few labels regime and show that our work is complementary to current state of the art.},
	urldate = {2025-08-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondrej},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Categorization, Deep Learning, Recognition: Detection, Retrieval},
	pages = {5065--5074},
}


@inproceedings{yang_revisiting_2016,
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {40--48},
	file = {Full Text PDF:/Users/admin/Zotero/storage/2UCYCRWS/Yang et al. - 2016 - Revisiting Semi-Supervised Learning with Graph Emb.pdf:application/pdf},
}

@inproceedings{ye_discrete_2019,
	title = {Discrete {Overlapping} {Community} {Detection} with {Pseudo} {Supervision}},
	doi = {10.1109/ICDM.2019.00081},
	abstract = {Community detection is of significant importance in understanding the structures and functions of networks. Recently, overlapping community detection has drawn much attention due to the ubiquity of overlapping community structures in real-world networks. Nonnegative matrix factorization (NMF), as an emerging standard framework, has been widely employed for overlapping community detection, which obtains nodes' soft community memberships by factorizing the adjacency matrix into low-rank factor matrices. However, in order to determine the ultimate community memberships, we have to post-process the real-valued factor matrix by manually specifying a threshold on it, which is undoubtedly a difficult task. Even worse, a unified threshold may not be suitable for all nodes. To circumvent the cumbersome post-processing step, we propose a novel discrete overlapping community detection approach, i.e., Discrete Nonnegative Matrix Factorization (DNMF), which seeks for a discrete (binary) community membership matrix directly. Thus DNMF is able to assign explicit community memberships to nodes without post-processing. Moreover, DNMF incorporates a pseudo supervision module into it to exploit the discriminative information in an unsupervised manner, which further enhances its robustness. We thoroughly evaluate DNMF using both synthetic and real-world networks. Experiments show that DNMF has the ability to outperform state-of-the-art baseline approaches.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Ye, Fanghua and Chen, Chuan and Zheng, Zibin and Li, Rong-Hua and Yu, Jeffrey Xu},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {community detection, discrete nonnegative matrix factorization, overlapping communities, pseudo supervision},
	pages = {708--717},
	file = {IEEE Xplore Abstract Record:/Users/admin/Zotero/storage/BK5MZ9DY/8970691.html:text/html;IEEE Xplore Full Text PDF:/Users/admin/Zotero/storage/ZT7UDV2V/Ye et al. - 2019 - Discrete Overlapping Community Detection with Pseu.pdf:application/pdf},
}

@inproceedings{talukdar_new_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {New {Regularized} {Algorithms} for {Transductive} {Learning}},
	isbn = {978-3-642-04174-7},
	doi = {10.1007/978-3-642-04174-7_29},
	abstract = {We propose a new graph-based label propagation algorithm for transductive learning. Each example is associated with a vertex in an undirected graph and a weighted edge between two vertices represents similarity between the two corresponding example. We build on Adsorption, a recently proposed algorithm and analyze its properties. We then state our learning algorithm as a convex optimization problem over multi-label assignments and derive an efficient algorithm to solve this problem. We state the conditions under which our algorithm is guaranteed to converge. We provide experimental evidence on various real-world datasets demonstrating the effectiveness of our algorithm over other algorithms for such problems. We also show that our algorithm can be extended to incorporate additional prior information, and demonstrate it with classifying data where the labels are not mutually exclusive.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Talukdar, Partha Pratim and Crammer, Koby},
	editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
	year = {2009},
	keywords = {graph based semi-supervised learning, label propagation, transductive learning},
	pages = {442--457},
	file = {Springer Full Text PDF:/Users/admin/Zotero/storage/DHE9EDKX/Talukdar and Crammer - 2009 - New Regularized Algorithms for Transductive Learni.pdf:application/pdf},
}

@article{gupta_overlapping_2020,
	title = {An overlapping community detection algorithm based on rough clustering of links},
	volume = {125},
	issn = {0169-023X},
	url = {https://www.sciencedirect.com/science/article/pii/S0169023X17304780},
	doi = {10.1016/j.datak.2019.101777},
	abstract = {The growth of networks is prevalent in almost every field due to the digital transformation of consumers, business and society at large. The unfolding of community structure in such real-world complex networks is crucial since it aids in gaining strategic insights leading to informed decisions. Moreover, the co-occurrence of disjoint, overlapping and nested community patterns in such networks demands methodologically rigorous community detection algorithms so as to foster cumulative tradition in data and knowledge engineering. In this paper, we introduce an algorithm for overlapping community detection based on granular information of links and concepts of rough set theory. First, neighborhood links around each pair of nodes are utilized to form initial link subsets. Subsequently, constrained linkage upper approximation of the link subsets is computed iteratively until convergence. The upper approximation subsets obtained during each iteration are constrained and merged using the notion of mutual link reciprocity. The experimental results on ten real-world networks and comparative evaluation with state-of-the-art community detection algorithms demonstrate the effectiveness of the proposed algorithm.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Data \& Knowledge Engineering},
	author = {Gupta, Samrat and Kumar, Pradeep},
	month = jan,
	year = {2020},
	keywords = {Community structure, Clustering, Complex networks, Overlapping communities, Rough sets},
	pages = {101777},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/ZLRXMRJC/Gupta and Kumar - 2020 - An overlapping community detection algorithm based.pdf:application/pdf},
}

@article{liu_deep_2020,
	title = {Deep {Learning} for {Community} {Detection}: {Progress}, {Challenges} and {Opportunities}},
	shorttitle = {Deep {Learning} for {Community} {Detection}},
	url = {http://arxiv.org/abs/2005.08225},
	doi = {10.24963/ijcai.2020/693},
	abstract = {As communities represent similar opinions, similar functions, similar purposes, etc., community detection is an important and extremely useful tool in both scientific inquiry and data analytics. However, the classic methods of community detection, such as spectral clustering and statistical inference, are falling by the wayside as deep learning techniques demonstrate an increasing capacity to handle high-dimensional graph data with impressive performance. Thus, a survey of current progress in community detection through deep learning is timely. Structured into three broad research streams in this domain - deep neural networks, deep graph embedding, and graph neural networks, this article summarizes the contributions of the various frameworks, models, and algorithms in each stream along with the current challenges that remain unsolved and the future research opportunities yet to be explored.},
	urldate = {2021-11-03},
	journal = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
	author = {Liu, Fanzhen and Xue, Shan and Wu, Jia and Zhou, Chuan and Hu, Wenbin and Paris, Cecile and Nepal, Surya and Yang, Jian and Yu, Philip S.},
	month = jul,
	year = {2020},
	note = {arXiv: 2005.08225},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Artificial Intelligence},
	pages = {4981--4987},
	annote = {Comment: Accepted Paper in the 29th International Joint Conference on Artificial Intelligence (IJCAI 20), Survey Track},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/ZC8DJTYN/Liu et al. - 2020 - Deep Learning for Community Detection Progress, C.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/J6HWNKGB/2005.html:text/html},
}

@article{gao_overlapping_2021,
	title = {Overlapping community detection by constrained personalized {PageRank}},
	volume = {173},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421001238},
	doi = {10.1016/j.eswa.2021.114682},
	abstract = {Given a network, local community detection (a.k.a. graph clustering) methods aim at finding communities around the selected initial nodes (also referred to as seeds, starting nodes or core nodes). Methods in this kind successfully address the efficiency problem confronted by global clustering methods. And techniques, such as personalized PageRank and heat kernel diffusion, for ranking the proximity score of vertices nearby with respect to the corresponding starting nodes are developed. However, most of the random-walk based metrics allow a walker to diffuse without any constraint, and the walker can easily run into irrelevant communities. As a result, the corresponding community could include irrelevant high-quality communities (communities with good fitness score) nearby, we refer to the case that a walker goes into irrelevant communities and causes inaccurate expansion of a community as redundant diffusion. In this work, we develop a constrained personalized PageRank method for community expansion to reduce the problem of redundant diffusion. In the mechanism, a walker moves with lower probability to neighbor nodes already in the existing communities, and a walker tends to walk out of the community if the walker walks into an irrelevant community. Extensive experiments on synthetic and large real-world networks demonstrate that the proposed method outperforms approaches in the state of the art by a large margin in accuracy and efficiency.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Expert Systems with Applications},
	author = {Gao, Yang and Yu, Xiangzhan and Zhang, Hongli},
	month = jul,
	year = {2021},
	keywords = {Constrained personalized PageRank, Local community detection, Random walk, Redundant diffusion},
	pages = {114682},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/3VNDT8RA/Gao et al. - 2021 - Overlapping community detection by constrained per.pdf:application/pdf},
}

@article{wang_effective_2021,
	title = {An effective and scalable overlapping community detection approach: {Integrating} social identity model and game theory},
	volume = {390},
	issn = {0096-3003},
	shorttitle = {An effective and scalable overlapping community detection approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300320305567},
	doi = {10.1016/j.amc.2020.125601},
	abstract = {Because of its broad real-life application, community detection (in the realm of a complex network) is an attractive challenge to many researchers. However, current methods fail to reveal the full community structure and its formation process. Thus, here we present SIMGT, an effective and Scalable approach that detects overlapping communities: Integrating social identity Model and Game Theory. Inspired by social identity theory and nodes’ high-order proximities, first we weight and rewire the original network, then we associate each node with a new utility function. Next, we model community formation as a non-cooperative game among all nodes, and we regard the nodes as self-interested players. Further, we use a stochastic gradient-ascent method to update players’ strategies toward different communities, and prove that our game greatly resembles and matches how a potential game works (in the classical sense in game theory), indicating that the Nash equilibrium point must exist. Finally, we implement comprehensive experiments on several synthetic and real-life networks. The results show that whatever weighting strategy we choose, SIMGT can gain better performance on community detection task. In particular, SIMGT achieves a best result when we choose the Jaccard coefficient. After comparing SIMGT with six benchmark algorithms, we obtain convincing results in terms of how well the algorithms reveal communities, as well as algorithms’ scalability.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Applied Mathematics and Computation},
	author = {Wang, Yuyao and Bu, Zhan and Yang, Huan and Li, Hui-Jia and Cao, Jie},
	month = feb,
	year = {2021},
	keywords = {High-order proximities, Non-cooperative game, Overlapping community detection, Social identity model, Stochastic gradient-ascent},
	pages = {125601},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/GR73VQCQ/Wang et al. - 2021 - An effective and scalable overlapping community de.pdf:application/pdf},
}

@article{li_local_2018,
	title = {Local {Spectral} {Clustering} for {Overlapping} {Community} {Detection}},
	volume = {12},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3106370},
	doi = {10.1145/3106370},
	abstract = {Large graphs arise in a number of contexts and understanding their structure and extracting information from them is an important research area. Early algorithms for mining communities have focused on global graph structure, and often run in time proportional to the size of the entire graph. As we explore networks with millions of vertices and find communities of size in the hundreds, it becomes important to shift our attention from macroscopic structure to microscopic structure in large networks. A growing body of work has been adopting local expansion methods in order to identify communities from a few exemplary seed members. In this article, we propose a novel approach for finding overlapping communities called Lemon (Local Expansion via Minimum One Norm). Provided with a few known seeds, the algorithm finds the community by performing a local spectral diffusion. The core idea of Lemon is to use short random walks to approximate an invariant subspace near a seed set, which we refer to as local spectra. Local spectra can be viewed as the low-dimensional embedding that captures the nodes’ closeness in the local network structure. We show that Lemon’s performance in detecting communities is competitive with state-of-the-art methods. Moreover, the running time scales with the size of the community rather than that of the entire graph. The algorithm is easy to implement and is highly parallelizable. We further provide theoretical analysis of the local spectral properties, bounding the measure of tightness of extracted community using the eigenvalues of graph Laplacian. We thoroughly evaluate our approach using both synthetic and real-world datasets across different domains, and analyze the empirical variations when applying our method to inherently different networks in practice. In addition, the heuristics on how the seed set quality and quantity would affect the performance are provided.},
	number = {2},
	urldate = {2021-11-03},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Li, Yixuan and He, Kun and Kloster, Kyle and Bindel, David and Hopcroft, John},
	month = jan,
	year = {2018},
	keywords = {Community detection, graph diffusion, local spectral clustering, random walk, seed set expansion},
	pages = {17:1--17:27},
	file = {Full Text PDF:/Users/admin/Zotero/storage/E6MBELST/Li et al. - 2018 - Local Spectral Clustering for Overlapping Communit.pdf:application/pdf},
}

@article{lu_lpanni_2019,
	title = {{LPANNI}: {Overlapping} {Community} {Detection} {Using} {Label} {Propagation} in {Large}-{Scale} {Complex} {Networks}},
	volume = {31},
	issn = {1558-2191},
	shorttitle = {{LPANNI}},
	doi = {10.1109/TKDE.2018.2866424},
	abstract = {Overlapping community structure is a significant feature of large-scale complex networks. Some existing community detection algorithms cannot be applied to large-scale complex networks due to their high time or space complexity. Label propagation algorithms were proposed for detecting communities in large-scale networks because of their linear time complexity, however most of which can only detect non-overlapping communities, or the results are inaccurate and unstable. Aimed at the defects, we proposed an improved overlapping community detection algorithm, LPANNI (Label Propagation Algorithm with Neighbor Node Influence), which detects overlapping community structures by adopting fixed label propagation sequence based on the ascending order of node importance and label update strategy based on neighbor node influence and historical label preferred strategy. Extensive experimental results in both real networks and synthetic networks show that, LPANNI can significantly improve the accuracy and stability of community detection algorithms based on label propagation in large-scale complex networks. Meanwhile, LPANNI can detect overlapping community structures in large-scale complex networks under linear time complexity.},
	number = {9},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lu, Meilian and Zhang, Zhenglin and Qu, Zhihe and Kang, Yu},
	month = sep,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Clustering algorithms, Measurement, Complex networks, Detection algorithms, Label propagation algorithm, large-scale network, neighbor node influence, overlapping community detection, Stability analysis, Time complexity},
	pages = {1736--1749},
	file = {IEEE Xplore Full Text PDF:/Users/admin/Zotero/storage/KJJMCYJD/Lu et al. - 2019 - LPANNI Overlapping Community Detection Using Labe.pdf:application/pdf},
}

@article{bello-orgaz_multi-objective_2018,
	title = {A {Multi}-{Objective} {Genetic} {Algorithm} for overlapping community detection based on edge encoding},
	volume = {462},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518304559},
	doi = {10.1016/j.ins.2018.06.015},
	abstract = {The Community Detection Problem (CDP) in Social Networks has been widely studied from different areas such as Data Mining, Graph Theory Physics, or Social Network Analysis, among others. This problem tries to divide a graph into different groups of nodes (communities), according to the graph topology. A partition is a division of the graph where each node belongs to only one community. However, a common feature observed in real-world networks is the existence of overlapping communities, where a given node can belong to more than one community. This paper presents a new Multi-Objective Genetic Algorithm (MOGA-OCD) designed to detect overlapping communities, by using measures related to the network connectivity. For this purpose, the proposed algorithm uses a phenotype-type encoding based on the edge information, and a new fitness function focused on optimizing two classical objectives in CDP: the first one is used to maximize the internal connectivity of the communities, whereas the second one is used to minimize the external connections to the rest of the graph. To select the most appropriate metrics for these objectives, a comparative assessment of several connectivity metrics has been carried out using real-world networks. Finally, the algorithm has been evaluated against other well-known algorithms from the state of the art in CDP. The experimental results show that the proposed approach improves overall the accuracy and quality of alternative methods in CDP, showing its effectiveness as a new powerful algorithm for detecting structured overlapping communities.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Information Sciences},
	author = {Bello-Orgaz, Gema and Salcedo-Sanz, Sancho and Camacho, David},
	month = sep,
	year = {2018},
	keywords = {Overlapping community detection, Edge-based encoding, Graph clustering, Multi-Objective genetic algorithms, Network metrics},
	pages = {290--314},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/RRM56TLR/Bello-Orgaz et al. - 2018 - A Multi-Objective Genetic Algorithm for overlappin.pdf:application/pdf},
}

@article{zhang_mixed_2017,
	title = {A {Mixed} {Representation}-{Based} {Multiobjective} {Evolutionary} {Algorithm} for {Overlapping} {Community} {Detection}},
	volume = {47},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2017.2711038},
	abstract = {Designing multiobjective evolutionary algorithms (MOEAs) for community detection in complex networks has attracted much attention of researchers recently. However, most of the existing methods focus on addressing the task of nonoverlapping community detection, where each node must belong to one and only one community. In fact, communities are often overlapped with each other in many real-world networks, thus it is necessary to design overlapping community detection algorithms. To this end, this paper proposes a mixed representation-based MOEA (MR-MOEA) for overlapping community detection. In MR-MOEA, a mixed individual representation scheme is proposed to fast encode and decode the overlapping divisions of complex networks. Specifically, this mixed representation consists of two parts: one represents all potential overlapping nodes and the other delegates all nonoverlapping nodes. These two parts evolve together to detect the overlapping communities of networks based on different updating strategies suggested in MR-MOEA. We verify the effectiveness of the proposed algorithm MR-MOEA on ten real-world complex networks and the experimental results demonstrate that MR-MOEA is superior over six representative algorithms for overlapping community detection.},
	number = {9},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhang, Lei and Pan, Hebin and Su, Yansen and Zhang, Xingyi and Niu, Yunyun},
	month = sep,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Cybernetics},
	keywords = {Complex networks, Detection algorithms, overlapping community detection, Complex network, Cybernetics, Encoding, Evolutionary computation, individual representation scheme, Lapping, multiobjective evolutionary algorithm (MOEA), Optimization},
	pages = {2703--2716},
	file = {IEEE Xplore Abstract Record:/Users/admin/Zotero/storage/XWU888JE/7947122.html:text/html;IEEE Xplore Full Text PDF:/Users/admin/Zotero/storage/9MGQF3VK/Zhang et al. - 2017 - A Mixed Representation-Based Multiobjective Evolut.pdf:application/pdf},
}

@inproceedings{yang_overlapping_2013,
	address = {New York, NY, USA},
	series = {{WSDM} '13},
	title = {Overlapping community detection at scale: a nonnegative matrix factorization approach},
	isbn = {978-1-4503-1869-3},
	shorttitle = {Overlapping community detection at scale},
	url = {https://doi.org/10.1145/2433396.2433471},
	doi = {10.1145/2433396.2433471},
	abstract = {Network communities represent basic structures for understanding the organization of real-world networks. A community (also referred to as a module or a cluster) is typically thought of as a group of nodes with more connections amongst its members than between its members and the remainder of the network. Communities in networks also overlap as nodes belong to multiple clusters at once. Due to the difficulties in evaluating the detected communities and the lack of scalable algorithms, the task of overlapping community detection in large networks largely remains an open problem. In this paper we present BIGCLAM (Cluster Affiliation Model for Big Networks), an overlapping community detection method that scales to large networks of millions of nodes and edges. We build on a novel observation that overlaps between communities are densely connected. This is in sharp contrast with present community detection methods which implicitly assume that overlaps between communities are sparsely connected and thus cannot properly extract overlapping communities in networks. In this paper, we develop a model-based community detection algorithm that can detect densely overlapping, hierarchically nested as well as non-overlapping communities in massive networks. We evaluate our algorithm on 6 large social, collaboration and information networks with ground-truth community information. Experiments show state of the art performance both in terms of the quality of detected communities as well as in speed and scalability of our algorithm.},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the sixth {ACM} international conference on {Web} search and data mining},
	publisher = {Association for Computing Machinery},
	author = {Yang, Jaewon and Leskovec, Jure},
	month = feb,
	year = {2013},
	keywords = {overlapping community detection, matrix factorization, network communities},
	pages = {587--596},
	file = {Full Text PDF:/Users/admin/Zotero/storage/V5W7F3ZL/Yang and Leskovec - 2013 - Overlapping community detection at scale a nonneg.pdf:application/pdf},
}

@article{xie_overlapping_2013,
	title = {Overlapping community detection in networks: {The} state-of-the-art and comparative study},
	volume = {45},
	issn = {0360-0300},
	shorttitle = {Overlapping community detection in networks},
	url = {https://doi.org/10.1145/2501654.2501657},
	doi = {10.1145/2501654.2501657},
	abstract = {This article reviews the state-of-the-art in overlapping community detection algorithms, quality measures, and benchmarks. A thorough comparison of different algorithms (a total of fourteen) is provided. In addition to community-level evaluation, we propose a framework for evaluating algorithms' ability to detect overlapping nodes, which helps to assess overdetection and underdetection. After considering community-level detection performance measured by normalized mutual information, the Omega index, and node-level detection performance measured by F-score, we reached the following conclusions. For low overlapping density networks, SLPA, OSLOM, Game, and COPRA offer better performance than the other tested algorithms. For networks with high overlapping density and high overlapping diversity, both SLPA and Game provide relatively stable performance. However, test results also suggest that the detection in such networks is still not yet fully resolved. A common feature observed by various algorithms in real-world networks is the relatively small fraction of overlapping nodes (typically less than 30\%), each of which belongs to only 2 or 3 communities.},
	number = {4},
	urldate = {2021-11-03},
	journal = {ACM Computing Surveys},
	author = {Xie, Jierui and Kelley, Stephen and Szymanski, Boleslaw K.},
	month = aug,
	year = {2013},
	keywords = {social networks, Overlapping community detection},
	pages = {43:1--43:35},
	file = {Full Text PDF:/Users/admin/Zotero/storage/SGCWX4TI/Xie et al. - 2013 - Overlapping community detection in networks The s.pdf:application/pdf},
}

@inproceedings{zheng_distdgl_2020,
	title = {{DistDGL}: {Distributed} {Graph} {Neural} {Network} {Training} for {Billion}-{Scale} {Graphs}},
	shorttitle = {{DistDGL}},
	doi = {10.1109/IA351965.2020.00011},
	abstract = {Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability. We demonstrate our optimizations on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines.},
	booktitle = {2020 {IEEE}/{ACM} 10th {Workshop} on {Irregular} {Applications}: {Architectures} and {Algorithms} ({IA3})},
	author = {Zheng, Da and Ma, Chao and Wang, Minjie and Zhou, Jinjing and Su, Qidong and Song, Xiang and Gan, Quan and Zhang, Zheng and Karypis, George},
	month = nov,
	year = {2020},
	keywords = {Training, Computational modeling, Graph neural networks, Libraries, Load management, Memory management, Scalability},
	pages = {36--44},
	file = {IEEE Xplore Abstract Record:/Users/admin/Zotero/storage/T7NT57WR/9407264.html:text/html;IEEE Xplore Full Text PDF:/Users/admin/Zotero/storage/YM58X388/Zheng et al. - 2020 - DistDGL Distributed Graph Neural Network Training.pdf:application/pdf},
}

@inproceedings{lerer_pytorch-biggraph_2019,
  title={{PyTorch-BigGraph: A Large-scale Graph Embedding System}},
  author={Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},
  booktitle={Proceedings of the 2nd SysML Conference},
  month = apr,
  year={2019},
  address={Palo Alto, CA, USA},
  url = {https://mlsys.org/Conferences/2019/doc/2019/71.pdf},
  urldate = {2022-09-20},
}

@article{hogan_knowledge_2021,
	title = {Knowledge {Graphs}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3447772},
	doi = {10.1145/3447772},
	abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
	number = {4},
	urldate = {2022-08-23},
	journal = {ACM Computing Surveys},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, José Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = jul,
	year = {2021},
	keywords = {embeddings, graph algorithms, graph databases, graph neural networks, graph query languages, Knowledge graphs, ontologies, rule mining, shapes},
	pages = {71:1--71:37},
	annote = {\_eprint: 2003.02320},
	annote = {\_eprint: 2003.02320},
	file = {Full Text:/Users/admin/Zotero/storage/JWAMR8YF/Hogan et al. - 2020 - Knowledge Graphs.pdf:application/pdf;Full Text PDF:/Users/admin/Zotero/storage/I3QBAWS3/Hogan et al. - 2021 - Knowledge Graphs.pdf:application/pdf;Full Text PDF:/Users/admin/Zotero/storage/CFLHVP74/Hogan et al. - 2020 - Knowledge Graphs.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/JXWYFZ6E/2003.html:text/html},
}

@misc{ren_smore_2021,
	title = {{SMORE}: {Knowledge} {Graph} {Completion} and {Multi}-hop {Reasoning} in {Massive} {Knowledge} {Graphs}},
	shorttitle = {{SMORE}},
	url = {http://arxiv.org/abs/2110.14890},
	doi = {10.48550/arXiv.2110.14890},
	abstract = {Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500x larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.},
	urldate = {2022-06-16},
	publisher = {arXiv},
	author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2110.14890
arXiv:2110.14890 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, and Cluster Computing, Computer Science - Distributed, Parallel},
	annote = {Number: arXiv:2110.14890 arXiv:2110.14890 [cs]},
	file = {arXiv Fulltext PDF:/Users/admin/Zotero/storage/WZ2QHFKT/Ren et al. - 2021 - SMORE Knowledge Graph Completion and Multi-hop Re.pdf:application/pdf;arXiv.org Snapshot:/Users/admin/Zotero/storage/78TXXFPG/2110.html:text/html},
}

@article{csardi_igraph_2005,
	title = {The {Igraph} {Software} {Package} for {Complex} {Network} {Research}},
	volume = {Complex Systems},
	journal = {InterJournal},
	author = {Csardi, Gabor and Nepusz, Tamas},
	month = nov,
	year = {2005},
	pages = {1695},
	file = {Full Text PDF:/Users/admin/Zotero/storage/GQJ22DYQ/Csardi and Nepusz - 2005 - The Igraph Software Package for Complex Network Re.pdf:application/pdf},
}

@article{traag_louvain_2019,
	title = {From {Louvain} to {Leiden}: guaranteeing well-connected communities},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {From {Louvain} to {Leiden}},
	url = {https://www.nature.com/articles/s41598-019-41695-z},
	doi = {10.1038/s41598-019-41695-z},
	abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
	language = {en},
	number = {1},
	urldate = {2022-05-29},
	journal = {Scientific Reports},
	author = {Traag, V. A. and Waltman, L. and van Eck, N. J.},
	month = mar,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Applied mathematics},
	pages = {5233},
	annote = {Publisher: Nature Publishing Group},
	file = {Full Text PDF:/Users/admin/Zotero/storage/GDPCHQL8/Traag et al. - 2019 - From Louvain to Leiden guaranteeing well-connecte.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/N83VGGLB/s41598-019-41695-z.html:text/html},
}

@inproceedings{epasto_ego-splitting_2017,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {Ego-{Splitting} {Framework}: from {Non} {Overlapping} to {Overlapping} {Clusters}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {Ego-{Splitting} {Framework}},
	url = {https://doi.org/10.1145/3097983.3098054},
	doi = {10.1145/3097983.3098054},
	abstract = {We propose ego-splitting, a new framework for detecting clusters in complex networks which leverage the local structures known as ego-nets (i.e. the subgraph induced by the neighborhood of each node) to de-couple overlapping clusters. Ego-splitting is a highly scalable and flexible framework, with provable theoretical guarantees, that reduces the complex overlapping clustering problem to a simpler and more amenable non-overlapping (partitioning) problem. We can scale community detection to graphs with tens of billions of edges and outperform previous solutions based on ego-nets analysis. More precisely, our framework works in two steps: a local ego-net analysis phase, and a global graph partitioning phase. In the local step, we first partition the nodes' ego-nets using a partitioning algorithm. We then use the computed clusters to split each node into its persona nodes that represent the instantiations of the node in its communities. Finally, in the global step, we partition the newly created graph to obtain an overlapping clustering of the original graph.},
	urldate = {2022-05-29},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Epasto, Alessandro and Lattanzi, Silvio and Paes Leme, Renato},
	month = aug,
	year = {2017},
	keywords = {ego-nets, large-scale graph algorithms, overlapping clustering},
	pages = {145--154},
	file = {Full Text PDF:/Users/admin/Zotero/storage/4VKG5726/Epasto et al. - 2017 - Ego-Splitting Framework from Non-Overlapping to O.pdf:application/pdf},
}

@article{wang_community_2017,
	title = {Community {Preserving} {Network} {Embedding}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10488},
	abstract = {Network embedding, aiming to learn the low-dimensional representations of nodes in networks, is of paramount importance in many real applications. One basic requirement of network embedding is to preserve the structure and inherent properties of the networks. While previous network embedding methods primarily preserve the microscopic structure, such as the first- and second-order proximities of nodes, the mesoscopic community structure, which is one of the most prominent feature of networks, is largely ignored. In this paper, we propose a novel Modularized Nonnegative Matrix Factorization (M-NMF) model to incorporate the community structure into network embedding. We exploit the consensus relationship between the representations of nodes and community structure, and then jointly optimize NMF based representation learning model and modularity based community detection model in a unified framework, which enables the learned representations of nodes to preserve both of the microscopic and community structures. We also provide efficient updating rules to infer the parameters of our model, together with the correctness and convergence guarantees. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over the state-of-the-arts.},
	language = {en},
	number = {1},
	urldate = {2021-11-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Xiao and Cui, Peng and Wang, Jing and Pei, Jian and Zhu, Wenwu and Yang, Shiqiang},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {nonnegative matrix factorization},
	file = {Full Text PDF:/Users/admin/Zotero/storage/C73H52SE/Wang et al. - 2017 - Community Preserving Network Embedding.pdf:application/pdf},
}

@article{collins_omega_1988,
	title = {Omega: {A} {General} {Formulation} of the {Rand} {Index} of {Cluster} {Recovery} {Suitable} for {Non}-disjoint {Solutions}},
	volume = {23},
	issn = {0027-3171},
	shorttitle = {Omega},
	doi = {10.1207/s15327906mbr2302_6},
	abstract = {Cluster recovery indices are more important than ever, because of the necessity for comparing the large number of clustering procedures available today. Of the cluster recovery indices prominent in contemporary literature, the Hubert and Arabie (1985) adjustment to the Rand index (1971) has been demonstrated to have the most desirable properties (Milligan \& Cooper, 1986). However, use of the Hubert and Arabie adjustment to the Rand index is limited to cluster solutions involving non-overlapping, or disjoint, clusters. The present paper introduces a generalization of the Hubert and Arabie adjusted Rand index. This generalization, called the Omega index, can be applied to situations where both, one, or neither of the solutions being compared is non-disjoint. In the special case where both solutions are disjoint, the Omega index is equivalent to the Hubert and Arabie adjusted Rand index.},
	language = {eng},
	number = {2},
	journal = {Multivariate Behavioral Research},
	author = {Collins, L. M. and Dent, C. W.},
	month = apr,
	year = {1988},
	pmid = {26764947},
	pages = {231--242},
	file = {collins1988.pdf:D\:\\Users\\Andrej Janchevski\\Pornici\\PhD\\Semester project - GG-GAN partitions\\Report\\collins1988.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/4P932JAW/s15327906mbr2302_6.html:text/html},
}

@article{hakimi_realizability_1963,
	title = {On {Realizability} of a {Set} of {Integers} as {Degrees} of the {Vertices} of a {Linear} {Graph} {II}. {Uniqueness}},
	volume = {11},
	issn = {0368-4245, 2168-3484},
	url = {http://epubs.siam.org/doi/10.1137/0111010},
	doi = {10.1137/0111010},
	language = {en},
	number = {1},
	urldate = {2022-01-10},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Hakimi, S. L.},
	month = mar,
	year = {1963},
	pages = {135--147},
}

@article{brin_anatomy_1998,
	series = {Proceedings of the {Seventh} {International} {World} {Wide} {Web} {Conference}},
	title = {The anatomy of a large-scale hypertextual {Web} search engine},
	volume = {30},
	issn = {0169-7552},
	url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
	doi = {10.1016/S0169-7552(98)00110-X},
	abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine — the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
	language = {en},
	number = {1},
	urldate = {2022-05-29},
	journal = {Computer Networks and ISDN Systems},
	author = {Brin, Sergey and Page, Lawrence},
	month = apr,
	year = {1998},
	keywords = {Google, Information retrieval, PageRank, Search engines, World Wide Web},
	pages = {107--117},
	annote = {Publisher: Elsevier},
	file = {ScienceDirect Snapshot:/Users/admin/Zotero/storage/84CGWRZ2/S016975529800110X.html:text/html},
}

@inproceedings{xiong_deeppath_2017,
	address = {Copenhagen, Denmark},
	title = {{DeepPath}: {A} {Reinforcement} {Learning} {Method} for {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{DeepPath}},
	url = {https://aclanthology.org/D17-1060},
	doi = {10.18653/v1/D17-1060},
	abstract = {We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
	month = sep,
	year = {2017},
	pages = {564--573},
	file = {Full Text PDF:/Users/admin/Zotero/storage/X8QQ22IB/Xiong et al. - 2017 - DeepPath A Reinforcement Learning Method for Know.pdf:application/pdf},
}

@article{dettmers_convolutional_2018,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11573},
	doi = {10.1609/aaai.v32i1.11573},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models — which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree — which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set — however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets — deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {convolution},
	annote = {\_eprint: 1707.01476},
	file = {Full Text:/Users/admin/Zotero/storage/633YLC6Z/Dettmers et al. - 2017 - Convolutional 2D Knowledge Graph Embeddings.pdf:application/pdf;Full Text PDF:/Users/admin/Zotero/storage/QPCIVUI3/Dettmers et al. - 2018 - Convolutional 2D Knowledge Graph Embeddings.pdf:application/pdf},
}

@inproceedings{toutanova_observed_2015,
	address = {Beijing, China},
	title = {Observed versus latent features for knowledge base and text inference},
	url = {https://aclanthology.org/W15-4007},
	doi = {10.18653/v1/W15-4007},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 3rd {Workshop} on {Continuous} {Vector} {Space} {Models} and their {Compositionality}},
	publisher = {Association for Computational Linguistics},
	author = {Toutanova, Kristina and Chen, Danqi},
	month = jul,
	year = {2015},
	pages = {57--66},
	file = {Full Text PDF:/Users/admin/Zotero/storage/P8CXBHII/Toutanova and Chen - 2015 - Observed versus latent features for knowledge base.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Springer},
	pages = {5--32},
	annote = {Publisher: Springer},
	annote = {Publisher: Springer},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
	file = {Full Text:/Users/admin/Zotero/storage/ZFIXPZPI/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{barabasi_emergence_1999,
	title = {Emergence of scaling in random networks},
	volume = {286},
	number = {5439},
	journal = {science},
	author = {Barabási, Albert-László and Albert, Réka},
	year = {1999},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {509--512},
	annote = {Publisher: American Association for the Advancement of Science},
	annote = {Publisher: American Association for the Advancement of Science},
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	volume = {abs/1609.02907},
	url = {http://arxiv.org/abs/1609.02907},
	journal = {CoRR},
	author = {Kipf, Thomas N. and Welling, Max},
	year = {2016},
	note = {\_eprint: 1609.02907},
	annote = {\_eprint: 1609.02907},
	annote = {\_eprint: 1609.02907},
	file = {Full Text:/Users/admin/Zotero/storage/9KWKAD8N/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf},
}

@article{glorot_semantic_2014,
	title = {A semantic matching energy function for learning with multi-relational data},
	volume = {94},
	number = {2},
	journal = {Machine Learning},
	author = {Glorot, Xavier and Bordes, Antoine and Weston, Jason and Bengio, Yoshua},
	year = {2014},
	note = {Publisher: Springer},
	pages = {233--259},
	annote = {Publisher: Springer},
	annote = {Publisher: Springer},
}

@article{reddi_convergence_2019,
	title = {On the {Convergence} of {Adam} and {Beyond}},
	volume = {abs/1904.09237},
	url = {http://arxiv.org/abs/1904.09237},
	journal = {CoRR},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	year = {2019},
	note = {\_eprint: 1904.09237},
	annote = {\_eprint: 1904.09237},
	annote = {\_eprint: 1904.09237},
	file = {Full Text:/Users/admin/Zotero/storage/YQWWGMWJ/Reddi et al. - 2019 - On the Convergence of Adam and Beyond.pdf:application/pdf},
}

@article{kramer_nonlinear_1991,
	title = {Nonlinear principal component analysis using autoassociative neural networks},
	volume = {37},
	number = {2},
	journal = {AIChE journal},
	author = {Kramer, Mark A},
	year = {1991},
	note = {Publisher: Wiley Online Library},
	pages = {233--243},
	annote = {Publisher: Wiley Online Library},
	annote = {Publisher: Wiley Online Library},
}

@article{luong_effective_2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	volume = {abs/1508.04025},
	url = {http://arxiv.org/abs/1508.04025},
	journal = {CoRR},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	note = {\_eprint: 1508.04025},
	annote = {\_eprint: 1508.04025},
	file = {Full Text:/Users/admin/Zotero/storage/YI3Z42AT/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	volume = {abs/17-06.03762},
	url = {http://arxiv.org/abs/1706.03762},
	journal = {CoRR},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	year = {2017},
	note = {\_eprint: 1706.03762},
	annote = {\_eprint: 1706.03762},
	file = {Full Text:/Users/admin/Zotero/storage/R3MBD9MB/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	volume = {abs/1207.0580},
	url = {http://arxiv.org/abs/1207.0580},
	journal = {CoRR},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2012},
	note = {\_eprint: 1207.0580},
	annote = {\_eprint: 1207.0580},
	file = {Full Text:/Users/admin/Zotero/storage/3PXCI8CV/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf:application/pdf},
}

@article{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	volume = {abs/14-10.5401},
	url = {http://arxiv.org/abs/1410.5401},
	journal = {CoRR},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	year = {2014},
	note = {\_eprint: 1410.5401},
	annote = {\_eprint: 1410.5401},
	file = {Full Text:/Users/admin/Zotero/storage/F9WNAGK5/Graves et al. - 2014 - Neural Turing Machines.pdf:application/pdf},
}

@article{grover_node2vec_2016,
	title = {node2vec: {Scalable} {Feature} {Learning} for {Networks}},
	volume = {abs/1607.00653},
	url = {http://arxiv.org/abs/1607.00653},
	journal = {CoRR},
	author = {Grover, Aditya and Leskovec, Jure},
	year = {2016},
	note = {\_eprint: 1607.00653},
	annote = {\_eprint: 1607.00653},
	file = {Full Text:/Users/admin/Zotero/storage/XQH7IDLU/Grover and Leskovec - 2016 - node2vec Scalable Feature Learning for Networks.pdf:application/pdf},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	volume = {abs/1502.03167},
	url = {http://arxiv.org/abs/1502.03167},
	journal = {CoRR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {\_eprint: 1502.03167},
	annote = {\_eprint: 1502.03167},
	file = {Full Text:/Users/admin/Zotero/storage/SGIMYUZ3/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{blondel_fast_2008,
	title = {Fast unfolding of communities in large networks},
	volume = {2008},
	url = {https://doi.org/10.1088/1742-5468/2008/10/p10008},
	doi = {10.1088/1742-5468/2008/10/p10008},
	abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.},
	number = {10},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	month = oct,
	year = {2008},
	note = {Publisher: IOP Publishing},
	pages = {P10008},
	annote = {Publisher: IOP Publishing},
	file = {Submitted Version:/Users/admin/Zotero/storage/QP2FV3WX/Blondel et al. - 2008 - Fast unfolding of communities in large networks.pdf:application/pdf},
}

@article{girvan_community_2002,
	title = {Community structure in social and biological networks},
	volume = {99},
	issn = {0027-8424},
	url = {https://www.pnas.org/content/99/12/7821},
	doi = {10.1073/pnas.122653799},
	abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.},
	number = {12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Girvan, M. and Newman, M. E. J.},
	year = {2002},
	note = {Publisher: National Academy of Sciences
\_eprint: https://www.pnas.org/content/99/12/7821.full.pdf},
	pages = {7821--7826},
	annote = {Publisher: National Academy of Sciences \_eprint: https://www.pnas.org/content/99/12/7821.full.pdf},
	file = {Full Text:/Users/admin/Zotero/storage/FFKERIFZ/Girvan and Newman - 2002 - Community structure in social and biological netwo.pdf:application/pdf},
}

@article{lee_i-mix_2020,
	title = {i-{Mix}: {A} {Strategy} for {Regularizing} {Contrastive} {Representation} {Learning}},
	volume = {abs/2010.08887},
	url = {https://arxiv.org/abs/2010.08887},
	journal = {CoRR},
	author = {Lee, Kibok and Zhu, Yian and Sohn, Kihyuk and Li, Chun-Liang and Shin, Jinwoo and Lee, Honglak},
	year = {2020},
	note = {\_eprint: 2010.08887},
	annote = {\_eprint: 2010.08887},
	file = {Full Text:/Users/admin/Zotero/storage/64QJEENP/Lee et al. - 2020 - i-Mix A Strategy for Regularizing Contrastive Rep.pdf:application/pdf},
}

@article{chodrow_generative_2021,
	title = {Generative hypergraph clustering: from blockmodels to modularity},
	volume = {abs/2101.09611},
	url = {https://arxiv.org/abs/2101.09611},
	journal = {CoRR},
	author = {Chodrow, Philip S. and Veldt, Nate and Benson, Austin R.},
	year = {2021},
	note = {\_eprint: 2101.09611},
	annote = {\_eprint: 2101.09611},
	file = {Full Text:/Users/admin/Zotero/storage/SQDRYQ68/Chodrow et al. - 2021 - Generative hypergraph clustering from blockmodels.pdf:application/pdf},
}

@article{sun_rotate_2019,
	title = {{RotatE}: {Knowledge} {Graph} {Embedding} by {Relational} {Rotation} in {Complex} {Space}},
	volume = {abs/1902.10197},
	url = {http://arxiv.org/abs/1902.10197},
	journal = {CoRR},
	author = {Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
	year = {2019},
	note = {\_eprint: 1902.10197},
	annote = {\_eprint: 1902.10197},
	file = {Full Text:/Users/admin/Zotero/storage/TWCQKJPY/Sun et al. - 2019 - RotatE Knowledge Graph Embedding by Relational Ro.pdf:application/pdf},
}

@article{shang_end--end_2019,
	title = {End-to-{End} {Structure}-{Aware} {Convolutional} {Networks} for {Knowledge} {Base} {Completion}},
	volume = {33},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4164},
	doi = {10.1609/aaai.v33i01.33013060},
	number = {01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Shang, Chao and Tang, Yun and Huang, Jing and Bi, Jinbo and He, Xiaodong and Zhou, Bowen},
	month = jul,
	year = {2019},
	pages = {3060--3067},
	file = {Full Text:/Users/admin/Zotero/storage/33GLN7AF/Shang et al. - 2019 - End-to-End Structure-Aware Convolutional Networks .pdf:application/pdf},
}

@inproceedings{cen_representation_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Representation {Learning} for {Attributed} {Multiplex} {Heterogeneous} {Network}},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330964},
	doi = {10.1145/3292500.3330964},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Cen, Yukuo and Zou, Xu and Zhang, Jianwei and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
	year = {2019},
	note = {event-place: Anchorage, AK, USA},
	keywords = {network embedding, heterogeneous network, multiplex network},
	pages = {1358--1368},
	file = {Submitted Version:/Users/admin/Zotero/storage/8PIQHKEH/Cen et al. - 2019 - Representation Learning for Attributed Multiplex H.pdf:application/pdf},
}

@article{yang_embedding_2014,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	volume = {abs/1412.6575},
	url = {https://api.semanticscholar.org/CorpusID:2768038},
	journal = {CoRR},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	year = {2014},
	file = {Full Text:/Users/admin/Zotero/storage/8C8Z3X7Q/Yang et al. - 2014 - Embedding Entities and Relations for Learning and .pdf:application/pdf},
}

@inproceedings{trouillon_complex_2016,
	address = {New York, New York, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Complex {Embeddings} for {Simple} {Link} {Prediction}},
	volume = {48},
	url = {https://proceedings.mlr.press/v48/trouillon16.html},
	abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Trouillon, Théo and Welbl, Johannes and Riedel, Sebastian and Gaussier, Eric and Bouchard, Guillaume},
	editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
	month = jun,
	year = {2016},
	pages = {2071--2080},
	file = {Full Text:/Users/admin/Zotero/storage/YSPHMGN5/Trouillon et al. - 2016 - Complex Embeddings for Simple Link Prediction.pdf:application/pdf},
}

@inproceedings{hamilton_embedding_2018,
	title = {Embedding {Logical} {Queries} on {Knowledge} {Graphs}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/ef50c335cca9f340bde656363ebd02fd-Abstract.html},
	abstract = {Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict "em what drugs are likely to target proteins involved with both diseases X and Y?" -- a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries -- a flexible but tractable subset of first-order logic -- on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.},
	urldate = {2023-11-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hamilton, Will and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/admin/Zotero/storage/EN2XZTGK/Hamilton et al. - 2018 - Embedding Logical Queries on Knowledge Graphs.pdf:application/pdf},
}

@article{traag_narrow_2011,
	title = {Narrow scope for resolution-limit-free community detection},
	volume = {84},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.84.016114},
	doi = {10.1103/PhysRevE.84.016114},
	abstract = {Detecting communities in large networks has drawn much attention over the years. While modularity remains one of the more popular methods of community detection, the so-called resolution limit remains a significant drawback. To overcome this issue, it was recently suggested that instead of comparing the network to a random null model, as is done in modularity, it should be compared to a constant factor. However, it is unclear what is meant exactly by “resolution-limit-free,” that is, not suffering from the resolution limit. Furthermore, the question remains what other methods could be classified as resolution-limit-free. In this paper we suggest a rigorous definition and derive some basic properties of resolution-limit-free methods. More importantly, we are able to prove exactly which class of community detection methods are resolution-limit-free. Furthermore, we analyze which methods are not resolution-limit-free, suggesting there is only a limited scope for resolution-limit-free community detection methods. Finally, we provide such a natural formulation, and show it performs superbly.},
	number = {1},
	urldate = {2023-11-17},
	journal = {Physical Review E},
	author = {Traag, V. A. and Van Dooren, P. and Nesterov, Y.},
	month = jul,
	year = {2011},
	note = {Publisher: American Physical Society},
	pages = {016114},
	annote = {Publisher: American Physical Society},
	file = {APS Snapshot:/Users/admin/Zotero/storage/IHG639N9/PhysRevE.84.html:text/html;Full Text PDF:/Users/admin/Zotero/storage/CFXD5AKD/Traag et al. - 2011 - Narrow scope for resolution-limit-free community d.pdf:application/pdf;Submitted Version:/Users/admin/Zotero/storage/UGAMV6KM/Traag et al. - 2011 - Narrow scope for resolution-limit-free community d.pdf:application/pdf},
}

@techreport{karypis_metis_1997,
	type = {Report},
        institution = {University of Minnesota},
	title = {{METIS}: {A} {Software} {Package} for {Partitioning} {Unstructured} {Graphs}, {Partitioning} {Meshes}, and {Computing} {Fill}-{Reducing} {Orderings} of {Sparse} {Matrices}},
	shorttitle = {{METIS}},
	url = {http://conservancy.umn.edu/handle/11299/215346},
	abstract = {Metis is copyrighted by the regents of the University of Minnesota. This work was supponed by IST/BMDO through Army Research Office
contract DA/DAAH04-93-G-0080. and by Army High Performance Computing Research Center under the auspices of the Department of the Army.
Anny Research Laboratory cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does
not necessarily reflect the position or the policy of lhe government, and no official endorsement should be inferred. Access to computing facilities
were provided by Minnesota Supercomputer Institute, Cray Research Inc, and by the Pittsburgh Supercomputing Center.},
	language = {en},
	urldate = {2023-11-17},
	author = {Karypis, George and Kumar, Vipin},
	year = {1997},
	note = {Accepted: 2020-09-02T15:04:02Z},
}

@misc{ferludin_tf-gnn_2022,
	title = {{TF}-{GNN}: {Graph} {Neural} {Networks} in {TensorFlow}},
	shorttitle = {{TF}-{GNN}},
	url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220703522F},
	doi = {10.48550/arXiv.2207.03522},
	abstract = {TensorFlow-GNN (TF-GNN) is a scalable library for Graph Neural Networks in TensorFlow. It is designed from the bottom up to support the kinds of rich heterogeneous graph data that occurs in today's information ecosystems. In addition to enabling machine learning researchers and advanced developers, TF-GNN offers low-code solutions to empower the broader developer community in graph learning. Many production models at Google use TF-GNN, and it has been recently released as an open source project. In this paper we describe the TF-GNN data model, its Keras message passing API, and relevant capabilities such as graph sampling and distributed training.},
	urldate = {2023-11-17},
	author = {Ferludin, Oleksandr and Eigenwillig, Arno and Blais, Martin and Zelle, Dustin and Pfeifer, Jan and Sanchez-Gonzalez, Alvaro and Sibon Li, Wai Lok and Abu-El-Haija, Sami and Battaglia, Peter and Bulut, Neslihan and Halcrow, Jonathan and Gonçalves de Almeida, Filipe Miguel and Gonnet, Pedro and Jiang, Liangze and Kothari, Parth and Lattanzi, Silvio and Linhares, André and Mayer, Brandon and Mirrokni, Vahab and Palowitch, John and Paradkar, Mihir and She, Jennifer and Tsitsulin, Anton and Villela, Kevin and Wang, Lisa and Wong, David and Perozzi, Bryan},
	month = jul,
	year = {2022},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2022arXiv220703522F},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Neural and Evolutionary Computing, Physics - Physics and Society},
	annote = {Publication Title: arXiv e-prints ADS Bibcode: 2022arXiv220703522F},
	file = {Full Text PDF:/Users/admin/Zotero/storage/P854SVCW/Ferludin et al. - 2022 - TF-GNN Graph Neural Networks in TensorFlow.pdf:application/pdf},
}

@article{roghani_pldls_2021,
	title = {{PLDLS}: {A} novel parallel label diffusion and label {Selection}-based community detection algorithm based on {Spark} in social networks},
	volume = {183},
	issn = {0957-4174},
	shorttitle = {{PLDLS}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421008034},
	doi = {10.1016/j.eswa.2021.115377},
	abstract = {Parallel and distributed community detection in large-scale complex networks, such as social networks, is a challenging task. Parallel and distributed algorithm with high accuracy and low computational complexity is one of the essential issues in the community detection field. In this paper, we propose a novel fast, and accurate Spark-based parallel label diffusion and label selection-based (PLDLS) community detection algorithm with two-step of label diffusion of core nodes along with a new label selection (propagation) method. We have used multi-factor criteria for computing node's importance and adopted a new method for selecting core nodes. In the first phase, utilizing the fact that nodes forming triangles, tend to be in the same community, parallel label diffusion of core nodes is performed to diffuse labels up to two levels. In the second phase, through an iterative and parallel process, the most appropriate labels are assigned to the remaining nodes. PLDLS proposes an improved robust version of LPA by putting aside randomness parameter tuning. Furthermore, we utilize a fast and parallel merge phase to get even more dense and accurate communities. Conducted experiments on real-world and artificial networks, indicates the better accuracy and low execution time of PLDLS in comparison with other examined methods.},
	urldate = {2023-11-14},
	journal = {Expert Systems with Applications},
	author = {Roghani, Hamid and Bouyer, Asgarali and Nourani, Esmaeil},
	month = nov,
	year = {2021},
	keywords = {Label diffusion, Label selection, Local similarity, Parallel community detection, Social networks, Spark},
	pages = {115377},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/JWSJ2LQG/Roghani et al. - 2021 - PLDLS A novel parallel label diffusion and label .pdf:application/pdf},
}

@article{fionda_community_2022,
	title = {Community deception: from undirected to directed networks},
	volume = {12},
	issn = {1869-5469},
	shorttitle = {Community deception},
	url = {https://doi.org/10.1007/s13278-022-00896-7},
	doi = {10.1007/s13278-022-00896-7},
	abstract = {Community deception is about hiding a target community that wants to remain below the radar of community detection algorithms. The goal is to devise algorithms that, given a maximum number of updates (e.g., edge additions and removal), strive to find the best way to perform such updates in order to hide the target community inside the community structure found by a detection algorithm. So far, community deception has only been studied for undirected networks, although many real-world networks (e.g., Twitter) are directed. One way to overcome this problem would be to treat the network as undirected. However, this approach discards potentially helpful information in the edge directions (e.g., A follows B does not imply that B follows A). The aim of this paper is threefold. First, to give an account of the state-of-the-art community deception techniques in undirected networks underlying their peculiarities. Second, to investigate the community deception problem in directed networks and to show how deception techniques proposed for undirected networks should be modified and adapted to work on directed networks. Third, to evaluate deception techniques both in undirected and directed networks. Our experimental evaluation on a variety of (large) directed networks shows that techniques that work well for undirected networks fail short when directly applied to directed networks, thus underlying the need for specific approaches.},
	language = {en},
	number = {1},
	urldate = {2023-11-14},
	journal = {Social Network Analysis and Mining},
	author = {Fionda, Valeria and Madi, Saif Aldeen and Pirrò, Giuseppe},
	month = jul,
	year = {2022},
	pages = {74},
	file = {Full Text PDF:/Users/admin/Zotero/storage/G7GQ927M/Fionda et al. - 2022 - Community deception from undirected to directed n.pdf:application/pdf},
}

@article{karypis_fast_1998,
	title = {A {Fast} and {High} {Quality} {Multilevel} {Scheme} for {Partitioning} {Irregular} {Graphs}},
	volume = {20},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/S1064827595287997},
	doi = {10.1137/S1064827595287997},
	abstract = {In this paper we present a parallel formulation of a multilevel k-way graph partitioning algorithm. A key feature of this parallel formulation is that it is able to achieve a high degree of concurrency while maintaining the high quality of the partitions produced by the serial multilevel k-way partitioning algorithm. In particular, the time taken by our parallel graph partitioning algorithm is only slightly longer than the time taken for re-arrangement of the graph among processors according to the new partition. Experiments with a variety of finite element graphs show that our parallel formulation produces high-quality partitionings in a short amount of time. For example, a 128-way partitioning of graphs with one million vertices can be computed in a little over two seconds on a 128-processor Cray T3D. Furthermore, the quality of the partitions produced is comparable (edge-cuts within 5\%) to those produced by the serial multilevel k-way algorithm. Thus our parallel algorithm makes it feasible to perform frequent repartitioning of graphs in dynamic computations without compromising the partitioning quality.},
	number = {1},
	urldate = {2023-11-14},
	journal = {SIAM Journal on Scientific Computing},
	author = {Karypis, George and Kumar, Vipin},
	month = jan,
	year = {1998},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {359--392},
	annote = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{liu_ahng_2019,
	title = {{AHNG}: {Representation} learning on attributed heterogeneous network},
	volume = {50},
	issn = {1566-2535},
	shorttitle = {{AHNG}},
	url = {https://www.sciencedirect.com/science/article/pii/S156625351830647X},
	doi = {10.1016/j.inffus.2019.01.005},
	abstract = {Network embedding aims to encode nodes into a low-dimensional space with the structure and inherent properties of the networks preserved. It is an upstream technique for network analyses such as link prediction and node clustering. Most existing efforts are devoted to homogeneous or heterogeneous plain networks. However, networks in real-world scenarios are usually heterogeneous and not plain, i.e., they contain multi-type nodes/links and diverse node attributes. We refer such kind of networks with both heterogeneities and attributes as attributed heterogeneous networks (AHNs). Embedding AHNs faces two challenges: (1) how to fuse heterogeneous information sources including network structures, semantic information and node attributes; (2) how to capture uncertainty of node embeddings caused by diverse attributes. To tackle these challenges, we propose a unified embedding model which represents each node in an AHN with a Gaussian distribution (AHNG). AHNG fuses multi-type nodes/links and diverse attributes through a two-layer neural network and captures the uncertainty by embedding nodes as Gaussian distributions. Furthermore, the incorporation of node attributes makes AHNG inductive, embedding previously unseen nodes or isolated nodes without additional training. Extensive experiments on a large real-world dataset validate the effectiveness and efficiency of the proposed model.},
	urldate = {2023-11-17},
	journal = {Information Fusion},
	author = {Liu, Mengyue and Liu, Jun and Chen, Yihe and Wang, Meng and Chen, Hao and Zheng, Qinghua},
	month = oct,
	year = {2019},
	keywords = {Attributed heterogeneous network, Gaussian distribution, Network embedding},
	pages = {221--230},
	file = {ScienceDirect Full Text PDF:/Users/admin/Zotero/storage/A6XCDZ8A/Liu et al. - 2019 - AHNG Representation learning on attributed hetero.pdf:application/pdf},
}

@inproceedings{
velickovic_graph_2018,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@inproceedings{nguyen_novel_2018,
	address = {New Orleans, Louisiana},
	title = {A {Novel} {Embedding} {Model} for {Knowledge} {Base} {Completion} {Based} on {Convolutional} {Neural} {Network}},
	url = {https://aclanthology.org/N18-2053},
	doi = {10.18653/v1/N18-2053},
	abstract = {In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vector representing the input triple. The feature vector is multiplied with a weight vector via a dot product to return a score. This score is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237.},
	urldate = {2024-01-30},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nguyen, Dai Quoc and Nguyen, Tu Dinh and Nguyen, Dat Quoc and Phung, Dinh},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {327--333},
	file = {Full Text PDF:/Users/admin/Zotero/storage/DTQD6EYL/Nguyen et al. - 2018 - A Novel Embedding Model for Knowledge Base Complet.pdf:application/pdf},
}

@inproceedings{shiao_carl-g_2023,
	title = {{CARL}-{G}: {Clustering}-{Accelerated} {Representation} {Learning} on {Graphs}},
	shorttitle = {{CARL}-{G}},
	url = {http://arxiv.org/abs/2306.06936},
	doi = {10.1145/3580305.3599268},
	abstract = {Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Validation Indices (CVIs), i.e., internal measures of cluster quality (no ground truth required). CARL-G is adaptable to different clustering methods and CVIs, and we show that with the right choice of clustering method and CVI, CARL-G outperforms node classification baselines on 4/5 datasets with up to a 79x training speedup compared to the best-performing baseline. CARL-G also performs at par or better than baselines in node clustering and similarity search tasks, training up to 1,500x faster than the best-performing baseline. Finally, we also provide theoretical foundations for the use of CVI-inspired losses in graph representation learning.},
	urldate = {2024-03-27},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	author = {Shiao, William and Saini, Uday Singh and Liu, Yozen and Zhao, Tong and Shah, Neil and Papalexakis, Evangelos E.},
	month = aug,
	year = {2023},
	note = {arXiv:2306.06936 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {2036--2048},
	annote = {Comment: 14 pages. Accepted at KDD 2023},
	file = {arXiv.org Snapshot:/Users/admin/Zotero/storage/KJLVYMTP/2306.html:text/html;Full Text PDF:/Users/admin/Zotero/storage/ECJSV42Y/Shiao et al. - 2023 - CARL-G Clustering-Accelerated Representation Lear.pdf:application/pdf},
}

@article{ji_survey_2022,
	title = {A {Survey} on {Knowledge} {Graphs}: {Representation}, {Acquisition} and {Applications}},
	volume = {33},
	issn = {2162-237X, 2162-2388},
	shorttitle = {A {Survey} on {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2002.00388},
	doi = {10.1109/TNNLS.2021.3070843},
	abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review of knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning, are reviewed. We further explore several emerging topics, including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
	number = {2},
	urldate = {2024-03-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
	month = feb,
	year = {2022},
	note = {arXiv:2002.00388 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {494--514},
	file = {arXiv.org Snapshot:/Users/admin/Zotero/storage/5I2S52JP/2002.html:text/html;Full Text PDF:/Users/admin/Zotero/storage/SKP9C39C/Ji et al. - 2022 - A Survey on Knowledge Graphs Representation, Acqu.pdf:application/pdf},
}

@inproceedings{nathani_learning_2019,
	address = {Florence, Italy},
	title = {Learning {Attention}-based {Embeddings} for {Relation} {Prediction} in {Knowledge} {Graphs}},
	url = {https://aclanthology.org/P19-1466},
	doi = {10.18653/v1/P19-1466},
	abstract = {The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and relation features in any given entity's neighborhood. Additionally, we also encapsulate relation clusters and multi-hop relations in our model. Our empirical study offers insights into the efficacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets.},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nathani, Deepak and Chauhan, Jatin and Sharma, Charu and Kaul, Manohar},
	month = jul,
	year = {2019},
	pages = {4710--4723},
}

@inproceedings{ehrlinger_towards_2016,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {Towards a {Definition} of {Knowledge} {Graphs}},
	volume = {1695},
	url = {https://ceur-ws.org/Vol-1695/paper4.pdf},
	booktitle = {Joint {Proceedings} of the {Posters} and {Demos} {Track} of the 12th {International} {Conference} on {Semantic} {Systems} - {SEMANTiCS2016} and the 1st {International} {Workshop} on {Semantic} {Change} \& {Evolving} {Semantics} ({SuCCESS}'16) co-located with the 12th {International} {Conference} on {Semantic} {Systems} ({SEMANTiCS} 2016), {Leipzig}, {Germany}, {September} 12-15, 2016},
	publisher = {CEUR-WS.org},
	author = {Ehrlinger, Lisa and Wöß, Wolfram},
	editor = {Martin, Michael and Cuquet, Martí and Folmer, Erwin},
	year = {2016},
}

@inproceedings{kingma_adam_2015,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
}

@article{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	volume = {abs/1903.02428},
	url = {http://arxiv.org/abs/1903.02428},
	journal = {CoRR},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	year = {2019},
	note = {arXiv: 1903.02428},
}

@inproceedings{dai_adversarial_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Adversarial {Attack} on {Graph} {Structured} {Data}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/dai18b.html},
	abstract = {Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dai, Hanjun and Li, Hui and Tian, Tian and Huang, Xin and Wang, Lin and Zhu, Jun and Song, Le},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {1115--1124},
	file = {Full Text PDF:/Users/admin/Zotero/storage/UPDYLG2B/Dai et al. - 2018 - Adversarial Attack on Graph Structured Data.pdf:application/pdf},
}

@inproceedings{ghosh_distributed_2018,
	title = {Distributed {Louvain} {Algorithm} for {Graph} {Community} {Detection}},
	url = {https://ieeexplore.ieee.org/document/8425242},
	doi = {10.1109/IPDPS.2018.00098},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Ghosh, Sayan and Halappanavar, Mahantesh and Tumeo, Antonino and Kalyanaraman, Ananth and Lu, Hao and Chavarrià-Miranda, Daniel and Khan, Arif and Gebremedhin, Assefaw},
	year = {2018},
	pages = {885--895},
}

@inproceedings{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	file = {Full Text:/Users/admin/Zotero/storage/2VYEB482/Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relation.pdf:application/pdf},
}

@inproceedings{mckinney_data_2010,
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	url = {https://conference.scipy.org/proceedings/scipy2010/mckinney.html},
	doi = {10.25080/Majora-92bf1922-00a},
	booktitle = {Proceedings of the 9th {Python} in {Science} {Conference}},
	author = {McKinney, Wes},
	editor = {Walt, Stéfan van der and Millman, Jarrod},
	year = {2010},
	pages = {56 -- 61},
	file = {Full Text:/Users/admin/Zotero/storage/E5G4QFLL/McKinney - 2010 - Data Structures for Statistical Computing in Pytho.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	url = {https://jmlr.org/papers/v12/pedregosa11a.html},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@inproceedings{ren_beta_2020,
	title = {Beta {Embeddings} for {Multi}-{Hop} {Logical} {Reasoning} in {Knowledge} {Graphs}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e43739bba7cdb577e9e3e4e42447f5a5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Hongyu and Leskovec, Jure},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {19716--19726},
	file = {Full Text PDF:/Users/admin/Zotero/storage/AC5H399T/Ren and Leskovec - 2020 - Beta Embeddings for Multi-Hop Logical Reasoning in.pdf:application/pdf},
}

@article{vieira_comparative_2020,
	title = {A comparative study of overlapping community detection methods from the perspective of the structural properties},
	volume = {5},
	copyright = {2020 The Author(s)},
	issn = {2364-8228},
	url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-020-00289-9},
	doi = {10.1007/s41109-020-00289-9},
	abstract = {Community detection is one of the most important tasks in network analysis. It is increasingly clear that quality measures are not sufficient for assessing communities and structural properties play a key hole in understanding how nodes are organized in the network. This work presents a comparative study of some representative state-of-the-art methods for overlapping community detection from the perspective of the structural properties of the communities identified by them. Experiments with synthetic and real-world benchmark Ground-Truth networks show that, although the methods are able to identify modular communities, they often miss many structural properties of the communities, such as the number of nodes in the overlapping region and the memberships of the nodes. This is a strong suggestion that a deeper comprehension of the overlapping properties of the communities is needed for the design of more efficient community detection methods.},
	language = {en},
	number = {1},
	urldate = {2023-11-14},
	journal = {Applied Network Science},
	author = {Vieira, Vinícius da Fonseca and Xavier, Carolina Ribeiro and Evsukoff, Alexandre Gonçalves},
	month = dec,
	year = {2020},
	pages = {1--42},
	annote = {Number: 1 Publisher: SpringerOpen},
	file = {Full Text PDF:/Users/admin/Zotero/storage/Z2R3TAX6/Vieira et al. - 2020 - A comparative study of overlapping community detec.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/AAAC9Z5Z/s41109-020-00289-9.html:text/html},
}

@article{chen_entity-agnostic_2023,
	title = {Entity-{Agnostic} {Representation} {Learning} for {Parameter} {Efficient} {Knowledge} {Graph} {Embedding}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25535},
	doi = {10.1609/aaai.v37i4.25535},
	abstract = {We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings  (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.},
	language = {en},
	number = {4},
	urldate = {2024-09-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Chen, Mingyang and Zhang, Wen and Yao, Zhen and Zhu, Yushan and Gao, Yang and Pan, Jeff Z. and Chen, Huajun},
	month = jun,
	year = {2023},
	note = {Number: 4},
	keywords = {DMKM: Semantic Web},
	pages = {4182--4190},
	file = {Full Text PDF:/Users/admin/Zotero/storage/T4BVQK8X/Chen et al. - 2023 - Entity-Agnostic Representation Learning for Parame.pdf:application/pdf},
}

@inproceedings{galkin_nodepiece_2022,
	title = {{NodePiece}: {Compositional} and {Parameter}-{Efficient} {Representations} of {Large} {Knowledge} {Graphs}},
	url = {https://openreview.net/forum?id=xMJWUKJnFSw},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}, {ICLR} 2022, {Virtual} {Event}, {April} 25-29, 2022},
	publisher = {OpenReview.net},
	author = {Galkin, Mikhail and Denis, Etienne G. and Wu, Jiapeng and Hamilton, William L.},
	year = {2022},
	file = {Full Text:/Users/admin/Zotero/storage/4S3AAGX9/Galkin et al. - 2022 - NodePiece Compositional and Parameter-Efficient R.pdf:application/pdf},
}

@article{liang_survey_2024,
	title = {A {Survey} of {Knowledge} {Graph} {Reasoning} on {Graph} {Types}: {Static}, {Dynamic}, and {Multi}-{Modal}},
	issn = {1939-3539},
	shorttitle = {A {Survey} of {Knowledge} {Graph} {Reasoning} on {Graph} {Types}},
	url = {https://ieeexplore.ieee.org/abstract/document/10577554},
	doi = {10.1109/TPAMI.2024.3417451},
	abstract = {Knowledge graph reasoning (KGR), aiming to deduce new facts from existing facts based on mined logic rules underlying knowledge graphs (KGs), has become a fast-growing research direction. It has been proven to significantly benefit the usage of KGs in many AI applications, such as question answering, recommendation systems, and etc. According to the graph types, existing KGR models can be roughly divided into three categories, i.e., static models, temporal models, and multi-modal models. Early works in this domain mainly focus on static KGR, and recent works try to leverage the temporal and multi-modal information, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for knowledge graph reasoning tracing from static to temporal and then to multi-modal KGs. Concretely, the models are reviewed based on bi-level taxonomy, i.e., top-level (graph types) and base-level (techniques and scenarios). Besides, the performances, as well as datasets, are summarized and presented. Moreover, we point out the challenges and potential opportunities to enlighten the readers. The corresponding open-source repository is shared on GitHub https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning.},
	urldate = {2024-09-03},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liang, Ke and Meng, Lingyuan and Liu, Meng and Liu, Yue and Tu, Wenxuan and Wang, Siwei and Zhou, Sihang and Liu, Xinwang and Sun, Fuchun and He, Kunlun},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Knowledge graphs, Cognition, Extrapolation, Fasteners, Interpolation, Knowledge Graph, Knowledge Graph Reasoning, Multi-Modal Knowledge Graph, Surveys, Taxonomy, Temporal Knowledge Graph},
	pages = {1--20},
	file = {IEEE Xplore Abstract Record:/Users/admin/Zotero/storage/M8D676P4/10577554.html:text/html;IEEE Xplore Full Text PDF:/Users/admin/Zotero/storage/WJEQT6HS/Liang et al. - 2024 - A Survey of Knowledge Graph Reasoning on Graph Typ.pdf:application/pdf},
}

@inproceedings{ren_query2box_2020,
	title = {Query2box: {Reasoning} over {Knowledge} {Graphs} in {Vector} {Space} {Using} {Box} {Embeddings}},
	url = {https://openreview.net/forum?id=BJgr4kSFDS},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
	year = {2020},
	file = {Full Text:/Users/admin/Zotero/storage/SKXSAGQE/Ren et al. - 2020 - Query2box Reasoning over Knowledge Graphs in Vect.pdf:application/pdf},
}

@inproceedings{zaheer_deep_2017,
	title = {Deep {Sets}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
	abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	urldate = {2024-09-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	year = {2017},
	file = {Full Text PDF:/Users/admin/Zotero/storage/HSGW4D4N/Zaheer et al. - 2017 - Deep Sets.pdf:application/pdf},
}

@inproceedings{zheng_dgl-ke_2020,
	address = {New York, NY, USA},
	series = {{SIGIR} '20},
	title = {{DGL}-{KE}: {Training} {Knowledge} {Graph} {Embeddings} at {Scale}},
	isbn = {978-1-4503-8016-4},
	shorttitle = {{DGL}-{KE}},
	url = {https://dl.acm.org/doi/10.1145/3397271.3401172},
	doi = {10.1145/3397271.3401172},
	abstract = {Knowledge graphs have emerged as a key abstraction for organizing information in diverse domains and their embeddings are increasingly used to harness their information in various information retrieval and machine learning tasks. However, the ever growing size of knowledge graphs requires computationally efficient algorithms capable of scaling to graphs with millions of nodes and billions of edges. This paper presents DGL-KE, an open-source package to efficiently compute knowledge graph embeddings. DGL-KE introduces various novel optimizations that accelerate training on knowledge graphs with millions of nodes and billions of edges using multi-processing, multi-GPU, and distributed parallelism. These optimizations are designed to increase data locality, reduce communication overhead, overlap computations with memory accesses, and achieve high operation efficiency. Experiments on knowledge graphs consisting of over 86M nodes and 338M edges show that DGL-KE can compute embeddings in 100 minutes on an EC2 instance with 8 GPUs and 30 minutes on an EC2 cluster with 4 machines with 48 cores/machine. These results represent a 2× {\textasciitilde} 5× speedup over the best competing approaches. DGL-KE is available on https://github.com/awslabs/dgl-ke.},
	urldate = {2024-10-24},
	booktitle = {Proceedings of the 43rd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Da and Song, Xiang and Ma, Chao and Tan, Zeyuan and Ye, Zihao and Dong, Jin and Xiong, Hao and Zhang, Zheng and Karypis, George},
	month = jul,
	year = {2020},
	pages = {739--748},
	file = {Full Text PDF:/Users/admin/Zotero/storage/5QM26HDA/Zheng et al. - 2020 - DGL-KE Training Knowledge Graph Embeddings at Sca.pdf:application/pdf},
}

@misc{mahmoudzadeh_deep_2024,
	title = {Deep {Generative} {Models} for {Subgraph} {Prediction}},
	url = {http://arxiv.org/abs/2408.04053},
	doi = {10.48550/arXiv.2408.04053},
	abstract = {Graph Neural Networks (GNNs) are important across different domains, such as social network analysis and recommendation systems, due to their ability to model complex relational data. This paper introduces subgraph queries as a new task for deep graph learning. Unlike traditional graph prediction tasks that focus on individual components like link prediction or node classification, subgraph queries jointly predict the components of a target subgraph based on evidence that is represented by an observed subgraph. For instance, a subgraph query can predict a set of target links and/or node labels. To answer subgraph queries, we utilize a probabilistic deep Graph Generative Model. Specifically, we inductively train a Variational Graph Auto-Encoder (VGAE) model, augmented to represent a joint distribution over links, node features and labels. Bayesian optimization is used to tune a weighting for the relative importance of links, node features and labels in a specific domain. We describe a deterministic and a sampling-based inference method for estimating subgraph probabilities from the VGAE generative graph distribution, without retraining, in zero-shot fashion. For evaluation, we apply the inference methods on a range of subgraph queries on six benchmark datasets. We find that inference from a model achieves superior predictive performance, surpassing independent prediction baselines with improvements in AUC scores ranging from 0.06 to 0.2 points, depending on the dataset.},
	urldate = {2025-03-22},
	publisher = {arXiv},
	author = {Mahmoudzadeh, Erfaneh and Naddaf, Parmis and Zahirnia, Kiarash and Schulte, Oliver},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04053 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: accepted at ECAI 2024},
	file = {Preprint PDF:/Users/admin/Zotero/storage/VVPSR62W/Mahmoudzadeh et al. - 2024 - Deep Generative Models for Subgraph Prediction.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/28B4NFCH/2408.html:text/html},
}

@misc{thanapalasingam_intelligraphs_2023,
	title = {{IntelliGraphs}: {Datasets} for {Benchmarking} {Knowledge} {Graph} {Generation}},
	shorttitle = {{IntelliGraphs}},
	url = {http://arxiv.org/abs/2307.06698},
	doi = {10.48550/arXiv.2307.06698},
	abstract = {Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development of machine learning models that emphasize semantic understanding.},
	urldate = {2025-03-22},
	publisher = {arXiv},
	author = {Thanapalasingam, Thiviyan and Krieken, Emile van and Bloem, Peter and Groth, Paul},
	month = aug,
	year = {2023},
	note = {arXiv:2307.06698 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/admin/Zotero/storage/9MPUR5TM/Thanapalasingam et al. - 2023 - IntelliGraphs Datasets for Benchmarking Knowledge.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/7FATPVQW/2307.html:text/html},
}

@inproceedings{vignac_digress_2022,
	title = {{DiGress}: {Discrete} {Denoising} diffusion for graph generation},
	shorttitle = {{DiGress}},
	url = {https://openreview.net/forum?id=UaAD-Nu86WX},
	abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
	language = {en},
	urldate = {2025-01-30},
	author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
	month = sep,
	year = {2022},
}

@article{ruddigkeit_enumeration_2012,
	title = {Enumeration of 166 {Billion} {Organic} {Small} {Molecules} in the {Chemical} {Universe} {Database} {GDB}-17},
	volume = {52},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/ci300415d},
	doi = {10.1021/ci300415d},
	abstract = {Drug molecules consist of a few tens of atoms connected by covalent bonds. How many such molecules are possible in total and what is their structure? This question is of pressing interest in medicinal chemistry to help solve the problems of drug potency, selectivity, and toxicity and reduce attrition rates by pointing to new molecular series. To better define the unknown chemical space, we have enumerated 166.4 billion molecules of up to 17 atoms of C, N, O, S, and halogens forming the chemical universe database GDB-17, covering a size range containing many drugs and typical for lead compounds. GDB-17 contains millions of isomers of known drugs, including analogs with high shape similarity to the parent drug. Compared to known molecules in PubChem, GDB-17 molecules are much richer in nonaromatic heterocycles, quaternary centers, and stereoisomers, densely populate the third dimension in shape space, and represent many more scaffold types.},
	number = {11},
	urldate = {2025-01-30},
	journal = {Journal of Chemical Information and Modeling},
	author = {Ruddigkeit, Lars and van Deursen, Ruud and Blum, Lorenz C. and Reymond, Jean-Louis},
	month = nov,
	year = {2012},
	pages = {2864--2875},
	annote = {Publisher: American Chemical Society},
}

@incollection{paszke_pytorch_2019-1,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{fey_fast_2019-1,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	volume = {abs/1903.02428},
	url = {http://arxiv.org/abs/1903.02428},
	journal = {CoRR},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	year = {2019},
	note = {arXiv: 1903.02428},
}

@inproceedings{bojchevski_netgan_2018,
	title = {{NetGAN}: {Generating} {Graphs} via {Random} {Walks}},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	author = {Bojchevski, Aleksandar and Shchur, Oleksandr and Zügner, Daniel and Günnemann, Stephan},
	year = {2018},
	pages = {609--618},
}

@inproceedings{you_graphrnn_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{GraphRNN}: {Generating} {Realistic} {Graphs} with {Deep} {Auto}-regressive {Models}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/you18a.html},
	abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {5708--5717},
}

@inproceedings{niu_permutation_2020,
	address = {Online},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Permutation {Invariant} {Graph} {Generation} via {Score}-{Based} {Generative} {Modeling}},
	volume = {108},
	abstract = {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.},
	publisher = {PMLR},
	author = {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	month = aug,
	year = {2020},
	pages = {4474--4484},
}

@misc{mckay_chordal_2020,
	title = {chordal graphs},
	author = {McKay, Brendan},
	year = {2020},
}

@article{de_cao_molgan_2018,
	title = {{MolGAN}: {An} implicit generative model for small molecular graphs},
	journal = {arXiv preprint arXiv:1805.11973},
	author = {De Cao, Nicola and Kipf, Thomas},
	year = {2018},
}

@inproceedings{nauata_house-gan_2020,
	title = {House-gan: {Relational} generative adversarial networks for graph-constrained house layout generation},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Nauata, Nelson and Chang, Kai-Hung and Cheng, Chin-Yi and Mori, Greg and Furukawa, Yasutaka},
	year = {2020},
	pages = {162--177},
}

@article{csardi_igraph_2006,
	title = {The igraph software package for complex network research},
	volume = {Complex Systems},
	url = {https://igraph.org},
	journal = {InterJournal},
	author = {Csardi, Gabor and Nepusz, Tamas},
	year = {2006},
	pages = {1695},
}

@inproceedings{aldous_exchangeability_1985,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Mathematics}},
	title = {Exchangeability and related topics},
	isbn = {978-3-540-39316-0},
	doi = {10.1007/BFb0099421},
	language = {en},
	booktitle = {École d'Été de {Probabilités} de {Saint}-{Flour} {XIII} — 1983},
	publisher = {Springer},
	author = {Aldous, David J.},
	editor = {Aldous, David J. and Ibragimov, Illdar A. and Jacod, Jean and Hennequin, P. L.},
	year = {1985},
	pages = {1--198},
}

@inproceedings{karras_analyzing_2020,
	title = {Analyzing and {Improving} the {Image} {Quality} of {StyleGAN}},
	doi = {10.1109/CVPR42600.2020.00813},
	abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = jun,
	year = {2020},
	keywords = {Convolution, Generators, Image resolution, Measurement, Modulation, Standards, Training},
	pages = {8107--8116},
	annote = {ISSN: 2575-7075},
}

@article{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach – such as computer vision, playing Go, or protein folding – are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2021-10-08},
	journal = {arXiv:2104.13478 [cs, stat]},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 2104.13478},
}

@article{xie_overlapping_2013-1,
	title = {Overlapping community detection in networks: {The} state-of-the-art and comparative study},
	volume = {45},
	issn = {0360-0300},
	shorttitle = {Overlapping community detection in networks},
	url = {https://doi.org/10.1145/2501654.2501657},
	doi = {10.1145/2501654.2501657},
	abstract = {This article reviews the state-of-the-art in overlapping community detection algorithms, quality measures, and benchmarks. A thorough comparison of different algorithms (a total of fourteen) is provided. In addition to community-level evaluation, we propose a framework for evaluating algorithms' ability to detect overlapping nodes, which helps to assess overdetection and underdetection. After considering community-level detection performance measured by normalized mutual information, the Omega index, and node-level detection performance measured by F-score, we reached the following conclusions. For low overlapping density networks, SLPA, OSLOM, Game, and COPRA offer better performance than the other tested algorithms. For networks with high overlapping density and high overlapping diversity, both SLPA and Game provide relatively stable performance. However, test results also suggest that the detection in such networks is still not yet fully resolved. A common feature observed by various algorithms in real-world networks is the relatively small fraction of overlapping nodes (typically less than 30\%), each of which belongs to only 2 or 3 communities.},
	number = {4},
	urldate = {2021-11-03},
	journal = {ACM Computing Surveys},
	author = {Xie, Jierui and Kelley, Stephen and Szymanski, Boleslaw K.},
	month = aug,
	year = {2013},
	keywords = {Overlapping community detection, social networks},
	pages = {43:1--43:35},
}

@inproceedings{yang_overlapping_2013-1,
	address = {New York, NY, USA},
	series = {{WSDM} '13},
	title = {Overlapping community detection at scale: a nonnegative matrix factorization approach},
	isbn = {978-1-4503-1869-3},
	shorttitle = {Overlapping community detection at scale},
	url = {https://doi.org/10.1145/2433396.2433471},
	doi = {10.1145/2433396.2433471},
	abstract = {Network communities represent basic structures for understanding the organization of real-world networks. A community (also referred to as a module or a cluster) is typically thought of as a group of nodes with more connections amongst its members than between its members and the remainder of the network. Communities in networks also overlap as nodes belong to multiple clusters at once. Due to the difficulties in evaluating the detected communities and the lack of scalable algorithms, the task of overlapping community detection in large networks largely remains an open problem. In this paper we present BIGCLAM (Cluster Affiliation Model for Big Networks), an overlapping community detection method that scales to large networks of millions of nodes and edges. We build on a novel observation that overlaps between communities are densely connected. This is in sharp contrast with present community detection methods which implicitly assume that overlaps between communities are sparsely connected and thus cannot properly extract overlapping communities in networks. In this paper, we develop a model-based community detection algorithm that can detect densely overlapping, hierarchically nested as well as non-overlapping communities in massive networks. We evaluate our algorithm on 6 large social, collaboration and information networks with ground-truth community information. Experiments show state of the art performance both in terms of the quality of detected communities as well as in speed and scalability of our algorithm.},
	urldate = {2021-11-03},
	booktitle = {Proceedings of the sixth {ACM} international conference on {Web} search and data mining},
	publisher = {Association for Computing Machinery},
	author = {Yang, Jaewon and Leskovec, Jure},
	month = feb,
	year = {2013},
	keywords = {matrix factorization, network communities, overlapping community detection},
	pages = {587--596},
}

@article{zhang_mixed_2017-1,
	title = {A {Mixed} {Representation}-{Based} {Multiobjective} {Evolutionary} {Algorithm} for {Overlapping} {Community} {Detection}},
	volume = {47},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2017.2711038},
	abstract = {Designing multiobjective evolutionary algorithms (MOEAs) for community detection in complex networks has attracted much attention of researchers recently. However, most of the existing methods focus on addressing the task of nonoverlapping community detection, where each node must belong to one and only one community. In fact, communities are often overlapped with each other in many real-world networks, thus it is necessary to design overlapping community detection algorithms. To this end, this paper proposes a mixed representation-based MOEA (MR-MOEA) for overlapping community detection. In MR-MOEA, a mixed individual representation scheme is proposed to fast encode and decode the overlapping divisions of complex networks. Specifically, this mixed representation consists of two parts: one represents all potential overlapping nodes and the other delegates all nonoverlapping nodes. These two parts evolve together to detect the overlapping communities of networks based on different updating strategies suggested in MR-MOEA. We verify the effectiveness of the proposed algorithm MR-MOEA on ten real-world complex networks and the experimental results demonstrate that MR-MOEA is superior over six representative algorithms for overlapping community detection.},
	number = {9},
	journal = {IEEE Transactions on Cybernetics},
	author = {Zhang, Lei and Pan, Hebin and Su, Yansen and Zhang, Xingyi and Niu, Yunyun},
	month = sep,
	year = {2017},
	keywords = {Complex network, Complex networks, Cybernetics, Detection algorithms, Encoding, Evolutionary computation, individual representation scheme, Lapping, multiobjective evolutionary algorithm (MOEA), Optimization, overlapping community detection},
	pages = {2703--2716},
	annote = {Conference Name: IEEE Transactions on Cybernetics},
}

@article{bello-orgaz_multi-objective_2018-1,
	title = {A {Multi}-{Objective} {Genetic} {Algorithm} for overlapping community detection based on edge encoding},
	volume = {462},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518304559},
	doi = {10.1016/j.ins.2018.06.015},
	abstract = {The Community Detection Problem (CDP) in Social Networks has been widely studied from different areas such as Data Mining, Graph Theory Physics, or Social Network Analysis, among others. This problem tries to divide a graph into different groups of nodes (communities), according to the graph topology. A partition is a division of the graph where each node belongs to only one community. However, a common feature observed in real-world networks is the existence of overlapping communities, where a given node can belong to more than one community. This paper presents a new Multi-Objective Genetic Algorithm (MOGA-OCD) designed to detect overlapping communities, by using measures related to the network connectivity. For this purpose, the proposed algorithm uses a phenotype-type encoding based on the edge information, and a new fitness function focused on optimizing two classical objectives in CDP: the first one is used to maximize the internal connectivity of the communities, whereas the second one is used to minimize the external connections to the rest of the graph. To select the most appropriate metrics for these objectives, a comparative assessment of several connectivity metrics has been carried out using real-world networks. Finally, the algorithm has been evaluated against other well-known algorithms from the state of the art in CDP. The experimental results show that the proposed approach improves overall the accuracy and quality of alternative methods in CDP, showing its effectiveness as a new powerful algorithm for detecting structured overlapping communities.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Information Sciences},
	author = {Bello-Orgaz, Gema and Salcedo-Sanz, Sancho and Camacho, David},
	month = sep,
	year = {2018},
	keywords = {Edge-based encoding, Graph clustering, Multi-Objective genetic algorithms, Network metrics, Overlapping community detection},
	pages = {290--314},
}

@article{lu_lpanni_2019-1,
	title = {{LPANNI}: {Overlapping} {Community} {Detection} {Using} {Label} {Propagation} in {Large}-{Scale} {Complex} {Networks}},
	volume = {31},
	issn = {1558-2191},
	shorttitle = {{LPANNI}},
	doi = {10.1109/TKDE.2018.2866424},
	abstract = {Overlapping community structure is a significant feature of large-scale complex networks. Some existing community detection algorithms cannot be applied to large-scale complex networks due to their high time or space complexity. Label propagation algorithms were proposed for detecting communities in large-scale networks because of their linear time complexity, however most of which can only detect non-overlapping communities, or the results are inaccurate and unstable. Aimed at the defects, we proposed an improved overlapping community detection algorithm, LPANNI (Label Propagation Algorithm with Neighbor Node Influence), which detects overlapping community structures by adopting fixed label propagation sequence based on the ascending order of node importance and label update strategy based on neighbor node influence and historical label preferred strategy. Extensive experimental results in both real networks and synthetic networks show that, LPANNI can significantly improve the accuracy and stability of community detection algorithms based on label propagation in large-scale complex networks. Meanwhile, LPANNI can detect overlapping community structures in large-scale complex networks under linear time complexity.},
	number = {9},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lu, Meilian and Zhang, Zhenglin and Qu, Zhihe and Kang, Yu},
	month = sep,
	year = {2019},
	keywords = {Clustering algorithms, Complex networks, Detection algorithms, Label propagation algorithm, large-scale network, Measurement, neighbor node influence, overlapping community detection, Stability analysis, Time complexity},
	pages = {1736--1749},
	annote = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
}

@article{li_local_2018-1,
	title = {Local {Spectral} {Clustering} for {Overlapping} {Community} {Detection}},
	volume = {12},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3106370},
	doi = {10.1145/3106370},
	abstract = {Large graphs arise in a number of contexts and understanding their structure and extracting information from them is an important research area. Early algorithms for mining communities have focused on global graph structure, and often run in time proportional to the size of the entire graph. As we explore networks with millions of vertices and find communities of size in the hundreds, it becomes important to shift our attention from macroscopic structure to microscopic structure in large networks. A growing body of work has been adopting local expansion methods in order to identify communities from a few exemplary seed members. In this article, we propose a novel approach for finding overlapping communities called Lemon (Local Expansion via Minimum One Norm). Provided with a few known seeds, the algorithm finds the community by performing a local spectral diffusion. The core idea of Lemon is to use short random walks to approximate an invariant subspace near a seed set, which we refer to as local spectra. Local spectra can be viewed as the low-dimensional embedding that captures the nodes’ closeness in the local network structure. We show that Lemon’s performance in detecting communities is competitive with state-of-the-art methods. Moreover, the running time scales with the size of the community rather than that of the entire graph. The algorithm is easy to implement and is highly parallelizable. We further provide theoretical analysis of the local spectral properties, bounding the measure of tightness of extracted community using the eigenvalues of graph Laplacian. We thoroughly evaluate our approach using both synthetic and real-world datasets across different domains, and analyze the empirical variations when applying our method to inherently different networks in practice. In addition, the heuristics on how the seed set quality and quantity would affect the performance are provided.},
	number = {2},
	urldate = {2021-11-03},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Li, Yixuan and He, Kun and Kloster, Kyle and Bindel, David and Hopcroft, John},
	month = jan,
	year = {2018},
	keywords = {Community detection, graph diffusion, local spectral clustering, random walk, seed set expansion},
	pages = {17:1--17:27},
}

@article{wang_effective_2021-1,
	title = {An effective and scalable overlapping community detection approach: {Integrating} social identity model and game theory},
	volume = {390},
	issn = {0096-3003},
	shorttitle = {An effective and scalable overlapping community detection approach},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300320305567},
	doi = {10.1016/j.amc.2020.125601},
	abstract = {Because of its broad real-life application, community detection (in the realm of a complex network) is an attractive challenge to many researchers. However, current methods fail to reveal the full community structure and its formation process. Thus, here we present SIMGT, an effective and Scalable approach that detects overlapping communities: Integrating social identity Model and Game Theory. Inspired by social identity theory and nodes’ high-order proximities, first we weight and rewire the original network, then we associate each node with a new utility function. Next, we model community formation as a non-cooperative game among all nodes, and we regard the nodes as self-interested players. Further, we use a stochastic gradient-ascent method to update players’ strategies toward different communities, and prove that our game greatly resembles and matches how a potential game works (in the classical sense in game theory), indicating that the Nash equilibrium point must exist. Finally, we implement comprehensive experiments on several synthetic and real-life networks. The results show that whatever weighting strategy we choose, SIMGT can gain better performance on community detection task. In particular, SIMGT achieves a best result when we choose the Jaccard coefficient. After comparing SIMGT with six benchmark algorithms, we obtain convincing results in terms of how well the algorithms reveal communities, as well as algorithms’ scalability.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Applied Mathematics and Computation},
	author = {Wang, Yuyao and Bu, Zhan and Yang, Huan and Li, Hui-Jia and Cao, Jie},
	month = feb,
	year = {2021},
	keywords = {High-order proximities, Non-cooperative game, Overlapping community detection, Social identity model, Stochastic gradient-ascent},
	pages = {125601},
}

@article{gao_overlapping_2021-1,
	title = {Overlapping community detection by constrained personalized {PageRank}},
	volume = {173},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421001238},
	doi = {10.1016/j.eswa.2021.114682},
	abstract = {Given a network, local community detection (a.k.a. graph clustering) methods aim at finding communities around the selected initial nodes (also referred to as seeds, starting nodes or core nodes). Methods in this kind successfully address the efficiency problem confronted by global clustering methods. And techniques, such as personalized PageRank and heat kernel diffusion, for ranking the proximity score of vertices nearby with respect to the corresponding starting nodes are developed. However, most of the random-walk based metrics allow a walker to diffuse without any constraint, and the walker can easily run into irrelevant communities. As a result, the corresponding community could include irrelevant high-quality communities (communities with good fitness score) nearby, we refer to the case that a walker goes into irrelevant communities and causes inaccurate expansion of a community as redundant diffusion. In this work, we develop a constrained personalized PageRank method for community expansion to reduce the problem of redundant diffusion. In the mechanism, a walker moves with lower probability to neighbor nodes already in the existing communities, and a walker tends to walk out of the community if the walker walks into an irrelevant community. Extensive experiments on synthetic and large real-world networks demonstrate that the proposed method outperforms approaches in the state of the art by a large margin in accuracy and efficiency.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Expert Systems with Applications},
	author = {Gao, Yang and Yu, Xiangzhan and Zhang, Hongli},
	month = jul,
	year = {2021},
	keywords = {Constrained personalized PageRank, Local community detection, Random walk, Redundant diffusion},
	pages = {114682},
}

@article{vieira_comparative_2020-1,
	title = {A comparative study of overlapping community detection methods from the perspective of the structural properties},
	volume = {5},
	copyright = {2020 The Author(s)},
	issn = {2364-8228},
	url = {https://appliednetsci.springeropen.com/articles/10.1007/s41109-020-00289-9},
	doi = {10.1007/s41109-020-00289-9},
	abstract = {Community detection is one of the most important tasks in network analysis. It is increasingly clear that quality measures are not sufficient for assessing communities and structural properties play a key hole in understanding how nodes are organized in the network. This work presents a comparative study of some representative state-of-the-art methods for overlapping community detection from the perspective of the structural properties of the communities identified by them. Experiments with synthetic and real-world benchmark Ground-Truth networks show that, although the methods are able to identify modular communities, they often miss many structural properties of the communities, such as the number of nodes in the overlapping region and the memberships of the nodes. This is a strong suggestion that a deeper comprehension of the overlapping properties of the communities is needed for the design of more efficient community detection methods.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {Applied Network Science},
	author = {Vieira, Vinícius da Fonseca and Xavier, Carolina Ribeiro and Evsukoff, Alexandre Gonçalves},
	month = dec,
	year = {2020},
	pages = {1--42},
	annote = {Number: 1 Publisher: SpringerOpen},
}

@article{liu_deep_2020-1,
	title = {Deep {Learning} for {Community} {Detection}: {Progress}, {Challenges} and {Opportunities}},
	shorttitle = {Deep {Learning} for {Community} {Detection}},
	url = {http://arxiv.org/abs/2005.08225},
	doi = {10.24963/ijcai.2020/693},
	abstract = {As communities represent similar opinions, similar functions, similar purposes, etc., community detection is an important and extremely useful tool in both scientific inquiry and data analytics. However, the classic methods of community detection, such as spectral clustering and statistical inference, are falling by the wayside as deep learning techniques demonstrate an increasing capacity to handle high-dimensional graph data with impressive performance. Thus, a survey of current progress in community detection through deep learning is timely. Structured into three broad research streams in this domain - deep neural networks, deep graph embedding, and graph neural networks, this article summarizes the contributions of the various frameworks, models, and algorithms in each stream along with the current challenges that remain unsolved and the future research opportunities yet to be explored.},
	urldate = {2021-11-03},
	journal = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
	author = {Liu, Fanzhen and Xue, Shan and Wu, Jia and Zhou, Chuan and Hu, Wenbin and Paris, Cecile and Nepal, Surya and Yang, Jian and Yu, Philip S.},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	pages = {4981--4987},
	annote = {arXiv: 2005.08225},
	annote = {Comment: Accepted Paper in the 29th International Joint Conference on Artificial Intelligence (IJCAI 20), Survey Track},
}

@article{gupta_overlapping_2020-1,
	title = {An overlapping community detection algorithm based on rough clustering of links},
	volume = {125},
	issn = {0169-023X},
	url = {https://www.sciencedirect.com/science/article/pii/S0169023X17304780},
	doi = {10.1016/j.datak.2019.101777},
	abstract = {The growth of networks is prevalent in almost every field due to the digital transformation of consumers, business and society at large. The unfolding of community structure in such real-world complex networks is crucial since it aids in gaining strategic insights leading to informed decisions. Moreover, the co-occurrence of disjoint, overlapping and nested community patterns in such networks demands methodologically rigorous community detection algorithms so as to foster cumulative tradition in data and knowledge engineering. In this paper, we introduce an algorithm for overlapping community detection based on granular information of links and concepts of rough set theory. First, neighborhood links around each pair of nodes are utilized to form initial link subsets. Subsequently, constrained linkage upper approximation of the link subsets is computed iteratively until convergence. The upper approximation subsets obtained during each iteration are constrained and merged using the notion of mutual link reciprocity. The experimental results on ten real-world networks and comparative evaluation with state-of-the-art community detection algorithms demonstrate the effectiveness of the proposed algorithm.},
	language = {en},
	urldate = {2021-11-03},
	journal = {Data \& Knowledge Engineering},
	author = {Gupta, Samrat and Kumar, Pradeep},
	month = jan,
	year = {2020},
	keywords = {Clustering, Community structure, Complex networks, Overlapping communities, Rough sets},
	pages = {101777},
}

@inproceedings{talukdar_new_2009-1,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {New {Regularized} {Algorithms} for {Transductive} {Learning}},
	isbn = {978-3-642-04174-7},
	doi = {10.1007/978-3-642-04174-7_29},
	abstract = {We propose a new graph-based label propagation algorithm for transductive learning. Each example is associated with a vertex in an undirected graph and a weighted edge between two vertices represents similarity between the two corresponding example. We build on Adsorption, a recently proposed algorithm and analyze its properties. We then state our learning algorithm as a convex optimization problem over multi-label assignments and derive an efficient algorithm to solve this problem. We state the conditions under which our algorithm is guaranteed to converge. We provide experimental evidence on various real-world datasets demonstrating the effectiveness of our algorithm over other algorithms for such problems. We also show that our algorithm can be extended to incorporate additional prior information, and demonstrate it with classifying data where the labels are not mutually exclusive.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Talukdar, Partha Pratim and Crammer, Koby},
	editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
	year = {2009},
	keywords = {graph based semi-supervised learning, label propagation, transductive learning},
	pages = {442--457},
}

@inproceedings{ye_discrete_2019-1,
	title = {Discrete {Overlapping} {Community} {Detection} with {Pseudo} {Supervision}},
	doi = {10.1109/ICDM.2019.00081},
	abstract = {Community detection is of significant importance in understanding the structures and functions of networks. Recently, overlapping community detection has drawn much attention due to the ubiquity of overlapping community structures in real-world networks. Nonnegative matrix factorization (NMF), as an emerging standard framework, has been widely employed for overlapping community detection, which obtains nodes' soft community memberships by factorizing the adjacency matrix into low-rank factor matrices. However, in order to determine the ultimate community memberships, we have to post-process the real-valued factor matrix by manually specifying a threshold on it, which is undoubtedly a difficult task. Even worse, a unified threshold may not be suitable for all nodes. To circumvent the cumbersome post-processing step, we propose a novel discrete overlapping community detection approach, i.e., Discrete Nonnegative Matrix Factorization (DNMF), which seeks for a discrete (binary) community membership matrix directly. Thus DNMF is able to assign explicit community memberships to nodes without post-processing. Moreover, DNMF incorporates a pseudo supervision module into it to exploit the discriminative information in an unsupervised manner, which further enhances its robustness. We thoroughly evaluate DNMF using both synthetic and real-world networks. Experiments show that DNMF has the ability to outperform state-of-the-art baseline approaches.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Ye, Fanghua and Chen, Chuan and Zheng, Zibin and Li, Rong-Hua and Yu, Jeffrey Xu},
	month = nov,
	year = {2019},
	keywords = {community detection, discrete nonnegative matrix factorization, overlapping communities, pseudo supervision},
	pages = {708--717},
	annote = {ISSN: 2374-8486},
}

@inproceedings{yang_revisiting_2016-1,
	title = {Revisiting {Semi}-{Supervised} {Learning} with {Graph} {Embeddings}},
	url = {https://proceedings.mlr.press/v48/yanga16.html},
	abstract = {We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Zhilin and Cohen, William and Salakhudinov, Ruslan},
	month = jun,
	year = {2016},
	pages = {40--48},
	annote = {ISSN: 1938-7228},
}

@inproceedings{iscen_label_2019-1,
	title = {Label {Propagation} for {Deep} {Semi}-{Supervised} {Learning}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Iscen_Label_Propagation_for_Deep_Semi-Supervised_Learning_CVPR_2019_paper.html},
	urldate = {2021-11-03},
	author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ondrej},
	year = {2019},
	pages = {5070--5079},
}

@article{wang_unifying_2020-1,
	title = {Unifying {Graph} {Convolutional} {Neural} {Networks} and {Label} {Propagation}},
	url = {http://arxiv.org/abs/2002.06755},
	abstract = {Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.},
	urldate = {2021-11-03},
	journal = {arXiv:2002.06755 [cs, stat]},
	author = {Wang, Hongwei and Leskovec, Jure},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 2002.06755},
}

@article{huang_combining_2020-1,
	title = {Combining {Label} {Propagation} and {Simple} {Models} {Out}-performs {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.13993},
	abstract = {Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an "error correlation" that spreads residual errors in training data to correct errors in test data and (ii) a "prediction correlation" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C\&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at https://github.com/Chillee/CorrectAndSmooth.},
	urldate = {2021-11-03},
	journal = {arXiv:2010.13993 [cs]},
	author = {Huang, Qian and He, Horace and Singh, Abhay and Lim, Ser-Nam and Benson, Austin R.},
	month = nov,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote = {arXiv: 2010.13993},
}

@inproceedings{li_deeper_2018-1,
	title = {Deeper {Insights} {Into} {Graph} {Convolutional} {Networks} for {Semi}-{Supervised} {Learning}},
	copyright = {Authors who publish a paper in this conference agree to the following terms: Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright. The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered. The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein. Author(s) retain all proprietary rights other than copyright (such as patent rights). Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship. Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission. Author(s) may make limited distribution of all or portions of their article/paper prior to publication. In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes. In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16098},
	abstract = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
	language = {en},
	urldate = {2021-11-03},
	booktitle = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Qimai and Han, Zhichao and Wu, Xiao-ming},
	month = apr,
	year = {2018},
}

@article{traag_louvain_2019-1,
	title = {From {Louvain} to {Leiden}: guaranteeing well-connected communities},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {From {Louvain} to {Leiden}},
	url = {https://www.nature.com/articles/s41598-019-41695-z},
	doi = {10.1038/s41598-019-41695-z},
	abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {Scientific Reports},
	author = {Traag, V. A. and Waltman, L. and van Eck, N. J.},
	month = mar,
	year = {2019},
	keywords = {Applied mathematics, Computational science, Computer science},
	pages = {5233},
}

@article{lancichinetti_finding_2011-1,
	title = {Finding {Statistically} {Significant} {Communities} in {Networks}},
	volume = {6},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018961},
	doi = {10.1371/journal.pone.0018961},
	abstract = {Community structure is one of the main structural features of networks, revealing both their internal organization and the similarity of their elementary units. Despite the large variety of methods proposed to detect communities in graphs, there is a big need for multi-purpose techniques, able to handle different types of datasets and the subtleties of community structure. In this paper we present OSLOM (Order Statistics Local Optimization Method), the first method capable to detect clusters in networks accounting for edge directions, edge weights, overlapping communities, hierarchies and community dynamics. It is based on the local optimization of a fitness function expressing the statistical significance of clusters with respect to random fluctuations, which is estimated with tools of Extreme and Order Statistics. OSLOM can be used alone or as a refinement procedure of partitions/covers delivered by other techniques. We have also implemented sequential algorithms combining OSLOM with other fast techniques, so that the community structure of very large networks can be uncovered. Our method has a comparable performance as the best existing algorithms on artificial benchmark graphs. Several applications on real networks are shown as well. OSLOM is implemented in a freely available software (http://www.oslom.org), and we believe it will be a valuable tool in the analysis of networks.},
	language = {en},
	number = {4},
	urldate = {2021-11-04},
	journal = {PLOS ONE},
	author = {Lancichinetti, Andrea and Radicchi, Filippo and Ramasco, José J. and Fortunato, Santo},
	month = apr,
	year = {2011},
	keywords = {Airports, Algorithms, Clustering algorithms, Community structure, Graphs, Mathematical models, Network analysis, Random graphs},
	pages = {e18961},
	annote = {Publisher: Public Library of Science},
}

@inproceedings{jia_communitygan_2019-1,
	address = {New York, NY, USA},
	series = {{WWW} '19},
	title = {{CommunityGAN}: {Community} {Detection} with {Generative} {Adversarial} {Nets}},
	isbn = {978-1-4503-6674-8},
	shorttitle = {{CommunityGAN}},
	url = {https://doi.org/10.1145/3308558.3313564},
	doi = {10.1145/3308558.3313564},
	abstract = {Community detection refers to the task of discovering groups of vertices sharing similar properties or functions so as to understand the network data. With the recent development of deep learning, graph representation learning techniques are also utilized for community detection. However, the communities can only be inferred by applying clustering algorithms based on learned vertex embeddings. These general cluster algorithms like K-means and Gaussian Mixture Model cannot output much overlapped communities, which have been proved to be very common in many real-world networks. In this paper, we propose CommunityGAN, a novel community detection framework that jointly solves overlapping community detection and graph representation learning. First, unlike the embedding of conventional graph representation learning algorithms where the vector entry values have no specific meanings, the embedding of CommunityGAN indicates the membership strength of vertices to communities. Second, a specifically designed Generative Adversarial Net (GAN) is adopted to optimize such embedding. Through the minimax competition between the motif-level generator and discriminator, both of them can alternatively and iteratively boost their performance and finally output a better community structure. Extensive experiments on synthetic data and real-world tasks demonstrate that CommunityGAN achieves substantial community detection performance gains over the state-of-the-art methods.},
	urldate = {2021-11-04},
	booktitle = {The {World} {Wide} {Web} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Jia, Yuting and Zhang, Qinqin and Zhang, Weinan and Wang, Xinbing},
	month = may,
	year = {2019},
	keywords = {Community Detection, Generative Adversarial Nets, Graph Representation Learning},
	pages = {784--794},
}

@article{lancichinetti_benchmarks_2009-1,
	title = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
	volume = {80},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.80.016118},
	doi = {10.1103/PhysRevE.80.016118},
	abstract = {Many complex networks display a mesoscopic structure with groups of nodes sharing many links with the other nodes in their group and comparatively few with nodes of different groups. This feature is known as community structure and encodes precious information about the organization and the function of the nodes. Many algorithms have been proposed but it is not yet clear how they should be tested. Recently we have proposed a general class of undirected and unweighted benchmark graphs, with heterogeneous distributions of node degree and community size. An increasing attention has been recently devoted to develop algorithms able to consider the direction and the weight of the links, which require suitable benchmark graphs for testing. In this paper we extend the basic ideas behind our previous benchmark to generate directed and weighted networks with built-in community structure. We also consider the possibility that nodes belong to more communities, a feature occurring in real systems, such as social networks. As a practical application, we show how modularity optimization performs on our benchmark.},
	number = {1},
	urldate = {2021-11-04},
	journal = {Physical Review E},
	author = {Lancichinetti, Andrea and Fortunato, Santo},
	month = jul,
	year = {2009},
	pages = {016118},
	annote = {Publisher: American Physical Society},
}

@article{lancichinetti_benchmark_2008-1,
	title = {Benchmark graphs for testing community detection algorithms},
	volume = {78},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.78.046110},
	doi = {10.1103/PhysRevE.78.046110},
	abstract = {Community structure is one of the most important features of real networks and reveals the internal organization of the nodes. Many algorithms have been proposed but the crucial issue of testing, i.e., the question of how good an algorithm is, with respect to others, is still open. Standard tests include the analysis of simple artificial graphs with a built-in community structure, that the algorithm has to recover. However, the special graphs adopted in actual tests have a structure that does not reflect the real properties of nodes and communities found in real networks. Here we introduce a class of benchmark graphs, that account for the heterogeneity in the distributions of node degrees and of community sizes. We use this benchmark to test two popular methods of community detection, modularity optimization, and Potts model clustering. The results show that the benchmark poses a much more severe test to algorithms than standard benchmarks, revealing limits that may not be apparent at a first analysis.},
	number = {4},
	urldate = {2021-11-04},
	journal = {Physical Review E},
	author = {Lancichinetti, Andrea and Fortunato, Santo and Radicchi, Filippo},
	month = oct,
	year = {2008},
	pages = {046110},
	annote = {Publisher: American Physical Society},
}

@article{collins_omega_1988-1,
	title = {Omega: {A} {General} {Formulation} of the {Rand} {Index} of {Cluster} {Recovery} {Suitable} for {Non}-disjoint {Solutions}},
	volume = {23},
	issn = {1532-7906},
	shorttitle = {Omega},
	url = {https://doi.org/10.1207/s15327906mbr2302_6},
	doi = {10.1207/s15327906mbr2302_6},
	abstract = {Cluster recovery indices are more important than ever, because of the necessity for comparing the large number of clustering procedures available today. Of the cluster recovery indices prominent in contemporary literature, the Hubert and Arabie (1985) adjustment to the Rand index (1971) has been demonstrated to have the most desirable properties (Milligan \& Cooper, 1986). However, use of the Hubert and Arabie adjustment to the Rand index is limited to cluster solutions involving non-overlapping, or disjoint, clusters. The present paper introduces a generalization of the Hubert and Arabie adjusted Rand index. This generalization, called the Omega index, can be applied to situations where both, one, or neither of the solutions being compared is non-disjoint. In the special case where both solutions are disjoint, the Omega index is equivalent to the Hubert and Arabie adjusted Rand index.},
	language = {eng},
	number = {2},
	urldate = {2021-11-04},
	journal = {Multivariate behavioral research},
	author = {Collins, L M and Dent, C W},
	month = apr,
	year = {1988},
	pmid = {26764947},
	pages = {231--242},
}

@article{gulcehre_hyperbolic_2018-1,
	title = {Hyperbolic {Attention} {Networks}},
	url = {http://arxiv.org/abs/1805.09786},
	abstract = {We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact.},
	urldate = {2021-11-05},
	journal = {arXiv:1805.09786 [cs]},
	author = {Gulcehre, Caglar and Denil, Misha and Malinowski, Mateusz and Razavi, Ali and Pascanu, Razvan and Hermann, Karl Moritz and Battaglia, Peter and Bapst, Victor and Raposo, David and Santoro, Adam and de Freitas, Nando},
	month = may,
	year = {2018},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1805.09786},
}

@article{wang_community_2017-1,
	title = {Community {Preserving} {Network} {Embedding}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10488},
	abstract = {Network embedding, aiming to learn the low-dimensional representations of nodes in networks, is of paramount importance in many real applications. One basic requirement of network embedding is to preserve the structure and inherent properties of the networks. While previous network embedding methods primarily preserve the microscopic structure, such as the first- and second-order proximities of nodes, the mesoscopic community structure, which is one of the most prominent feature of networks, is largely ignored. In this paper, we propose a novel Modularized Nonnegative Matrix Factorization (M-NMF) model to incorporate the community structure into network embedding. We exploit the consensus relationship between the representations of nodes and community structure, and then jointly optimize NMF based representation learning model and modularity based community detection model in a unified framework, which enables the learned representations of nodes to preserve both of the microscopic and community structures. We also provide efficient updating rules to infer the parameters of our model, together with the correctness and convergence guarantees. Extensive experimental results on a variety of real-world networks show the superior performance of the proposed method over the state-of-the-arts.},
	language = {en},
	number = {1},
	urldate = {2021-11-10},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Xiao and Cui, Peng and Wang, Jing and Pei, Jian and Zhu, Wenwu and Yang, Shiqiang},
	month = feb,
	year = {2017},
	keywords = {nonnegative matrix factorization},
	annote = {Number: 1},
}

@inproceedings{ye_deep_2018-1,
	address = {New York, NY, USA},
	series = {{CIKM} '18},
	title = {Deep {Autoencoder}-like {Nonnegative} {Matrix} {Factorization} for {Community} {Detection}},
	isbn = {978-1-4503-6014-2},
	url = {https://doi.org/10.1145/3269206.3271697},
	doi = {10.1145/3269206.3271697},
	abstract = {Community structure is ubiquitous in real-world complex networks. The task of community detection over these networks is of paramount importance in a variety of applications. Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection due to its great interpretability and its natural fitness for capturing the community membership of nodes. However, the existing NMF-based community detection approaches are shallow methods. They learn the community assignment by mapping the original network to the community membership space directly. Considering the complicated and diversified topology structures of real-world networks, it is highly possible that the mapping between the original network and the community membership space contains rather complex hierarchical information, which cannot be interpreted by classic shallow NMF-based approaches. Inspired by the unique feature representation learning capability of deep autoencoder, we propose a novel model, named Deep Autoencoder-like NMF (DANMF), for community detection. Similar to deep autoencoder, DANMF consists of an encoder component and a decoder component. This architecture empowers DANMF to learn the hierarchical mappings between the original network and the final community assignment with implicit low-to-high level hidden attributes of the original network learnt in the intermediate layers. Thus, DANMF should be better suited to the community detection task. Extensive experiments on benchmark datasets demonstrate that DANMF can achieve better performance than the state-of-the-art NMF-based community detection approaches.},
	urldate = {2021-11-10},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Ye, Fanghua and Chen, Chuan and Zheng, Zibin},
	month = oct,
	year = {2018},
	keywords = {community detection, deep learning, deep nonnegative matrix factorization, graph clustering, network analytics},
	pages = {1393--1402},
}

@article{rozemberczki_karate_2020-1,
	title = {Karate {Club}: {An} {API} {Oriented} {Open}-source {Python} {Framework} for {Unsupervised} {Learning} on {Graphs}},
	shorttitle = {Karate {Club}},
	url = {http://arxiv.org/abs/2003.04819},
	abstract = {We present Karate Club a Python framework combining more than 30 state-of-the-art graph mining algorithms which can solve unsupervised machine learning tasks. The primary goal of the package is to make community detection, node and whole graph embedding available to a wide audience of machine learning researchers and practitioners. We designed Karate Club with an emphasis on a consistent application interface, scalability, ease of use, sensible out of the box model behaviour, standardized dataset ingestion, and output generation. This paper discusses the design principles behind this framework with practical examples. We show Karate Club's efficiency with respect to learning performance on a wide range of real world clustering problems, classification tasks and support evidence with regards to its competitive speed.},
	urldate = {2021-11-10},
	journal = {arXiv:2003.04819 [cs, stat]},
	author = {Rozemberczki, Benedek and Kiss, Oliver and Sarkar, Rik},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote = {arXiv: 2003.04819},
	annote = {Comment: The frameworks is available at: https://github.com/benedekrozemberczki/karateclub},
}

@inproceedings{epasto_ego-splitting_2017-1,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {Ego-{Splitting} {Framework}: from {Non}-{Overlapping} to {Overlapping} {Clusters}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {Ego-{Splitting} {Framework}},
	url = {https://doi.org/10.1145/3097983.3098054},
	doi = {10.1145/3097983.3098054},
	abstract = {We propose ego-splitting, a new framework for detecting clusters in complex networks which leverage the local structures known as ego-nets (i.e. the subgraph induced by the neighborhood of each node) to de-couple overlapping clusters. Ego-splitting is a highly scalable and flexible framework, with provable theoretical guarantees, that reduces the complex overlapping clustering problem to a simpler and more amenable non-overlapping (partitioning) problem. We can scale community detection to graphs with tens of billions of edges and outperform previous solutions based on ego-nets analysis. More precisely, our framework works in two steps: a local ego-net analysis phase, and a global graph partitioning phase. In the local step, we first partition the nodes' ego-nets using a partitioning algorithm. We then use the computed clusters to split each node into its persona nodes that represent the instantiations of the node in its communities. Finally, in the global step, we partition the newly created graph to obtain an overlapping clustering of the original graph.},
	urldate = {2021-11-10},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Epasto, Alessandro and Lattanzi, Silvio and Paes Leme, Renato},
	month = aug,
	year = {2017},
	keywords = {ego-nets, large-scale graph algorithms, overlapping clustering},
	pages = {145--154},
}

@article{bevilacqua_equivariant_2021-1,
	title = {Equivariant {Subgraph} {Aggregation} {Networks}},
	url = {http://arxiv.org/abs/2110.02910},
	abstract = {Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures.},
	urldate = {2021-11-22},
	journal = {arXiv:2110.02910 [cs, stat]},
	author = {Bevilacqua, Beatrice and Frasca, Fabrizio and Lim, Derek and Srinivasan, Balasubramaniam and Cai, Chen and Balamurugan, Gopinath and Bronstein, Michael M. and Maron, Haggai},
	month = oct,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 2110.02910},
	annote = {Comment: 42 pages},
}

@article{fristedt_structure_1993-1,
	title = {The structure of random partitions of large integers},
	volume = {337},
	issn = {0002-9947, 1088-6850},
	url = {https://www.ams.org/tran/1993-337-02/S0002-9947-1993-1094553-1/},
	doi = {10.1090/S0002-9947-1993-1094553-1},
	abstract = {Random partitions of integers are treated in the case where all partitions of an integer are assumed to have the same probability. The focus is on limit theorems as the number being partitioned approaches ∞. The limiting probability distribution of the appropriately normalized number of parts of some small size is exponential. The large parts are described by a particular Markov chain. A central limit theorem and a law of large numbers holds for the numbers of intermediate parts of certain sizes. The major tool is a simple construction of random partitions that treats the number being partitioned as a random variable. The same technique is useful when some restriction is placed on partitions, such as the requirement that all parts must be distinct.},
	language = {en},
	number = {2},
	urldate = {2021-12-08},
	journal = {Transactions of the American Mathematical Society},
	author = {Fristedt, Bert},
	year = {1993},
	keywords = {integer partitions, probabilistic limit theorems, Random partitions},
	pages = {703--735},
}

@article{arratia_probabilistic_2016-1,
	title = {Probabilistic {Divide}-and-{Conquer}: {A} {New} {Exact} {Simulation} {Method}, {With} {Integer} {Partitions} as an {Example}},
	volume = {25},
	issn = {0963-5483, 1469-2163},
	shorttitle = {Probabilistic {Divide}-and-{Conquer}},
	url = {https://www.cambridge.org/core/journals/combinatorics-probability-and-computing/article/probabilistic-divideandconquer-a-new-exact-simulation-method-with-integer-partitions-as-an-example/85C6175903F96D32609D1BF6820A4664},
	doi = {10.1017/S0963548315000358},
	abstract = {We propose a new method, probabilistic divide-and-conquer, for improving the success probability in rejection sampling. For the example of integer partitions, there is an ideal recursive scheme which improves the rejection cost from asymptotically order n 3/4 to a constant. We show other examples for which a non-recursive, one-time application of probabilistic divide-and-conquer removes a substantial fraction of the rejection sampling cost. We also present a variation of probabilistic divide-and-conquer for generating i.i.d. samples that exploits features of the coupon collector's problem, in order to obtain a cost that is sublinear in the number of samples.},
	language = {en},
	number = {3},
	urldate = {2021-12-14},
	journal = {Combinatorics, Probability and Computing},
	author = {Arratia, Richard and DeSALVO, Stephen},
	month = may,
	year = {2016},
	keywords = {Primary 60C05, Secondary 05A17},
	pages = {324--351},
	annote = {Publisher: Cambridge University Press},
}

@article{newman_structure_2003-1,
	title = {The {Structure} and {Function} of {Complex} {Networks}},
	volume = {45},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/s003614450342480},
	doi = {10.1137/S003614450342480},
	abstract = {Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.},
	number = {2},
	urldate = {2022-01-10},
	journal = {SIAM Review},
	author = {Newman, M. E. J.},
	month = jan,
	year = {2003},
	keywords = {05C75, 05C90, 94C15, complex systems, computer networks, graph theory, networks, percolation theory, random graphs, social networks},
	pages = {167--256},
	annote = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{hakimi_realizability_1963-1,
	title = {On {Realizability} of a {Set} of {Integers} as {Degrees} of the {Vertices} of a {Linear} {Graph} {II}. {Uniqueness}},
	volume = {11},
	issn = {0368-4245, 2168-3484},
	url = {http://epubs.siam.org/doi/10.1137/0111010},
	doi = {10.1137/0111010},
	language = {en},
	number = {1},
	urldate = {2022-01-10},
	journal = {Journal of the Society for Industrial and Applied Mathematics},
	author = {Hakimi, S. L.},
	month = mar,
	year = {1963},
	pages = {135--147},
}

@article{kleitman_algorithms_1973-1,
	title = {Algorithms for constructing graphs and digraphs with given valences and factors},
	volume = {6},
	issn = {0012-365X},
	url = {https://doi.org/10.1016/0012-365X(73)90037-X},
	doi = {10.1016/0012-365X(73)90037-X},
	abstract = {Given a set of valences \{v"i\} such that \{v"i\} and \{v"i-k\} are both realizable as valences of graphs without loops or multiple edges, an explicit construction method is described for obtaining a graph with valences \{v"i\} having a k-factor. A number of extensions of the result are obtained. Similar results are obtained for directed graphs.},
	number = {1},
	urldate = {2022-01-10},
	journal = {Discrete Mathematics},
	author = {Kleitman, D. J. and Wang, D. L.},
	month = sep,
	year = {1973},
	pages = {79--88},
}

@article{krawczuk_gg-gan_2020,
	title = {{GG}-{GAN}: {A} {Geometric} {Graph} {Generative} {Adversarial} {Network}},
	shorttitle = {{GG}-{GAN}},
	url = {https://openreview.net/forum?id=qiAxL3Xqx1o},
	abstract = {We study the fundamental problem of graph generation. Specifically, we treat graph generation from a geometric perspective by associating each node with a position in space and then connecting the...},
	language = {en},
	urldate = {2021-09-12},
	author = {Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
	month = sep,
	year = {2020},
}

@misc{janchevski_dnmf-python_2022,
	title = {dnmf-python: {Unofficial} {Python} implementation of the {Discrete} {Non}-negative {Matrix} {Factorization} ({DNMF}) overlapping community detection algorithm},
	url = {https://github.com/Bani57/dnmf-python},
	author = {Janchevski, Andrej},
	month = jan,
	year = {2022},
	doi = {10.5281/zenodo.13881411},
}

@mastersthesis{janchevski_graph_2021,
	address = {Lausanne},
	title = {Graph {Embedding} {Methods} for {Graph} {Completion}},
	url = {https://infoscience.epfl.ch/handle/20.500.14299/253964},
	abstract = {Knowledge graphs have recently attracted significant attention from both industry and academia in scenarios that require exploiting large-scale heterogeneous data collections. Knowledge graphs are a type of database where the general structure is a network of entities, their semantic types, properties, and relationships. They support reasoning over the integrated information as the main application. This process is very closely linked to solving the problems of link prediction and query answering for the knowledge graph. The most common approach in tackling these tasks is to compute suitable numeric representations for each graph element, called graph embeddings.
In this work, we present an approach to constructing a model that generates meaningful graph representations while maintaining as significant scalability and prediction performance as possible. During preprocessing, network analysis techniques provide graph features, which are utilized by a novel graph embedding model that integrates local representations, obtained using standard and state-of-the-art techniques, into a global picture. Evaluation results show that the approach performs significantly well on the link prediction and query answering tasks on data from Swisscom, achieving accuracy of more than 90\% and 50\% respectively, reproducing results reported in related work. Certain experiments on academic data confirm the possibility for even further improvement through more focused research.},
	language = {en},
	school = {EPFL},
	author = {Janchevski, Andrej},
	month = aug,
	year = {2021},
	keywords = {Knowledge Graph Representation, Graph Embeddings, Representation Integration},
}

@article{hogan_knowledge_2020,
	title = {Knowledge {Graphs}},
	url = {https://arxiv.org/abs/2003.02320v6},
	doi = {10.1145/3447772},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	language = {en},
	urldate = {2022-06-14},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = mar,
	year = {2020},
}

@book{lewis_network_2011,
	title = {Network {Science}: {Theory} and {Applications}},
	isbn = {978-1-118-21101-4},
	shorttitle = {Network {Science}},
	abstract = {A comprehensive look at the emerging science of networks Network science helps you design faster, more resilient communication networks; revise infrastructure systems such as electrical power grids, telecommunications networks, and airline routes; model market dynamics; understand synchronization in biological systems; and analyze social interactions among people. This is the first book to take a comprehensive look at this emerging science. It examines the various kinds of networks (regular, random, small-world, influence, scale-free, and social) and applies network processes and behaviors to emergence, epidemics, synchrony, and risk. The book's uniqueness lies in its integration of concepts across computer science, biology, physics, social network analysis, economics, and marketing. The book is divided into easy-to-understand topical chapters and the presentation is augmented with clear illustrations, problems and answers, examples, applications, tutorials, and a discussion of related Java software. Chapters cover: Origins Graphs Regular Networks Random Networks Small-World Networks Scale-Free Networks Emergence Epidemics Synchrony Influence Networks Vulnerability Net Gain Biology This book offers a new understanding and interpretation of the field of network science. It is an indispensable resource for researchers, professionals, and technicians in engineering, computing, and biology. It also serves as a valuable textbook for advanced undergraduate and graduate courses in related fields of study.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Lewis, Ted G.},
	month = sep,
	year = {2011},
	keywords = {Computers / Networking / General, Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / General},
}

@book{barabasi_network_2016,
	address = {Cambridge, United Kingdom},
	edition = {1st edition},
	title = {Network {Science}},
	isbn = {978-1-107-07626-6},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Barabasi, Albert-Laszlo},
	month = aug,
	year = {2016},
}

@article{liu_graph_2019,
	title = {Graph {Summarization} {Methods} and {Applications}: {A} {Survey}},
	volume = {51},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Graph {Summarization} {Methods} and {Applications}},
	url = {https://dl.acm.org/doi/10.1145/3186727},
	doi = {10.1145/3186727},
	abstract = {While advances in computing resources have made processing enormous amounts of data possible, human ability to identify patterns in such data has not scaled accordingly. Efficient computational methods for condensing and simplifying data are thus becoming vital for extracting actionable insights. In particular, while data summarization techniques have been studied extensively, only recently has summarizing interconnected data, or graphs , become popular. This survey is a structured, comprehensive overview of the state-of-the-art methods for summarizing graph data. We first broach the motivation behind and the challenges of graph summarization. We then categorize summarization approaches by the type of graphs taken as input and further organize each category by core methodology. Finally, we discuss applications of summarization on real-world graphs and conclude by describing some open problems in the field.},
	language = {en},
	number = {3},
	urldate = {2022-03-15},
	journal = {ACM Computing Surveys},
	author = {Liu, Yike and Safavi, Tara and Dighe, Abhilash and Koutra, Danai},
	month = may,
	year = {2019},
	pages = {1--34},
}

@inproceedings{belth_what_2020,
	address = {Taipei Taiwan},
	title = {What is {Normal}, {What} is {Strange}, and {What} is {Missing} in a {Knowledge} {Graph}: {Unified} {Characterization} via {Inductive} {Summarization}},
	isbn = {978-1-4503-7023-3},
	shorttitle = {What is {Normal}, {What} is {Strange}, and {What} is {Missing} in a {Knowledge} {Graph}},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380189},
	doi = {10.1145/3366423.3380189},
	abstract = {Knowledge graphs (KGs) store highly heterogeneous information about the world in the structure of a graph, and are useful for tasks such as question answering and reasoning. However, they often contain errors and are missing information. Vibrant research in KG refinement has worked to resolve these issues, tailoring techniques to either detect specific types of errors or complete a KG.},
	language = {en},
	urldate = {2022-03-15},
	booktitle = {Proceedings of {The} {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Belth, Caleb and Zheng, Xinyi and Vreeken, Jilles and Koutra, Danai},
	month = apr,
	year = {2020},
	pages = {1115--1126},
}

@book{pinheiro_social_2011,
	title = {Social {Network} {Analysis} in {Telecommunications}},
	isbn = {978-1-118-01095-2},
	abstract = {A timely look at effective use of social network analysis within the telecommunications industry to boost customer relationships The key to any successful company is the relationship that it builds with its customers. This book shows how social network analysis, analytics, and marketing knowledge can be combined to create a positive customer experience within the telecommunications industry. Reveals how telecommunications companies can effectively enhance their relationships with customers Provides the groundwork for defining social network analysis Defines the tools that can be used to address social network problems A must-read for any professionals eager to distinguish their products in the marketplace, this book shows you how to get it done right, with social network analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Pinheiro, Carlos Andre Reis},
	month = may,
	year = {2011},
	keywords = {Business \& Economics / General, Business \& Economics / Management, Business \& Economics / Strategic Planning},
}

@article{kostic_social_2020,
	title = {Social {Network} {Analysis} and {Churn} {Prediction} in {Telecommunications} {Using} {Graph} {Theory}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/7/753},
	doi = {10.3390/e22070753},
	abstract = {Due to telecommunications market saturation, it is very important for telco operators to always have fresh insights into their customer’s dynamics. In that regard, social network analytics and its application with graph theory can be very useful. In this paper we analyze a social network that is represented by a large telco network graph and perform clustering of its nodes by studying a broad set of metrics, e.g., node in/out degree, first and second order influence, eigenvector, authority and hub values. This paper demonstrates that it is possible to identify some important nodes in our social network (graph) that are vital regarding churn prediction. We show that if such a node leaves a monitored telco operator, customers that frequently interact with that specific node will be more prone to leave the monitored telco operator network as well; thus, by analyzing existing churn and previous call patterns, we proactively predict new customers that will probably churn. The churn prediction results are quantified by using top decile lift metrics. The proposed method is general enough to be readily adopted in any field where homophilic or friendship connections can be assumed as a potential churn driver.},
	language = {en},
	number = {7},
	urldate = {2022-03-15},
	journal = {Entropy},
	author = {Kostić, Stefan M. and Simić, Mirjana I. and Kostić, Miroljub V.},
	month = jul,
	year = {2020},
	keywords = {call data record, churn prediction, data mining, graph theory, machine learning, social network analysis},
	pages = {753},
}

@article{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2022-03-16},
	journal = {arXiv:2005.00687 [cs, stat]},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{kujala_collection_2018,
	title = {A collection of public transport network data sets for 25 cities},
	volume = {5},
	copyright = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201889},
	doi = {10.1038/sdata.2018.89},
	abstract = {Various public transport (PT) agencies publish their route and timetable information with the General Transit Feed Specification (GTFS) as the standard open format. Timetable data are commonly used for PT passenger routing. They can also be used for studying the structure and organization of PT networks, as well as the accessibility and the level of service these networks provide. However, using raw GTFS data is challenging as researchers need to understand the details of the GTFS data format, make sure that the data contain all relevant modes of public transport, and have no errors. To lower the barrier for using GTFS data in research, we publish a curated collection of 25 cities' public transport networks in multiple easy-to-use formats including network edge lists, temporal network event lists, SQLite databases, GeoJSON files, and the GTFS data format. This collection promotes the study of how PT is organized across the globe, and also provides a testbed for developing tools for PT network analysis and PT routing algorithms.},
	language = {en},
	number = {1},
	urldate = {2022-04-20},
	journal = {Scientific Data},
	author = {Kujala, Rainer and Weckström, Christoffer and Darst, Richard K. and Mladenović, Miloš N. and Saramäki, Jari},
	month = may,
	year = {2018},
	keywords = {Civil engineering, Computational science, Databases, Geography, Scientific data},
	pages = {180089},
}

@inproceedings{kumar_community_2018,
	address = {Lyon, France},
	title = {Community {Interaction} and {Conflict} on the {Web}},
	isbn = {978-1-4503-5639-8},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186141},
	doi = {10.1145/3178876.3186141},
	abstract = {Users organize themselves into communities on web platform60 s. These communities can interact with one another, often leading to conflicts and toxic interactions. However, little is known about the mechanisms of interactions between communities and how th40ey impact users.},
	language = {en},
	urldate = {2022-04-21},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Kumar, Srijan and Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	year = {2018},
	pages = {933--943},
}

@inproceedings{leskovec_graphs_2005,
	address = {Chicago, Illinois, USA},
	title = {Graphs over time: densification laws, shrinking diameters and possible explanations},
	isbn = {978-1-59593-135-1},
	shorttitle = {Graphs over time},
	url = {http://portal.acm.org/citation.cfm?doid=1081870.1081893},
	doi = {10.1145/1081870.1081893},
	abstract = {How do real graphs evolve over time? What are “normal” growth patterns in social, technological, and information networks? Many studies have discovered patterns in static graphs, identifying properties in a single snapshot of a large network, or in a very small number of snapshots; these include heavy tails for in- and out-degree distributions, communities, small-world phenomena, and others. However, given the lack of information about network evolution over long periods, it has been hard to convert these ﬁndings into statements about trends over time.},
	language = {en},
	urldate = {2022-04-21},
	booktitle = {Proceeding of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining - {KDD} '05},
	publisher = {ACM Press},
	author = {Leskovec, Jure and Kleinberg, Jon and Faloutsos, Christos},
	year = {2005},
	pages = {177},
}

@article{gehrke_overview_2003,
	title = {Overview of the 2003 {KDD} {Cup}},
	volume = {5},
	issn = {1931-0145, 1931-0153},
	url = {https://dl.acm.org/doi/10.1145/980972.980992},
	doi = {10.1145/980972.980992},
	abstract = {This paper surveys the 2003 KDD Cup, a competition held in conjunction with the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) in August 2003. The competition focused on mining the complex real-life social network inherent in the e-print arXiv (arXiv.org). We describe the four KDD Cup tasks: citation prediction, download prediction, data cleaning, and an open task.},
	language = {en},
	number = {2},
	urldate = {2022-04-21},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Gehrke, Johannes and Ginsparg, Paul and Kleinberg, Jon},
	month = dec,
	year = {2003},
	pages = {149--151},
}

@article{newman_mixing_2003,
	title = {Mixing patterns in networks},
	volume = {67},
	issn = {1063-651X, 1095-3787},
	url = {http://arxiv.org/abs/cond-mat/0209450},
	doi = {10.1103/PhysRevE.67.026126},
	abstract = {We study assortative mixing in networks, the tendency for vertices in networks to be connected to other vertices that are like (or unlike) them in some way. We consider mixing according to discrete characteristics such as language or race in social networks and scalar characteristics such as age. As a special example of the latter we consider mixing according to vertex degree, i.e., according to the number of connections vertices have to other vertices: do gregarious people tend to associate with other gregarious people? We propose a number of measures of assortative mixing appropriate to the various mixing types, and apply them to a variety of real-world networks, showing that assortative mixing is a pervasive phenomenon found in many networks. We also propose several models of assortatively mixed networks, both analytic ones based on generating function methods, and numerical ones based on Monte Carlo graph generation techniques. We use these models to probe the properties of networks as their level of assortativity is varied. In the particular case of mixing by degree, we find strong variation with assortativity in the connectivity of the network and in the resilience of the network to the removal of vertices.},
	number = {2},
	urldate = {2022-05-29},
	journal = {Physical Review E},
	author = {Newman, M. E. J.},
	month = feb,
	year = {2003},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {026126},
}

@book{chung_spectral_1996,
	address = {Providence, R.I},
	edition = {UK ed. edition},
	title = {Spectral {Graph} {Theory}},
	isbn = {978-0-8218-0315-8},
	language = {English},
	publisher = {American Mathematical Society},
	author = {Chung, Fan R. K.},
	month = dec,
	year = {1996},
}

@article{kleinberg_authoritative_1999,
	title = {Authoritative sources in a hyperlinked environment},
	volume = {46},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/324133.324140},
	doi = {10.1145/324133.324140},
	abstract = {The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.},
	number = {5},
	urldate = {2022-05-29},
	journal = {Journal of the ACM},
	author = {Kleinberg, Jon M.},
	month = sep,
	year = {1999},
	keywords = {graph algorithms, hypertext structure, link analysis, World Wide Web},
	pages = {604--632},
}

@article{brin_anatomy_1998-1,
	series = {Proceedings of the {Seventh} {International} {World} {Wide} {Web} {Conference}},
	title = {The anatomy of a large-scale hypertextual {Web} search engine},
	volume = {30},
	issn = {0169-7552},
	url = {https://www.sciencedirect.com/science/article/pii/S016975529800110X},
	doi = {10.1016/S0169-7552(98)00110-X},
	abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the Web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and Web proliferation, creating a Web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale Web search engine — the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
	language = {en},
	number = {1},
	urldate = {2022-05-29},
	journal = {Computer Networks and ISDN Systems},
	author = {Brin, Sergey and Page, Lawrence},
	month = apr,
	year = {1998},
	keywords = {Google, Information retrieval, PageRank, Search engines, World Wide Web},
	pages = {107--117},
}

@article{kashtan_efficient_2004,
	title = {Efficient sampling algorithm for estimating subgraph concentrations and detecting network motifs},
	volume = {20},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/bth163},
	doi = {10.1093/bioinformatics/bth163},
	abstract = {Summary: Biological and engineered networks have recently been shown to display network motifs: a small set of characteristic patterns that occur much more frequently than in randomized networks with the same degree sequence. Network motifs were demonstrated to play key information processing roles in biological regulation networks. Existing algorithms for detecting network motifs act by exhaustively enumerating all subgraphs with a given number of nodes in the network. The runtime of such algorithms increases strongly with network size. Here, we present a novel algorithm that allows estimation of subgraph concentrations and detection of network motifs at a runtime that is asymptotically independent of the network size. This algorithm is based on random sampling of subgraphs. Network motifs are detected with a surprisingly small number of samples in a wide variety of networks. Our method can be applied to estimate the concentrations of larger subgraphs in larger networks than was previously possible with exhaustive enumeration algorithms. We present results for high-order motifs in several biological networks and discuss their possible functions.Availability: A software tool for estimating subgraph concentrations and detecting network motifs (mfinder 1.1) and further information is available at http://www.weizmann.ac.il/mcb/UriAlon/},
	number = {11},
	urldate = {2022-05-29},
	journal = {Bioinformatics},
	author = {Kashtan, N. and Itzkovitz, S. and Milo, R. and Alon, U.},
	month = jul,
	year = {2004},
	pages = {1746--1758},
}

@article{traag_louvain_2019-2,
	title = {From {Louvain} to {Leiden}: guaranteeing well-connected communities},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {From {Louvain} to {Leiden}},
	url = {https://www.nature.com/articles/s41598-019-41695-z},
	doi = {10.1038/s41598-019-41695-z},
	abstract = {Community detection is often used to understand the structure of large and complex networks. One of the most popular algorithms for uncovering community structure is the so-called Louvain algorithm. We show that this algorithm has a major defect that largely went unnoticed until now: the Louvain algorithm may yield arbitrarily badly connected communities. In the worst case, communities may even be disconnected, especially when running the algorithm iteratively. In our experimental analysis, we observe that up to 25\% of the communities are badly connected and up to 16\% are disconnected. To address this problem, we introduce the Leiden algorithm. We prove that the Leiden algorithm yields communities that are guaranteed to be connected. In addition, we prove that, when the Leiden algorithm is applied iteratively, it converges to a partition in which all subsets of all communities are locally optimally assigned. Furthermore, by relying on a fast local move approach, the Leiden algorithm runs faster than the Louvain algorithm. We demonstrate the performance of the Leiden algorithm for several benchmark and real-world networks. We find that the Leiden algorithm is faster than the Louvain algorithm and uncovers better partitions, in addition to providing explicit guarantees.},
	language = {en},
	number = {1},
	urldate = {2022-05-29},
	journal = {Scientific Reports},
	author = {Traag, V. A. and Waltman, L. and van Eck, N. J.},
	month = mar,
	year = {2019},
	keywords = {Applied mathematics, Computational science, Computer science},
	pages = {5233},
}

@inproceedings{epasto_ego-splitting_2017-2,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {Ego-{Splitting} {Framework}: from {Non}-{Overlapping} to {Overlapping} {Clusters}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {Ego-{Splitting} {Framework}},
	url = {https://doi.org/10.1145/3097983.3098054},
	doi = {10.1145/3097983.3098054},
	abstract = {We propose ego-splitting, a new framework for detecting clusters in complex networks which leverage the local structures known as ego-nets (i.e. the subgraph induced by the neighborhood of each node) to de-couple overlapping clusters. Ego-splitting is a highly scalable and flexible framework, with provable theoretical guarantees, that reduces the complex overlapping clustering problem to a simpler and more amenable non-overlapping (partitioning) problem. We can scale community detection to graphs with tens of billions of edges and outperform previous solutions based on ego-nets analysis. More precisely, our framework works in two steps: a local ego-net analysis phase, and a global graph partitioning phase. In the local step, we first partition the nodes' ego-nets using a partitioning algorithm. We then use the computed clusters to split each node into its persona nodes that represent the instantiations of the node in its communities. Finally, in the global step, we partition the newly created graph to obtain an overlapping clustering of the original graph.},
	urldate = {2022-05-29},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Epasto, Alessandro and Lattanzi, Silvio and Paes Leme, Renato},
	month = aug,
	year = {2017},
	keywords = {ego-nets, large-scale graph algorithms, overlapping clustering},
	pages = {145--154},
}

@article{gretton_kernel_2012-1,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v13/gretton12a.html},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {25},
	urldate = {2022-05-29},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773},
}

@inproceedings{maccioni_scalable_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {Scalable {Pattern} {Matching} over {Compressed} {Graphs} via {Dedensification}},
	isbn = {978-1-4503-4232-2},
	url = {https://doi.org/10.1145/2939672.2939856},
	doi = {10.1145/2939672.2939856},
	abstract = {One of the most common operations on graph databases is graph pattern matching (e.g., graph isomorphism and more general types of "subgraph pattern matching"). In fact, in some graph query languages every single query is expressed as a graph matching operation. Consequently, there has been a significant amount of research effort in optimizing graph matching operations in graph database systems. As graph databases have scaled in recent years, so too has recent work on scaling graph matching operations. However, the performance of recent proposals for scaling graph pattern matching is limited by the presence of high-degree nodes. These high-degree nodes result in an explosion of intermediate result sizes during query execution, and therefore significant performance bottlenecks. In this paper we present a dedensification technique that losslessly compresses the neighborhood around high-degree nodes. Furthermore, we introduce a query processing technique that enables direct operation of graph query processing operations over the compressed data, without ever having to decompress the data. For pattern matching operations, we show how this technique can be implemented as a layer above existing graph database systems, so that the end-user can benefit from this technique without requiring modifications to the core graph database engine code. Our technique reduces the size of the intermediate result sets during query processing, and thereby improves query performance.},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Maccioni, Antonio and Abadi, Daniel J.},
	month = aug,
	year = {2016},
	keywords = {big graphs, graph databases, graph pattern matching, power-law, rdf, scale-free graphs, sparql},
	pages = {1755--1764},
}

@article{song_mining_2018,
	title = {Mining {Summaries} for {Knowledge} {Graph} {Search}},
	volume = {30},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2807442},
	abstract = {Querying heterogeneous and large-scale knowledge graphs is expensive. This paper studies a graph summarization framework to facilitate knowledge graph search. (1) We introduce a class of reduced summaries. Characterized by approximate graph pattern matching, these summaries are capable of summarizing entities in terms of their neighborhood similarity up to a certain hop, using small and informative graph patterns. (2) We study a diversified graph summarization problem. Given a knowledge graph, it is to discover top-k summaries that maximize a bi-criteria function, characterized by both informativeness and diversity. We show that diversified summarization is feasible for large graphs, by developing both sequential and parallel summarization algorithms. (a) We show that there exists a 2-approximation algorithm to discover diversified summaries. We further develop an anytime sequential algorithm which discovers summaries under resource constraints. (b) We present a new parallel algorithm with quality guarantees. The algorithm is parallel scalable, which ensures its feasibility in distributed graphs. (3) We also develop a summary-based query evaluation scheme, which only refers to a small number of summaries. Using real-world knowledge graphs, we experimentally verify the effectiveness and efficiency of our summarization algorithms, and query processing using summaries.},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Song, Qi and Wu, Yinghui and Lin, Peng and Dong, Luna Xin and Sun, Hui},
	month = oct,
	year = {2018},
	keywords = {Data mining, Graph summarization, Knowledge based systems, Motion pictures, parallel algorithm, Parallel algorithms, pattern mining, Query processing, Scalability, Time factors},
	pages = {1887--1900},
}

@article{krawczuk_gg-gan_2020-1,
	title = {{GG}-{GAN}: {A} {Geometric} {Graph} {Generative} {Adversarial} {Network}},
	shorttitle = {{GG}-{GAN}},
	url = {https://openreview.net/forum?id=qiAxL3Xqx1o},
	abstract = {We study the fundamental problem of graph generation. Specifically, we treat graph generation from a geometric perspective by associating each node with a position in space and then connecting the...},
	language = {en},
	urldate = {2021-09-12},
	author = {Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
	month = sep,
	year = {2020},
}

@inproceedings{you_graphrnn_2018-1,
	title = {{GraphRNN}: {Generating} {Realistic} {Graphs} with {Deep} {Auto}-regressive {Models}},
	shorttitle = {{GraphRNN}},
	url = {https://proceedings.mlr.press/v80/you18a.html},
	abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
	language = {en},
	urldate = {2021-10-08},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
	month = jul,
	year = {2018},
	pages = {5708--5717},
	annote = {ISSN: 2640-3498},
}

@article{mckinney_data_2010-1,
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	url = {https://conference.scipy.org/proceedings/scipy2010/mckinney.html},
	doi = {10.25080/Majora-92bf1922-00a},
	urldate = {2022-06-14},
	journal = {Proceedings of the 9th Python in Science Conference},
	author = {McKinney, Wes},
	year = {2010},
	pages = {56--61},
}

@article{csardi_igraph_2005-1,
	title = {The {Igraph} {Software} {Package} for {Complex} {Network} {Research}},
	volume = {Complex Systems},
	journal = {InterJournal},
	author = {Csardi, Gabor and Nepusz, Tamas},
	month = nov,
	year = {2005},
	pages = {1695},
}

@inproceedings{rozemberczki_karate_2020-2,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {Karate {Club}: {An} {API} {Oriented} {Open}-{Source} {Python} {Framework} for {Unsupervised} {Learning} on {Graphs}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {Karate {Club}},
	url = {https://doi.org/10.1145/3340531.3412757},
	doi = {10.1145/3340531.3412757},
	abstract = {Graphs encode important structural properties of complex systems. Machine learning on graphs has therefore emerged as an important technique in research and applications. We present Karate Club - a Python framework combining more than 30 state-of-the-art graph mining algorithms. These unsupervised techniques make it easy to identify and represent common graph features. The primary goal of the package is to make community detection, node and whole graph embedding available to a wide audience of machine learning researchers and practitioners. Karate Club is designed with an emphasis on a consistent application interface, scalability, ease of use, sensible out of the box model behaviour, standardized dataset ingestion, and output generation. This paper discusses the design principles behind the framework with practical examples. We show Karate Club's efficiency in learning performance on a wide range of real world clustering problems and classification tasks along with supporting evidence of its competitive speed.},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Rozemberczki, Benedek and Kiss, Oliver and Sarkar, Rik},
	month = oct,
	year = {2020},
	keywords = {community detection, graph classification, graph embedding, graph mining, machine learning, network embedding, node embedding},
	pages = {3125--3132},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: fundamental algorithms for scientific computing in {Python}},
	volume = {17},
	copyright = {2020 The Author(s)},
	issn = {1548-7105},
	shorttitle = {{SciPy} 1.0},
	url = {https://www.nature.com/articles/s41592-019-0686-2},
	doi = {10.1038/s41592-019-0686-2},
	abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
	language = {en},
	number = {3},
	urldate = {2022-06-14},
	journal = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C. J. and Polat, Ilhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul},
	month = mar,
	year = {2020},
	keywords = {Biophysical chemistry, Computational biology and bioinformatics, Technology},
	pages = {261--272},
}

@misc{neo4j_inc_neo4j_2022,
	title = {neo4j: {Neo4j} {Bolt} driver for {Python}},
	copyright = {Apache Software License},
	shorttitle = {neo4j},
	url = {https://github.com/neo4j/neo4j-python-driver},
	urldate = {2022-06-14},
	author = {{Neo4j Inc}},
	month = feb,
	year = {2022},
	keywords = {database, Database, graph, neo4j, Software Development},
}

@misc{huang_tensorboardx_2022,
	title = {{tensorboardX}: {TensorBoardX} lets you watch {Tensors} {Flow} without {Tensorflow}},
	copyright = {MIT License},
	shorttitle = {{tensorboardX}},
	url = {https://github.com/lanpa/tensorboardX},
	urldate = {2022-06-14},
	author = {Huang, Tzu-Wei},
	month = may,
	year = {2022},
}

@inproceedings{toutanova_observed_2015-1,
	address = {Beijing, China},
	title = {Observed versus latent features for knowledge base and text inference},
	url = {https://aclanthology.org/W15-4007},
	doi = {10.18653/v1/W15-4007},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 3rd {Workshop} on {Continuous} {Vector} {Space} {Models} and their {Compositionality}},
	publisher = {Association for Computational Linguistics},
	author = {Toutanova, Kristina and Chen, Danqi},
	month = jul,
	year = {2015},
	pages = {57--66},
}

@article{dettmers_convolutional_2018-1,
	title = {Convolutional {2D} {Knowledge} {Graph} {Embeddings}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11573},
	doi = {10.1609/aaai.v32i1.11573},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models — which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree — which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set — however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets — deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = apr,
	year = {2018},
	keywords = {convolution},
	annote = {Number: 1},
}

@inproceedings{xiong_deeppath_2017-1,
	address = {Copenhagen, Denmark},
	title = {{DeepPath}: {A} {Reinforcement} {Learning} {Method} for {Knowledge} {Graph} {Reasoning}},
	shorttitle = {{DeepPath}},
	url = {https://aclanthology.org/D17-1060},
	doi = {10.18653/v1/D17-1060},
	abstract = {We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector-space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
	month = sep,
	year = {2017},
	pages = {564--573},
}

@book{marshall_unity_2002,
	title = {Unity {Of} {Nature}, {The}: {Wholeness} {And} {Disintegration} {In} {Ecology} {And} {Science}},
	isbn = {978-1-78326-116-1},
	shorttitle = {Unity {Of} {Nature}, {The}},
	abstract = {The idea behind The Unity of Nature is a strong theoretical theme in a number of scientific and environmental fields from ecosystems ecology, through quantum physics to environmental philosophy and ecopolitics giving rise to an inspiring, optimistic, socially-responsive and environment-friendly worldview. The fields of science and environmentalism have inherited this theme of natural unity through an intellectual lineage that encompasses many non-scientific and non-environmental fields such as sociology, theology and political philosophy. Many of these fields have used natural unity in a way which is in stark opposition to the metaphysical and political desires of those who promulgate the unity of nature for progressive social change.This book discusses how this has transpired and examines the social and intellectual processes that have been at work. These include the social construction of the Organicism versus Mechanicism debate in ecology, the intellectual links between neo-classical economic principles and the ‘New Sciences’, the techno-scientific background of Gaia theory, and the social conservatism of ecological functionalism.},
	language = {en},
	publisher = {World Scientific},
	author = {Marshall, Alan},
	month = oct,
	year = {2002},
	note = {Google-Books-ID: 3Oy3CgAAQBAJ},
	keywords = {Science / General, Science / History},
}

@misc{noauthor_what_2022,
	title = {What is a {Holistic} {Approach} to {Problem} {Solving}? {\textbar} {Joyce} {University}},
	shorttitle = {What is a {Holistic} {Approach} to {Problem} {Solving}?},
	url = {https://www.joyce.edu/blog/approaching-problems-holistically/},
	abstract = {A holistic problem solving is thinking about the entire picture, where each change that you make to one part directly affects the whole.},
	language = {en-US},
	urldate = {2025-08-20},
	journal = {Joyce},
	author = {, Melissa},
	month = mar,
	year = {2022},
	file = {Snapshot:/Users/admin/Zotero/storage/7P5I7CA6/approaching-problems-holistically.html:text/html},
}

@book{doniger_merriam-websters_1999,
	title = {Merriam-{Webster}'s encyclopedia of world religions ; {Wendy} {Doniger}, consulting editor},
	isbn = {978-0-87779-044-0 978-0-9650672-3-2},
	url = {http://archive.org/details/isbn_9780877790440},
	abstract = {Includes bibliographical references (pages 1168-1181); Contains 3,500 alphabetically arranged entries that provide information about various aspects of the world's religions; features thirty in-depth discussions of major religions; and includes illustrations and maps; An encyclopedia of world religions from Aaron to Zwingli -- Sacred places -- Sacred rituals -- Sacred images -- Sacred costumes; xvii, 1181 pages : 26 cm},
	language = {eng},
	urldate = {2025-08-20},
	publisher = {Springfield, Mass. : Merriam-Webster},
	author = {Doniger, Wendy and Merriam-Webster, Inc},
	collaborator = {{Internet Archive}},
	year = {1999},
	keywords = {Religion},
}

@book{kricheldorf_getting_2016,
	title = {Getting {It} {Right} in {Science} and {Medicine}: {Can} {Science} {Progress} through {Errors}? {Fallacies} and {Facts}},
	isbn = {978-3-319-30386-4},
	shorttitle = {Getting {It} {Right} in {Science} and {Medicine}},
	abstract = {This book advocates the importance and value of errors for the progress of scientific research! Hans Kricheldorf explains that most of the great scientific achievements are based on an iterative process (an ‘innate self-healing mechanism’): errors are committed, being checked over and over again, through which finally new findings and knowledge can arise. New ideas are often first confronted with refusal. This is so not only in real life, but also in scientific and medical research. The author outlines in this book how great ideas had to ripen over time before winning recognition and being accepted. The book showcases in an entertaining way, but without schadenfreude, that even some of the most famous discoverers may appear in completely different light, when regarding errors they have committed in their work.This book is divided into two parts. The first part creates a fundament for the discussion and understanding by introducing important concepts, terms and definitions, suchas (natural) sciences and scientific research, laws of nature, paradigm shift, and progress (in science). It compares natural sciences with other scientific disciplines, such as historical research or sociology, and examines the question if scientific research can generate knowledge of permanent validity. The second part contains a collection of famous fallacies and errors from medicine, biology, chemistry, physics and geology, and how they were corrected. Readers will be astonished and intrigued what meanders had to be explored in some cases before scientists realized facts, which are today’s standard and state-of-the-art of science and technology. This is an entertaining and amusing, but also highly informative book not only for scientists and specialists, but for everybody interested in science, research, their progress, and their history!},
	language = {en},
	publisher = {Springer International Publishing},
	author = {Kricheldorf, Hans R.},
	month = jun,
	year = {2016},
	note = {Google-Books-ID: qJQyjwEACAAJ},
	keywords = {Science / Chemistry / General, Science / History, Science / Philosophy \& Social Aspects, Science / Physics / General},
}

@article{boneva_graph_2007,
	title = {Graph {Abstraction} and {Abstract} {Graph} {Transformation}},
	url = {https://research.utwente.nl/en/publications/graph-abstraction-and-abstract-graph-transformation},
	language = {Undefined},
	urldate = {2025-08-20},
	author = {Boneva, I. B. and Rensink, Arend and Kurban, M. E. and Bauer, J.},
	month = jul,
	year = {2007},
	note = {Publisher: Centre for Telematics and Information Technology (CTIT)},
	file = {Full Text PDF:/Users/admin/Zotero/storage/QURR4FA6/Boneva et al. - 2007 - Graph Abstraction and Abstract Graph Transformatio.pdf:application/pdf},
}

@misc{guangzhou_ganyuan_intelligent_technology_co_ltd_5x5_nodate,
	title = {5x5 {Cube} {Solving} {Guide} {\textbar} {GANCUBE} {Official} {Tutorial} {\textbar} {GANCUBE} {Official} {Website}},
	url = {https://www.gancube.com/pages/5x5x5-cube-tutorial},
	abstract = {Challenge yourself with the 5x5 speed cubes. Follow our step-by-step speedcubing guide for solving larger puzzle cubes.},
	language = {en},
	urldate = {2025-08-20},
	journal = {GANCUBE},
	author = {Guangzhou Ganyuan Intelligent Technology Co., Ltd.},
	file = {Snapshot:/Users/admin/Zotero/storage/SRD3R67V/5x5x5-cube-tutorial.html:text/html},
}

@misc{telgarsky_representation_2015,
	title = {Representation {Benefits} of {Deep} {Feedforward} {Networks}},
	url = {http://arxiv.org/abs/1509.08101},
	doi = {10.48550/arXiv.1509.08101},
	abstract = {This note provides a family of classification problems, indexed by a positive integer \$k\$, where all shallow networks with fewer than exponentially (in \$k\$) many nodes exhibit error at least \$1/6\$, whereas a deep network with 2 nodes in each of \$2k\$ layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated \$k\$ times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities.},
	urldate = {2025-08-20},
	publisher = {arXiv},
	author = {Telgarsky, Matus},
	month = sep,
	year = {2015},
	note = {arXiv:1509.08101 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/Users/admin/Zotero/storage/52UJCUSE/Telgarsky - 2015 - Representation Benefits of Deep Feedforward Networ.pdf:application/pdf;Snapshot:/Users/admin/Zotero/storage/8LBWHMYX/1509.html:text/html},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	number = {5},
	urldate = {2025-08-20},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	file = {ScienceDirect Snapshot:/Users/admin/Zotero/storage/5D739HJQ/0893608089900208.html:text/html},
}

@inproceedings{telgarsky_benefits_2016,
	title = {Benefits of depth in neural networks},
	url = {https://proceedings.mlr.press/v49/telgarsky16.html},
	abstract = {For any positive integer k, there exist neural networks with Θ(k{\textasciicircum}3) layers, Θ(1) nodes per layer, and Θ(1) distinct parameters which can not be approximated by networks with O(k) layers unless they are exponentially large — they must possess Ω(2{\textasciicircum}k) nodes. This result is proved here for a class of nodes termed {\textbackslash}emphsemi-algebraic gates which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: Ω(2{\textasciicircum}k{\textasciicircum}3) total tree nodes are required).},
	language = {en},
	urldate = {2025-08-20},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Telgarsky, Matus},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1517--1539},
	file = {Full Text PDF:/Users/admin/Zotero/storage/886BXY5S/Telgarsky - 2016 - benefits of depth in neural networks.pdf:application/pdf},
}
